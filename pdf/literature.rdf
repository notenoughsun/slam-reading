<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <bib:Article rdf:about="#item_1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>31</prism:volume>
                <dc:title>IEEE Transactions on Robotics</dc:title>
                <dc:identifier>DOI 10.1109/tro.2015.2463671</dc:identifier>
                <prism:number>5</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mur-Artal</foaf:surname>
                        <foaf:givenName>Raul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel</foaf:surname>
                        <foaf:givenName>J. M. M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>Juan D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORB-SLAM: A Versatile and Accurate Monocular SLAM System</dc:title>
        <dc:date>2015</dc:date>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers (IEEE)</dc:description>
        <bib:pages>1147–1163</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="#item_2">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>33</prism:volume>
                <dc:title>IEEE Transactions on Robotics</dc:title>
                <dc:identifier>DOI 10.1109/tro.2017.2705103</dc:identifier>
                <prism:number>5</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mur-Artal</foaf:surname>
                        <foaf:givenName>Raul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>Juan D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras</dc:title>
        <dc:date>2017-06-19</dc:date>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers (IEEE)
_eprint: arXiv:1610.06475v2[cs.RO]</dc:description>
        <bib:pages>1255–1262</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cameras</foaf:surname>
                        <foaf:givenName>RGB-D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>ur-Artal</foaf:surname>
                        <foaf:givenName>Raul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Juan</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORB SLAM 2 : an Open-Source SLAM System for Monocular, Stereo and</dc:title>
    </rdf:Description>
    <rdf:Description rdf:about="#item_4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2017 IEEE International Conference on Robotics and Automation (ICRA) Singapore, May 29 - June 3, 2017</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bowman</foaf:surname>
                        <foaf:givenName>Sean</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Atanasov</foaf:surname>
                        <foaf:givenName>Nikolay</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Daniilidis</foaf:surname>
                        <foaf:givenName>Kostas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pappas</foaf:surname>
                        <foaf:givenName>George</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>Localization</dc:subject>
        <dc:subject>Recognition</dc:subject>
        <dc:subject>SLAM</dc:subject>
        <dc:title>Probabilistic Data Association for Semantic SLAM</dc:title>
        <dcterms:abstract>Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over sensor states and semantic landmark positions that integrates metric information, semantic information, and data associations, and decompose it into two interconnected problems: an estimation of discrete data association and landmark class probabilities, and a continuous optimization over the metric states. The estimated landmark and robot poses affect the association and class distributions, which in turn affect the robot-landmark pose optimization. The performance of our algorithm is demonstrated on indoor and outdoor datasets.</dcterms:abstract>
        <dc:date>2017</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>DOI 10.1109/ISMAR.2018.00024</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rünz</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Buffier</foaf:surname>
                        <foaf:givenName>Maud</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agapito</foaf:surname>
                        <foaf:givenName>Lourdes</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects</dc:title>
        <dc:date>2018-10</dc:date>
        <bib:pages>10-20</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mur-Artal</foaf:surname>
                        <foaf:givenName>Raul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel</foaf:surname>
                        <foaf:givenName>J. M. M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>Juan D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_7"/>
        <link:link rdf:resource="#item_8"/>
        <link:link rdf:resource="#item_9"/>
        <link:link rdf:resource="#item_10"/>
        <dc:subject>Lifelong mapping</dc:subject>
        <dc:subject>localization</dc:subject>
        <dc:subject>monocular vision</dc:subject>
        <dc:subject>recognition</dc:subject>
        <dc:subject>simultaneous localization and mapping (SLAM)</dc:subject>
        <dc:title>ORB-SLAM: A Versatile and Accurate Monocular SLAM System</dc:title>
        <dcterms:abstract>IEEE Transactions on Robotics;2015;31;5;10.1109/TRO.2015.2463671</dcterms:abstract>
        <dc:date>2021</dc:date>
    </rdf:Description>
    <z:Attachment rdf:about="#item_7">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="D:\media\git\slam-reading\pdf\semantic\MurArtal2021 - ORB SLAM_ a Versatile and Accurate Monocular SLAM System.pdf"/>
        <dc:title>Attachment</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_8">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="D:\media\git\slam-reading\pdf\MurArtal2021 - ORB SLAM_ a Versatile and Accurate Monocular SLAM System.pdf"/>
        <dc:title>Attachment</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_9">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="D:\media\git\slam-reading\pdf\Labbe2015ULaval.pdf"/>
        <dc:title>Attachment</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_10">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="D:\media\git\slam-reading\pdf\LabbeAURO2017.pdf"/>
        <dc:title>Attachment</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_11">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lianos</foaf:surname>
                        <foaf:givenName>Konstantinos-Nektarios</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schönberger</foaf:surname>
                        <foaf:givenName>Johannes</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pollefeys</foaf:surname>
                        <foaf:givenName>Marc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sattler</foaf:surname>
                        <foaf:givenName>Torsten</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>SLAM</dc:subject>
        <dc:subject>semantic segmentation</dc:subject>
        <dc:subject>visual odometry</dc:subject>
        <dc:title>VSO: Visual Semantic Odometry</dc:title>
        <dcterms:abstract>Robust data association is a core problem of visual odometry, where image-to-image correspondences provide constraints for camera pose and map estimation. Current state-of-the-art direct and indirect methods use short-term tracking to obtain continuous frame-to-frame constraints, while long-term constraints are established using loop closures. In this paper, we propose a novel visual semantic odometry (VSO) framework to enable medium-term continuous tracking of points using semantics. Our proposed framework can be easily integrated into existing direct and indirect visual odometry pipelines. Experiments on challenging real-world datasets demonstrate a significant improvement over state-of-the-art baselines in the context of autonomous driving simply by integrating our semantic constraints.</dcterms:abstract>
    </rdf:Description>
    <bib:Article rdf:about="#item_12">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>36</prism:volume>
                <dc:title>Journal of Field Robotics</dc:title>
                <dc:identifier>DOI 10.1002/rob.21831</dc:identifier>
                <prism:number>2</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Labbé</foaf:surname>
                        <foaf:givenName>Mathieu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Michaud</foaf:surname>
                        <foaf:givenName>François</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation</dc:title>
        <dcterms:abstract>Distributed as an open source library since 2013, RTAB-Map started as an appearancebased loop closure detection approach with memory management to deal with large-scale and long-term online operation. It then grew to implement Simultaneous Localization and Mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of contraints on sensors, processing capabilities and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power and ease of integration. Since most of SLAM approaches are either visual or lidar-based, comparison is difficult. Therefore, we decided to extend RTAB-Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB-Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real-world datasets (e.g., KITTI, EuRoC, TUM RGB-D, MIT Stata Center on PR2 robot), outlining strengths and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <dc:description>Publisher: Wiley</dc:description>
        <bib:pages>416–446</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_13">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Labbé</foaf:surname>
                        <foaf:givenName>Mathieu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Simultaneous Localization and Mapping (SLAM) with RTAB-Map</dc:title>
    </rdf:Description>
    <bib:Article rdf:about="#item_14">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>42</prism:volume>
                <dc:title>Autonomous Robots</dc:title>
                <dc:identifier>DOI 10.1007/s10514-017-9682-5</dc:identifier>
                <prism:number>6</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Labbé</foaf:surname>
                        <foaf:givenName>Mathieu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Michaud</foaf:surname>
                        <foaf:givenName>François</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>loop closure detection</dc:subject>
        <dc:subject>multi-session</dc:subject>
        <dc:subject>path planning</dc:subject>
        <dc:subject>pose graph</dc:subject>
        <dc:title>Long-term online multi-session graph-based SPLAM with memory management</dc:title>
        <dcterms:abstract>For long-term simultaneous planning, localization and mapping (SPLAM), a robot should be able to continuously update its map according to the dynamic changes of the environment and the new areas explored. With limited onboard computation capabilities, a robot should also be able to limit the size of the map used for online localization and mapping. This paper addresses these challenges using a memory management mechanism, which identifies locations that should remain in a Working Memory (WM) for online processing from locations that should be transferred to a Long-Term Memory (LTM). When revisiting previously mapped areas that are in LTM, the mechanism can retrieve these locations and place them back in WM for online SPLAM. The approach is tested on a robot equipped with a short-range laser rangefinder and a RGB-D camera, patrolling autonomously 10.5 km in an indoor environment over 11 sessions while having encountered 139 people.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <dc:description>Publisher: Springer Science and Business Media LLC</dc:description>
        <bib:pages>1133–1150</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="#item_15">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2</prism:volume>
                <dc:title>IEEE Robotics and Automation Letters</dc:title>
                <dc:identifier>DOI 10.1109/lra.2017.2653359</dc:identifier>
                <prism:number>2</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mur-Artal</foaf:surname>
                        <foaf:givenName>Raul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>Juan D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Visual-Inertial Monocular SLAM With Map Reuse</dc:title>
        <dc:date>2017-01-17</dc:date>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers (IEEE)
_eprint: arXiv:1610.05949v2[cs.RO]</dc:description>
        <bib:pages>796–803</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_16">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Campos</foaf:surname>
                        <foaf:givenName>Irene</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>DOT: Dynamic Object Tracking for Visual SLAM</dc:title>
        <dc:date>2020-07</dc:date>
    </rdf:Description>
    <bib:Article rdf:about="#item_17">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2021</prism:volume>
                <dc:title>Journal of Sensors</dc:title>
                <dc:identifier>DOI 10.1155/2021/2054828</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Servières</foaf:surname>
                        <foaf:givenName>Myriam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Renaudin</foaf:surname>
                        <foaf:givenName>Valérie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dupuis</foaf:surname>
                        <foaf:givenName>Alexis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Antigny</foaf:surname>
                        <foaf:givenName>Nicolas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Potirakis</foaf:surname>
                        <foaf:givenName>Stelios M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dc:title>Visual and Visual-Inertial SLAM: State of the Art, Classification, and Experimental Benchmarking</dc:title>
        <dcterms:abstract>Simultaneous Localization and Mapping is now widely adopted by many applications, and researchers have produced very dense literature on this topic. With the advent of smart devices, embedding cameras, inertial measurement units, visual SLAM (vSLAM), and visual-inertial SLAM (viSLAM) are enabling novel general public applications. In this context, this paper conducts a review of popular SLAM approaches with a focus on vSLAM/viSLAM, both at fundamental and experimental levels. It starts with a structured overview of existing vSLAM and viSLAM designs and continues with a new classification of a dozen main state-ofthe-art methods. A chronological survey of viSLAM's development highlights the historical milestones and presents more recent methods into a classification. Finally, the performance of vSLAM is experimentally assessed for the use case of pedestrian pose estimation with a handheld device in urban environments. The performance of five open-source methods Vins-Mono, ROVIO, ORB-SLAM2, DSO, and LSD-SLAM is compared using the EuRoC MAV dataset and a new visual-inertial dataset corresponding to urban pedestrian navigation. A detailed analysis of the computation results identifies the strengths and weaknesses for each method. Globally, ORB-SLAM2 appears to be the most promising algorithm to address the challenges of urban pedestrian navigation, tested with two datasets.</dcterms:abstract>
        <dc:date>2021-02-25</dc:date>
        <dc:description>Publisher: Hindawi Limited</dc:description>
        <bib:pages>1–26</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_18">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>DOI 2661-2666.10.1109/IROS.2014.6942926</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Labbe</foaf:surname>
                        <foaf:givenName>Mathieu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Michaud</foaf:surname>
                        <foaf:givenName>François</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Online Global Loop Closure Detection for Large-Scale Multi-Session Graph-Based SLAM</dc:title>
        <dcterms:abstract>For large-scale and long-term simultaneous lo- nodes” are used to keep transformation information between calization and mapping (SLAM), a robot has to deal with the maps. A similar approach is also used with multi-robot unknown initial positioning caused by either the kidnapped mapping [6]: transformations between maps are computed robot problem or multi-session mapping. This paper addresses these problems by tying the SLAM system with a global loop when a robot sees the other or when a landmark is seen by closure detection approach, which intrinsically handles these both robots in their respective maps. situations. However, online processing for global loop closure Global loop closure detection approaches, by being inde- detection approaches is generally influenced by the size of the pendent of the robot’s estimated position [7], can intrinsically environment. The proposed graph-based SLAM system uses solve the problem of determining when a robot comes back a memory management approach that only consider portions of the map to satisfy online processing requirements. The to a previous map using a different referential [8]. Popular approach is tested and demonstrated using five indoor mapping global loop detection approaches are appearance-based [9]– sessions of a building using a robot equipped with a laser [12], exploiting the distinctiveness of images. The underlying rangefinder and a Kinect. idea behind these approaches is that loop closure detection</dcterms:abstract>
        <dc:date>2500</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_19">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>DOI: 10.1109/TRO.2021.3075644</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>This paper has been accepted for publication in IEEE Transactions and Robotics</dc:title>
        <dc:date>2007</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_20">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Use of Consumer-grade Depth Cameras in Mobile Robot Navigation</dc:title>
        <dc:date>2015-09</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_21">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>This paper has been accepted in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elvira</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardós</foaf:surname>
                        <foaf:givenName>Juan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORBSLAM-Atlas: a robust and accurate multi-map system</dc:title>
        <dcterms:abstract>We propose ORBSLAM-Atlas, a system able to handle an unlimited number of disconnected sub-maps, that includes a robust map merging algorithm able to detect submaps with common regions and seamlessly fuse them. The outstanding robustness and accuracy of ORBSLAM are due to its ability to detect wide-baseline matches between keyframes, and to exploit them by means of non-linear optimization, however it only can handle a single map. ORBSLAM-Atlas brings the wide-baseline matching detection and exploitation to the multiple map arena. The result is a SLAM system significantly more general and robust, able to perform multisession mapping. If tracking is lost during exploration, instead of freezing the map, a new sub-map is launched, and it can be fused with the previous map when common parts are visited. Our criteria to declare the camera lost contrast with previous approaches that simply count the number of tracked points, we propose to discard also inaccurately estimated camera poses due to bad geometrical conditioning. As a result, the map is split into more accurate sub-maps, that are eventually merged in a more accurate global map, thanks to the multi-mapping capabilities. We provide extensive experimental validation in the EuRoC datasets, where ORBSLAM-Atlas obtains accurate monocular and stereo results in the difficult sequences where ORBSLAM failed. We also build global maps after multiple sessions in the same room, obtaining the best results to date, between 2 and 3 times more accurate than competing multi-map approaches. We also show the robustness and capability of our system to deal with dynamic scenes, quantitatively in the EuRoC datasets and qualitatively in a densely populated corridor where camera occlusions and tracking losses are frequent.</dcterms:abstract>
        <dc:date>2019-08-30</dc:date>
        <dc:description>_eprint: arXiv:1908.11585v1[cs.CV]</dc:description>
    </rdf:Description>
    <rdf:Description rdf:about="#item_22">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>DOI .org/10.1155/2021/2054828</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Article</foaf:surname>
                        <foaf:givenName>Research</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Art</foaf:surname>
                        <foaf:givenName>and VisualVisual-Inertial SLAM: State of the</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Classification</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Benchmarking</foaf:surname>
                        <foaf:givenName>Experimental</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Visual and Visual-Inertial SLAM: State of the Art, Classification,and Experimental Benchmarking</dc:title>
        <dc:date>1980</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_23">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sanfourche</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Delaune</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Besnerais</foaf:surname>
                        <foaf:givenName>G. Le</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Plinval</foaf:surname>
                        <foaf:givenName>H. de</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>UAV</foaf:surname>
                        <foaf:givenName>Perception for</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Israel</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cornic</foaf:surname>
                        <foaf:givenName>Ph</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Treil</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watanabe</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Navigation</foaf:surname>
                        <foaf:givenName>Vision-Based</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Plyer</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>(Onera)</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Environment</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Modeling</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>martial.sanfourche@onera.fr</foaf:surname>
                        <foaf:givenName>E.-mail:</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Mastering Complexity</dc:title>
        <dc:date>2012</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_24">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardós</foaf:surname>
                        <foaf:givenName>Juan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Artal</foaf:surname>
                        <foaf:givenName>Raúl</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel</foaf:surname>
                        <foaf:givenName>José</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORB-SLAM: a Real-Time Accurate Monocular SLAM System</dc:title>
        <dc:date>2015</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_25">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh Chaplot</foaf:surname>
                        <foaf:givenName>Devendra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gandhi</foaf:surname>
                        <foaf:givenName>Dhiraj</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>Saurabh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>Abhinav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>Ruslan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>LEARNING TO EXPLORE USING ACTIVE NEURAL SLAM</dc:title>
        <dcterms:abstract>This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called 'Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge.</dcterms:abstract>
        <dc:date>2020</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_26">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)</dc:title>
                <dc:identifier>DOI 10.1109/aiea53260.2021.00063</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Hongyang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Chengjun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Lequan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lv</foaf:surname>
                        <foaf:givenName>Hongfu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>semantic segmentation</dc:subject>
        <dc:subject>closed loop detection</dc:subject>
        <dc:subject>Convolutional Neural Network (CNN)</dc:subject>
        <dc:subject>Simultaneous Localization and Mapping (SLAM)</dc:subject>
        <dc:title>A loop closure detection method based on semantic segmentation and convolutional neural network</dc:title>
        <dcterms:abstract>2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA);2021; ; ;10.1109/AIEA53260.2021.00063</dcterms:abstract>
        <dc:date>2021</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_27">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2018 IEEE International Conference on Computational Science and Engineering (CSE)</dc:title>
                <dc:identifier>DOI 10.1109/cse.2018.00028</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zeng</foaf:surname>
                        <foaf:givenName>Xuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yewen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Ziqian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Liping</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>SLAM</dc:subject>
        <dc:subject>3D convolution</dc:subject>
        <dc:subject>stereo matching</dc:subject>
        <dc:title>A Hybrid 2D and 3D Convolution Neural Network for Stereo Matching</dc:title>
        <dcterms:abstract>Stereo matching plays an important role in computer vision and SLAM (simultaneous localization and mapping). In this paper, we propose a novel hybrid 2D and 3D convolution neural network for stereo matching. Unlike existing similarity metric based stereo matching methods which need extra postprocessing to finish the matching pipeline, the proposed approach is an end-to-end stereo matching method and it needs much less time for an image pair. Unlike a lot of cost volume and disparity based stereo matching methods which are too complicated to run on performance-constrained devices, the proposed method is much more simple and can run on the real-time sweeping robot that we build. Experimental results on two widely used stereo matching datasets verified the effectiveness of the proposed approach, meanwhile, our real-time SLAM system-the sweeping robot demonstrates that our method can apply to real-time applications.</dcterms:abstract>
        <dc:date>2018</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_28">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luiz</foaf:surname>
                        <foaf:givenName>Ricardo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Horita</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wolf</foaf:surname>
                        <foaf:givenName>Denis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grassi</foaf:surname>
                        <foaf:givenName>Valdir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>goal-driven navigation</dc:subject>
        <dc:subject>reinforcement learning</dc:subject>
        <dc:subject>visual navigation</dc:subject>
        <dc:title>Effective Deep Reinforcement Learning Setups for Multiple Goals on Visual Navigation</dc:title>
        <dcterms:abstract>Deep Reinforcement Learning (DRL) represents an interesting class of algorithms, since its objective is to learn a behavioral policy through interaction with the environment, leveraging the function approximation properties of neural networks. Nonetheless, for episodic problems, it is usually modeled to deal with a unique goal. In this sense, some works showed that it is possible to learn multiple goals when using a Universal Value Function Approximator (UVFA), i.e. a method to learn a universal policy by taking information about the current state of the agent and the goal. Their results are promising but show that there is still space for new contributions regarding the integration of the goal information into the model. For this reason, we propose using the Hadamard product or the Gated-Attention module in the UVFA architecture for visual-based problems. Also, we propose a hybrid exploration strategy based on the-greedy and the categorical probability distribution, namely-categorical. By systematically comparing different architectures of UVFA for different exploration strategies, and applying or not the Trust Region Policy Optimization (TRPO), we demonstrate through experiments that, for visual topologic navigation, combining visual information of the current and goal states through Hadamard product or Gated-Attention module allows the network learning near-optimal navigation policies. Also, we empirically show that the-categorical policy helps to avoid local minimums during the training, which facilitates the convergence to better results.</dcterms:abstract>
        <dc:date>7281</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_29">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Wenbin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mccormac</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Clark</foaf:surname>
                        <foaf:givenName>Ronald</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset</dc:title>
        <dcterms:abstract>Datasets have gained an enormous amount of popularity in the computer vision community, from training and evaluation of Deep Learning-based methods to benchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt, synthetic imagery bears a vast potential due to scalability in terms of amounts of data obtainable without tedious manual ground truth annotations or measurements. Here, we present a dataset with the aim of providing a higher degree of photo-realism, larger scale, more variability as well as serving a wider range of purposes compared to existing datasets. Our dataset leverages the availability of millions of professional interior designs and millions of production-level furniture and object assets-all coming with fine geometric details and high-resolution texture. We render high-resolution and high frame-rate video sequences following realistic trajectories while supporting various camera types as well as providing inertial measurements. Together with the release of the dataset, we will make executable program of our interactive simulator software as well as our renderer available at https://interiornetdataset.github.io. To showcase the usability and uniqueness of our dataset, we show benchmarking results of both sparse and dense SLAM algorithms.</dcterms:abstract>
        <dc:date>2018</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_30">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of the 40th Chinese Control Conference</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Technical Committee on Control Theory, Chinese Association of Automation</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>Kun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Lan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Rui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yan</foaf:surname>
                        <foaf:givenName>Gaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>KPCA</dc:subject>
        <dc:subject>Loop closure detection</dc:subject>
        <dc:subject>Visual SLAM</dc:subject>
        <dc:title>Loop Closure Detection Using KPCA and CNN for Visual SLAM</dc:title>
        <dcterms:abstract>Loop closure detection is often applied to eliminate accumulative track error and mapping error in visual simultaneous localization and mapping (SLAM). Deep convolution neural network (CNN) integrating principal component analysis (PCA) has been recently proposed to implement loop closure detection and to reduce the dimension of features extracted by CNN. However, the combined methods encounter low detection accuracy. To address this problem, Resnet34 pre-trained model is first used to extract features. Then, kernel PCA(KPCA) is applied on the extracted features to reduce the dimension of the features. In the similarity calculation link, restriction range strategy is used to solve the mismatch problem caused by the large similarity of adjacent frames, so as to obtain more accurate recognition results. Finally, the proposed algorithm is analyzed on two open data sets. The experiments show that the proposed algorithm outperforms the traditional CNN and PCA combined method with regard to the accuracy of feature vector matching.</dcterms:abstract>
        <dc:date>2021</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_31">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Tianqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Xin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Xuanyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Auto-Navigator: Decoupled Neural Architecture Search for Visual Navigation</dc:title>
        <dcterms:abstract>Existing visual navigation approaches leverage classification neural networks to extract global features from visual data for navigation. However, these networks are not originally designed for navigation tasks. Thus, the neural architectures might not be suitable to capture scene contents. Fortunately, neural architecture search (NAS) brings a hope to solve this problem. In this paper, we propose an Auto-Navigator to customize a specialized network for visual navigation. However, as navigation tasks mainly rely on reinforcement learning (RL) rewards in training, such weak supervision is insufficiently indicative for NAS to optimize visual perception network. Thus, we introduce imitation learning (IL) with optimal paths to optimize navigation policies while selecting an optimal architecture. As Auto-Navigator can obtain a direct supervision in every step, such guidance greatly facilitates architecture search. In particular, we initialize our Auto-Navigator with a learnable distribution over the search space of visual perception architecture, and then optimize the distribution with IL supervision. Afterwards, we employ an RL reward function to fine-tune our Auto-Navigator to improve the generalization ability of our model. Extensive experiments demonstrate that our Auto-Navigator outperforms baseline methods on Gibson and Matterport3D without significantly increasing network parameters.</dcterms:abstract>
        <dc:date>3743</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_32">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS</dc:title>
                <dc:identifier>DOI 10.1109/igarss47720.2021.9554752</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garcia</foaf:surname>
                        <foaf:givenName>Thaisa Aline Correia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Campos</foaf:surname>
                        <foaf:givenName>Mariana Batista</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Castanheiro</foaf:surname>
                        <foaf:givenName>Leticia Ferrari</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maria</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tommaselli</foaf:surname>
                        <foaf:givenName>Garcia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>Convolutional Neural Networks</dc:subject>
        <dc:subject>fisheye images</dc:subject>
        <dc:subject>image matching</dc:subject>
        <dc:subject>ORB-SLAM fisheye</dc:subject>
        <dc:title>A Proposal to Integrate ORB-Slam Fisheye and Convolutional Neural Networks for Outdoor Terrestrial Mobile Mapping</dc:title>
        <dcterms:abstract>SLAM methods, such as ORB-SLAM, can build a map of an unknown environment (sparse point cloud) with optical images. The sensor motion provides image sequences over which keypoints are extracted and matched, enabling the simultaneous computation of sensor locations and 3D coordinates of points. In the last years, enormous progress has been done to solve the SLAM problem, especially focusing on computational efficiency and accurate sensor trajectory estimation. However, the auto-detection of incorrect or undesired match points (outliers) to support the auto-decision of include or not an image observation in the estimation process is still an open problem. ORB-SLAM fisheye is applied in this study to estimate sensor trajectory based on dual-fisheye images acquired with Ricoh Theta S omnidirectional camera in a terrestrial mobile mapping system carried by a backpack. This preliminary study demonstrated the possible effects of image observation outliers in the sensor trajectory estimation (planimetric and altimetric accuracy of 0.381m and 0.26m, respectively). A proposal to combine semantic segmentation using CNN in the photogrammetric process workflow to cope with this problem and detect potential image observation outlier areas is presented.</dcterms:abstract>
        <dc:date>2021</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_33">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)</dc:title>
                <dc:identifier>DOI 10.1109/ismar-adjunct.2017.39</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Makarov</foaf:surname>
                        <foaf:givenName>Ilya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aliev</foaf:surname>
                        <foaf:givenName>Vladimir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gerasimova</foaf:surname>
                        <foaf:givenName>Olga</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Polyakov</foaf:surname>
                        <foaf:givenName>Pavel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>Deep Convolutional Neural Networks</dc:subject>
        <dc:subject>Example of sample interpolation from NYUDepthv2 set. Left to right: intensity image</dc:subject>
        <dc:subject>FPS K.10.2 [Human-centered computing]: HCI-Mixed/augmented reality</dc:subject>
        <dc:subject>ground truth Depth Map</dc:subject>
        <dc:subject>input depth map</dc:subject>
        <dc:subject>K.11.3 [Computing methodologies]: AI-Computer vision problems: Reconstruction</dc:subject>
        <dc:subject>Mixed Reality</dc:subject>
        <dc:subject>output depth map</dc:subject>
        <dc:subject>Semi-Dense Depth Map Interpolation</dc:subject>
        <dc:title>[POSTER] Depth Map Interpolation Using Perceptual Loss</dc:title>
        <dcterms:abstract>In this paper, we discuss a semi-dense depth map interpolation method based on convolutional neural network. We propose a compact neural network architecture with loss function defined as Euclidean distance in the feature space of VGG-16 neural network used for deep visual recognition. The suggested solution shows state-of-art performance on synthetic and real datasets. Together with LSD-SLAM, the method could be used to provide a dense depth map for interaction purposes, such as creating a first person game in AR/MR or perception module for autonomous vehicle.</dcterms:abstract>
        <dc:date>2017</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_34">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McCormac</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Handa</foaf:surname>
                        <foaf:givenName>Ankur</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leutenegger</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lab</foaf:surname>
                        <foaf:givenName>Dyson Robotics</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>London</foaf:surname>
                        <foaf:givenName>Imperial College</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks</dc:title>
        <dcterms:abstract>Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence — they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN’s Fig. 1: The output of our system: On the left, a dense surfel semantic predictions from multiple view points to be proba- based reconstruction from a video sequence in the NYUv2 bilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that test set. On the right the same map, semantically annotated fusing multiple predictions leads to an improvement even in the with the classes given in the legend below. 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough surfels remain persistently associated with real-world entities to allow real-time interactive use at frame-rates of ≈25Hz. and this enables long-term fusion of per-frame semantic</dcterms:abstract>
        <dc:date>1609</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_35">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Peiliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qin</foaf:surname>
                        <foaf:givenName>Tong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Shaojie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kong</foaf:surname>
                        <foaf:givenName>Hong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>3D Object Localization</dc:subject>
        <dc:subject>Semantic SLAM</dc:subject>
        <dc:subject>Visual Odometry</dc:subject>
        <dc:title>Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving</dc:title>
        <dcterms:abstract>We propose a stereo vision-based approach for tracking the camera ego-motion and 3D semantic objects in dynamic autonomous driving scenarios. Instead of directly regressing the 3D bounding box using end-to-end approaches, we propose to use the easy-to-labeled 2D detection and discrete viewpoint classification together with a lightweight semantic inference method to obtain rough 3D object measurements. Based on the object-aware-aided camera pose tracking which is robust in dynamic environments, in combination with our novel dynamic object bundle adjustment (BA) approach to fuse temporal sparse feature correspondences and the semantic 3D measurement model, we obtain 3D object pose, velocity and anchored dynamic point cloud estimation with instance accuracy and temporal consistency. The performance of our proposed method is demonstrated in diverse scenarios. Both the ego-motion estimation and object localization are compared with the state-of-of-theart solutions.</dcterms:abstract>
        <dc:date>1807</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_36">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of the 23rd International Conference on Automation &amp; Computing, University of Huddersfield, Huddersfield, UK, 7-8 September 2017</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xiwu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Su</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Xinhua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>SLAM</dc:subject>
        <dc:subject>Convolutional Neural Network</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Loop Closure Detection</dc:subject>
        <dc:title>Loop Closure Detection for Visual SLAM Systems Using Convolutional Neural Network</dc:title>
        <dcterms:abstract>This paper is concerned of the loop closure detection problem, which is one of the most critical parts for visual Simultaneous Localization and Mapping (SLAM) systems. Most of state-of-the-art methods use hand-crafted features and bagof-visual-words (BoVW) to tackle this problem. Recent development in deep learning indicates that CNN features significantly outperform hand-crafted features for image representation. This advanced technology has not been fully exploited in robotics, especially in visual SLAM systems. We propose a loop closure detection method based on convolutional neural networks (CNNs). Images are fed into a pre-trained CNN model to extract features. We pre-process CNN features instead of using them directly as most of the presented approaches did before they are used to detect loops. The workflow of extracting CNN features, processing data, computing similarity score and detecting loops is presented. Finally the performance of proposed method is evaluated on several open datasets by comparing it with Fab-Map using precision-recall metric.</dcterms:abstract>
        <dc:date>2017-09-08</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_37">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2010)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>Xuefeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Lina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>data association</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:subject>simultaneous localization and mapping</dc:subject>
        <dc:subject>SOFNN</dc:subject>
        <dc:title>Local Map Matching Based on Fuzzy Neural Networks for Hierarchical SLAM</dc:title>
        <dcterms:abstract>In order to resolve the computational complexity for local map matching of hierarchical simultaneous localization and mapping (SLAM), a novel self-organizing fuzzy neural networks (SOFNN) based approach was proposed in this paper. The matching component for local maps in the hierarchical SLAM is realized by an SOFNN. A subset of signature elements included in a local map was chosen by a clustering algorithm, then was inputted to the SOFNN. The criteria to complete a local map, and the structure learning and parameter learning algorithms for our SOFNN were discussed.</dcterms:abstract>
        <dc:date>2010</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_38">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>IEEE</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Linhui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Zhijie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ozginer</foaf:surname>
                        <foaf:givenName>Umit</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lian</foaf:surname>
                        <foaf:givenName>Jing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Yafu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Yibing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>Semantic SLAM</dc:subject>
        <dc:subject>convolutional neural network</dc:subject>
        <dc:subject>stereo vision</dc:subject>
        <dc:title>Dense 3D Semantic SLAM of traffic environment based on stereo vision</dc:title>
        <dcterms:abstract>To solve the intelligent vehicles' problems of 'where am I?' and 'what is around me?', a dense 3D sematic Simultaneous Localization and Mapping (SLAM) system is proposed to evaluate the pose of the intelligent vehicles and build the dense 3D semantic map. We address these challenges by combining a state of art Stereo-ORB-SLAM system and Convolutional Neural Networks. Firstly, we build a dense 3D point cloud map by using a four thread Stereo-ORB-SLAM system. Subsequently, a fully convolutional neural network architecture which uses RGB-D image as input is used to obtain pixel-wise segmentation. Finally, we fuse the geometric information and semantic information to get the semantic map. We test our method on the KITTI dataset and our dataset made with the Fpgalena stereo camera. Results indicate the system was effective in the real-time building of a semantic map, the speed of the entire system is about 10Hz, and the loop closing function can eliminate most of the drifting errors.</dcterms:abstract>
        <dc:date>2018</dc:date>
    </rdf:Description>
    <bib:Article rdf:about="#item_39">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>68</prism:volume>
                <dc:title>IEEE Transactions on Industrial Electronics</dc:title>
                <dc:identifier>DOI 10.1109/tie.2020.2982096</dc:identifier>
                <prism:number>4</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Ruihao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Sen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gu</foaf:surname>
                        <foaf:givenName>Dongbing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>simultaneous localization and mapping (SLAM)</dc:subject>
        <dc:subject>Depth estimation</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>recurrent convolutional neural network (RCNN)</dc:subject>
        <dc:subject>unsupervised deep learning (DL)</dc:subject>
        <dc:title>DeepSLAM: A Robust Monocular SLAM System With Unsupervised Deep Learning</dc:title>
        <dcterms:abstract>In this article, we propose DeepSLAM, a novel unsupervised deep learning based visual simultaneous localization and mapping (SLAM) system. The DeepSLAM training is fully unsupervised since it only requires stereo imagery instead of annotating ground-truth poses. Its testing takes a monocular image sequence as the input. Therefore, it is a monocular SLAM paradigm. DeepSLAM consists of several essential components, including Mapping-Net, Tracking-Net, Loop-Net, and a graph optimization unit. Specifically, the Mapping-Net is an encoder and decoder architecture for describing the 3-D structure of environment, whereas the Tracking-Net is a recurrent convolutional neural network architecture for capturing the camera motion. The Loop-Net is a pretrained binary classifier for detecting loop closures. DeepSLAM can simultaneously generate pose estimate, depth map, and outlier rejection mask. In this article, we evaluate its performance on various datasets, and find that DeepSLAM achieves good performance in terms of pose estimation accuracy, and is robust in some challenging scenes.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers (IEEE)</dc:description>
        <bib:pages>3577–3587</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_40">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiang</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fox</foaf:surname>
                        <foaf:givenName>Dieter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Allen</foaf:surname>
                        <foaf:givenName>Paul G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Computer</foaf:surname>
                        <foaf:givenName>School of</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Science</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Engineering</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks</dc:title>
        <dcterms:abstract>3D scene understanding is important for robots to Recurrent Neural Network</dcterms:abstract>
        <dc:date>1703</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="#item_41">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh Chaplot</foaf:surname>
                        <foaf:givenName>Devendra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>Ruslan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>Abhinav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>Saurabh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Neural Topological SLAM for Visual Navigation</dc:title>
        <dcterms:abstract>This paper studies the problem of image-goal navigation which involves navigating to the location indicated by a goal image in a novel previously unseen environment. To tackle this problem, we design topological representations for space that effectively leverage semantics and afford approximate geometric reasoning. At the heart of our representations are nodes with associated semantic features, that are interconnected using coarse geometric information. We describe supervised learning-based algorithms that can build, maintain and use such representations under noisy actuation. Experimental study in visually and physically realistic simulation suggests that our method builds effective representations that capture structural regularities and efficiently solve long-horizon navigation problems. We observe a relative improvement of more than 50% over existing methods that study this task.</dcterms:abstract>
        <dc:date>3416</dc:date>
    </rdf:Description>
    <bib:BookSection rdf:about="urn:isbn:978-3-540-67973-8%20978-3-540-44480-0">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>1883</prism:volume>
                <dc:identifier>ISBN 978-3-540-67973-8 978-3-540-44480-0</dc:identifier>
                <dc:title>Vision Algorithms: Theory and Practice</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berlin, Heidelberg</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Berlin Heidelberg</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <z:seriesEditors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goos</foaf:surname>
                        <foaf:givenName>Gerhard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hartmanis</foaf:surname>
                        <foaf:givenName>Juris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van Leeuwen</foaf:surname>
                        <foaf:givenName>Jan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </z:seriesEditors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Triggs</foaf:surname>
                        <foaf:givenName>Bill</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szeliski</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Triggs</foaf:surname>
                        <foaf:givenName>Bill</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McLauchlan</foaf:surname>
                        <foaf:givenName>Philip F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hartley</foaf:surname>
                        <foaf:givenName>Richard I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fitzgibbon</foaf:surname>
                        <foaf:givenName>Andrew W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_45"/>
        <dc:title>Bundle Adjustment — A Modern Synthesis</dc:title>
        <dc:date>2000</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/3-540-44480-7_21</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:08:24</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/3-540-44480-7_21</dc:description>
        <bib:pages>298-372</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_45">
        <z:itemType>attachment</z:itemType>
        <dc:title>Отправленная версия</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://hal.inria.fr/inria-00548290/file/Triggs-va99.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:08:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.emerald.com/insight/content/doi/10.1108/k.2001.30.9_10.1333.2/full/html">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0368-492X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Andrew</foaf:surname>
                        <foaf:givenName>Alex M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Multiple View Geometry in Computer Vision20012Richard Hartley, Andrew Zisserman. &lt;i&gt;Multiple View Geometry in Computer Vision&lt;/i&gt; . Cambridge: Cambridge University Press 2000. xvi + 607 pp., ISBN: 0‐521‐62304‐9 hardback, £60.00</dc:title>
        <dc:date>12/2001</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Multiple View Geometry in Computer Vision20012Richard Hartley, Andrew Zisserman. &lt;i&gt;Multiple View Geometry in Computer Vision&lt;/i&gt; . Cambridge</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.emerald.com/insight/content/doi/10.1108/k.2001.30.9_10.1333.2/full/html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:08:46</dcterms:dateSubmitted>
        <bib:pages>1333-1341</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0368-492X">
        <prism:volume>30</prism:volume>
        <dc:title>Kybernetes</dc:title>
        <dc:identifier>DOI 10.1108/k.2001.30.9_10.1333.2</dc:identifier>
        <prism:number>9/10</prism:number>
        <dcterms:alternative>Kybernetes</dcterms:alternative>
        <dc:identifier>ISSN 0368-492X</dc:identifier>
    </bib:Journal>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/1640781/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2006.236</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mouragnon</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lhuillier</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dhome</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dekeyser</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sayd</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_48"/>
        <dc:title>Real Time Localization and 3D Reconstruction</dc:title>
        <dc:date>2006</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/1640781/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:09:07</dcterms:dateSubmitted>
        <bib:pages>363-370</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_48">
        <z:itemType>attachment</z:itemType>
        <dc:title>Отправленная версия</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://hal.archives-ouvertes.fr/hal-00091145/file/Cvpr06.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:09:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4244-1749-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4244-1749-0</dc:identifier>
                <dc:title>2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality</dc:title>
                <dc:identifier>DOI 10.1109/ISMAR.2007.4538852</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Nara, Japan</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Klein</foaf:surname>
                        <foaf:givenName>Georg</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murray</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Parallel Tracking and Mapping for Small AR Workspaces</dc:title>
        <dc:date>11/2007</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/4538852/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:09:55</dcterms:dateSubmitted>
        <bib:pages>1-10</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2007 6th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Article rdf:about="http://ieeexplore.ieee.org/document/6202705/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1552-3098,%201941-0468"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Galvez-López</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>J. D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Bags of Binary Words for Fast Place Recognition in Image Sequences</dc:title>
        <dc:date>10/2012</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/6202705/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:10:29</dcterms:dateSubmitted>
        <bib:pages>1188-1197</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1552-3098,%201941-0468">
        <prism:volume>28</prism:volume>
        <dc:title>IEEE Transactions on Robotics</dc:title>
        <dc:identifier>DOI 10.1109/TRO.2012.2197158</dc:identifier>
        <prism:number>5</prism:number>
        <dcterms:alternative>IEEE Trans. Robot.</dcterms:alternative>
        <dc:identifier>ISSN 1552-3098, 1941-0468</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="http://photonics.pl/PLP/index.php/letters/article/view/13-9">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2080-2242"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Czyżewski</foaf:surname>
                        <foaf:givenName>Dariusz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fryc</foaf:surname>
                        <foaf:givenName>Irena</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_52"/>
        <dc:title>Luminance Calibration and Linearity Correction Method of Imaging Luminance Measurement Devices</dc:title>
        <dcterms:abstract>This paper presents that the opto-electrical characteristic of a typical CCD based digital camera is nonlinear. It means that digital electric signal of the camera's CCD detector - is not a linear function of the luminance value on camera's lens. The opto-electrical characteristic feature of a digital camera needs to be transformed into a linear function if this camera is to be used as a luminance distribution measurement device known as Imaging Luminance Measurement Device (ILMD). The article presents the methodology for obtaining the opto-electrical characteristic feature of a typical CCD digital camera and focuses on the non- linearity correction method.   Full Text: PDF   ReferencesD. Wüller and H. Gabele, &quot;The usage of digital cameras as luminance meters,&quot; in Digital Photography III, 2007, p. 65020U  CrossRef P. Fiorentin and A. Scroccaro, &quot;Detector-Based Calibration for Illuminance and Luminance Meters-Experimental Results,&quot; IEEE Transactions on Instrumentation and Measurement, vol. 59, no. 5, pp. 1375-1381, 2010  CrossRef M. Shpak, P. Kärhä, G. Porrovecchio, M. Smid, and E. Ikonen, &quot;Luminance meter for photopic and scotopic measurements in the mesopic range,&quot; Meas. Sci. Technol, vol. 25, no. 9, p. 95001, 2014,  CrossRef P. Fiorentin, P. Iacomussi, and G. Rossi, &quot;Characterization and calibration of a CCD detector for light engineering,&quot; IEEE Transactions on Instrumentation and Measurement, vol. 54, no. 1, pp. 171-177, 2005,  CrossRef I. Fryc and E. Czech, &quot;Application of optical fibers and CCD array for measurement of luminance distribution,&quot; in Proc. SPIE 5064, Lightmetry 2002: Metrology and Testing Techniques Using Light, 2003, pp. 18-21,  CrossRef I. Fryc, &quot;Accuracy of spectral correction of a CCD array for luminance distribution measurement,&quot; in Proc. SPIE 5064, Lightmetry 2002: Metrology and Testing Techniques Using Light, 2003, pp. 38-42,  CrossRef I. Fryc, &quot;Analysis of the spectral correction errors of illuminance meter photometric head under the influence of the diffusing element,&quot; Optical Engineering, vol. 40, no. 8, pp. 1636-1640, 2001.  CrossRef D. Czyzewski, &quot;Monitoring of the subsequent LED lighting installation in Warsaw in the years 2014-2015,&quot; in Proceedings of 2016 IEEE Lighting Conference of the Visegrad Countries, Lumen V4 2016, 2016, pp. 1-4,  CrossRef M. Sielachowska, D. Tyniecki, and M. Zajkowski, &quot;Measurements of the Luminance Distribution in the Classroom Using the SkyWatcher Type System,&quot; in 2018 VII. Lighting Conference of the Visegrad Countries (Lumen V4), 2018, pp. 1-5,  CrossRef W. Malska and H. Wachta, &quot;Elements of inferential statistics in a quantitative assessment of illuminations of architectural structures,&quot; in 2016 IEEE Lighting Conference of the Visegrad Countries (Lumen V4), 2016, pp. 1-6,  CrossRef T. Kruisselbrink, R. Dangol, and A. Rosemann, &quot;Photometric measurements of lighting quality: An overview,&quot; Building and Environment, vol. 138, pp. 42-52, 2018.  CrossRef A. Borisuit, M. Münch, L. Deschamps, J. Kämpf, and J.-L. Scartezzini, &quot;A new device for dynamic luminance mapping and glare risk assessment in buildings,&quot; in Proc. SPIE 8485. Nonimaging Optics: Efficient Design for Illumination and Solar Concentration IX, 2012, vol. 8485, p. 84850M,  CrossRef I. Lewin and J. O'Farrell, &quot;Luminaire photometry using video camera techniques,&quot; Journal of the Illuminating Engineering Society, vol. 28, no. 1, pp. 57-63, 1999,  CrossRef D. Czyżewski, &quot;Research on luminance distributions of chip-on-board light-emitting diodes,&quot; Crystals, vol. 9, no. 12, pp. 1-14, 2019,  CrossRef K. Tohsing, M. Schrempf, S. Riechelmann, H. Schilke, and G. Seckmeyer, &quot;Measuring high-resolution sky luminance distributions with a CCD camera,&quot; Applied optics, vol. 52, no. 8, pp. 1564-1573, 2013.  CrossRef D. Czyzewski, &quot;Investigation of COB LED luminance distribution,&quot; in Proceedings of 2016 IEEE Lighting Conference of the Visegrad Countries, Lumen V4 2016, 2016, pp. 1-4,  CrossRef A. de Vries, J. L. Souman, B. de Ruyter, I. Heynderickx, and Y. A. W. de Kort, &quot;Lighting up the office: The effect of wall luminance on room appraisal, office workers' performance, and subjective alertness,&quot; Building and Environment, 2018  CrossRef D. Silvestre, J. Guy, J. Hanck, K. Cornish, and A. Bertone, &quot;Different luminance- and texture-defined contrast sensitivity profiles for school-aged children,&quot; Nature. Scientific Reports, vol. 10, no. 13039, 2020,  CrossRef H. Wachta, K. Baran, and M. Leśko, &quot;The meaning of qualitative reflective features of the facade in the design of illumination of architectural objects,&quot; in AIP Conference Proceedings, 2019, vol. 2078, no. 1, p. 20102.  CrossRef CIE, &quot;Technical raport CIE 231:2019. CIE Classification System of Illuminance and Luminance Meters.,&quot; Vienna, Austria, 2019.  CrossRef DIN, &quot;Standard DIN 5032-7:2017. Photometry - Part 7: Classification of illuminance meters and luminance meters.,&quot; 2017.  DirectLink CEN, &quot;EN 13032-1:2004. Light and lighting - Measurement and presentation of photometric data of lamps and luminaires - Part 1: Measurement and file format,&quot; Bruxelles, Belgium., 2004.  DirectLink CIE, &quot;Technical raport CIE 231:2019. CIE Classification System of Illuminance and Luminance Meters,&quot; Vienna, Austria, 2019  CrossRef E. Czech, D. Czyzewski, &quot;The linearization of the relationship between scene luminance and digital camera output levels&quot;, Photonics Letter of Poland 13, 1 (2021).  CrossRef</dcterms:abstract>
        <dc:date>2021-06-30</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://photonics.pl/PLP/index.php/letters/article/view/13-9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:10:44</dcterms:dateSubmitted>
        <bib:pages>25</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2080-2242">
        <prism:volume>13</prism:volume>
        <dc:title>Photonics Letters of Poland</dc:title>
        <dc:identifier>DOI 10.4302/plp.v13i2.1094</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Photon.Lett.PL</dcterms:alternative>
        <dc:identifier>ISSN 2080-2242</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_52">
        <z:itemType>attachment</z:itemType>
        <dc:title>Полный текст</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://photonics.pl/PLP/index.php/letters/article/download/13-9/652</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:10:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-0-262-51681-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-0-262-51681-5</dc:identifier>
                <dc:title>Robotics: Science and Systems VI</dc:title>
                <dc:identifier>DOI 10.15607/RSS.2010.VI.010</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Robotics: Science and Systems Foundation</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Strasdat</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>M. M. Montiel</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_54"/>
        <dc:title>Scale Drift-Aware Large Scale Monocular SLAM</dc:title>
        <dc:date>2010-06-27</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.roboticsproceedings.org/rss06/p10.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:11:12</dcterms:dateSubmitted>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Robotics: Science and Systems 2010</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_54">
        <z:itemType>attachment</z:itemType>
        <dc:title>Полный текст</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.15607/rss.2010.vi.010</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:11:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="urn:isbn:978-3-319-10604-5%20978-3-319-10605-2">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>8690</prism:volume>
                <dc:identifier>ISBN 978-3-319-10604-5 978-3-319-10605-2</dc:identifier>
                <dc:title>Computer Vision – ECCV 2014</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fleet</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pajdla</foaf:surname>
                        <foaf:givenName>Tomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schiele</foaf:surname>
                        <foaf:givenName>Bernt</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tuytelaars</foaf:surname>
                        <foaf:givenName>Tinne</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Engel</foaf:surname>
                        <foaf:givenName>Jakob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schöps</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cremers</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_56"/>
        <dc:title>LSD-SLAM: Large-Scale Direct Monocular SLAM</dc:title>
        <dc:date>2014</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>LSD-SLAM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-319-10605-2_54</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:12:12</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-319-10605-2_54</dc:description>
        <bib:pages>834-849</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_56">
        <z:itemType>attachment</z:itemType>
        <dc:title>Отправленная версия</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://vision.in.tum.de/_media/spezial/bib/engel14eccv.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:12:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4244-6674-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4244-6674-0</dc:identifier>
                <dc:title>2010 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2010.5652266</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Taipei</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mei</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sibley</foaf:surname>
                        <foaf:givenName>Gabe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Newman</foaf:surname>
                        <foaf:givenName>Paul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Closing loops without places</dc:title>
        <dc:date>10/2010</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/5652266/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:12:49</dcterms:dateSubmitted>
        <bib:pages>3738-3744</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <rdf:Description rdf:about="urn:isbn:978-1-4577-1102-2%20978-1-4577-1101-5%20978-1-4577-1100-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8</dc:identifier>
                <dc:title>2011 International Conference on Computer Vision</dc:title>
                <dc:identifier>DOI 10.1109/ICCV.2011.6126544</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Barcelona, Spain</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rublee</foaf:surname>
                        <foaf:givenName>Ethan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rabaud</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Konolige</foaf:surname>
                        <foaf:givenName>Kurt</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bradski</foaf:surname>
                        <foaf:givenName>Gary</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORB: An efficient alternative to SIFT or SURF</dc:title>
        <dc:date>11/2011</dc:date>
        <z:shortTitle>ORB</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/6126544/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:13:19</dcterms:dateSubmitted>
        <bib:pages>2564-2571</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2011 IEEE International Conference on Computer Vision (ICCV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <rdf:Description rdf:about="urn:isbn:978-1-4799-3685-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4799-3685-4</dc:identifier>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2014.6906953</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Hong Kong, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mur-Artal</foaf:surname>
                        <foaf:givenName>Raul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>Juan D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Fast relocalisation and loop closing in keyframe-based SLAM</dc:title>
        <dc:date>5/2014</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/6906953/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:14:06</dcterms:dateSubmitted>
        <bib:pages>846-853</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <rdf:Description rdf:about="#item_60">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mur-Artal</foaf:surname>
                        <foaf:givenName>Raul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardos</foaf:surname>
                        <foaf:givenName>Juan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORB-SLAM: Tracking and Mapping Recognizable Features</dc:title>
        <dc:date>2014-07</dc:date>
    </rdf:Description>
    <bib:Article rdf:about="https://linkinghub.elsevier.com/retrieve/pii/S0921889009000876">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09218890"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williams</foaf:surname>
                        <foaf:givenName>Brian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cummins</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Neira</foaf:surname>
                        <foaf:givenName>José</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Newman</foaf:surname>
                        <foaf:givenName>Paul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reid</foaf:surname>
                        <foaf:givenName>Ian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tardós</foaf:surname>
                        <foaf:givenName>Juan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_62"/>
        <dc:title>A comparison of loop closing techniques in monocular SLAM</dc:title>
        <dc:date>12/2009</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://linkinghub.elsevier.com/retrieve/pii/S0921889009000876</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:07</dcterms:dateSubmitted>
        <bib:pages>1188-1197</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09218890">
        <prism:volume>57</prism:volume>
        <dc:title>Robotics and Autonomous Systems</dc:title>
        <dc:identifier>DOI 10.1016/j.robot.2009.06.010</dc:identifier>
        <prism:number>12</prism:number>
        <dcterms:alternative>Robotics and Autonomous Systems</dcterms:alternative>
        <dc:identifier>ISSN 09218890</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_62">
        <z:itemType>attachment</z:itemType>
        <dc:title>Отправленная версия</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://webdiis.unizar.es/GRPTR/pubs/2008_Williams_RSS_IDA.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-72819-077-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72819-077-8</dc:identifier>
                <dc:title>2021 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA48506.2021.9561238</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Xi'an, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huai</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Guoquan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Markov Parallel Tracking and Mapping for Probabilistic SLAM</dc:title>
        <dc:date>2021-5-30</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9561238/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:15</dcterms:dateSubmitted>
        <bib:pages>11661-11667</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Article rdf:about="https://www.frontiersin.org/articles/10.3389/fenrg.2021.803631/full">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2296-598X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Zhenyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Xiangyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Shengming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yingshun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_65"/>
        <dc:title>OC-SLAM: Steadily Tracking and Mapping in Dynamic Environments</dc:title>
        <dcterms:abstract>Visual Simultaneous Localization and Mapping (SLAM) system is mainly used in real-time localization and mapping tasks of robots in various complex environments, while traditional monocular vision algorithms are struggling to cope with weak texture and dynamic scenes. To solve these problems, this work presents an object detection and clustering assisted SLAM algorithm (OC-SLAM), which adopts a faster object detection algorithm to add semantic information to the image and conducts geometrical constraint on the dynamic keypoints in the prediction box to optimize the camera pose. It also uses RGB-D camera to perform dense point cloud reconstruction with the dynamic objects rejected, and facilitates European clustering of dense point clouds to jointly eliminate dynamic features combining with object detection algorithm. Experiments in the TUM dataset indicate that OC-SLAM enhances the localization accuracy of the SLAM system in the dynamic environments compared with original algorithm and it has shown impressive performance in the localizition and can build a more precise dense point cloud map in dynamic scenes.</dcterms:abstract>
        <dc:date>2021-12-6</dc:date>
        <z:shortTitle>OC-SLAM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fenrg.2021.803631/full</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:22</dcterms:dateSubmitted>
        <bib:pages>803631</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2296-598X">
        <prism:volume>9</prism:volume>
        <dc:title>Frontiers in Energy Research</dc:title>
        <dc:identifier>DOI 10.3389/fenrg.2021.803631</dc:identifier>
        <dcterms:alternative>Front. Energy Res.</dcterms:alternative>
        <dc:identifier>ISSN 2296-598X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_65">
        <z:itemType>attachment</z:itemType>
        <dc:title>Полный текст</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fenrg.2021.803631/pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-72815-859-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72815-859-4</dc:identifier>
                <dc:title>2019 IEEE International Conference on Unmanned Systems and Artificial Intelligence (ICUSAI)</dc:title>
                <dc:identifier>DOI 10.1109/ICUSAI47366.2019.9124850</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Xi'an, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Hua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Chenguang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feng</foaf:surname>
                        <foaf:givenName>Ying</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>FD-SLAM: Real-time Tracking and Mapping in Dynamic Environments</dc:title>
        <dc:date>11/2019</dc:date>
        <z:shortTitle>FD-SLAM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9124850/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:30</dcterms:dateSubmitted>
        <bib:pages>166-171</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 International Conference on Unmanned Systems and Artificial Intelligence (ICUSAI)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s10846-018-0913-6">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0921-0296,%201573-0409"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>De Croce</foaf:surname>
                        <foaf:givenName>Mauro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pire</foaf:surname>
                        <foaf:givenName>Taihú</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bergero</foaf:surname>
                        <foaf:givenName>Federico</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>DS-PTAM: Distributed Stereo Parallel Tracking and Mapping SLAM System</dc:title>
        <dc:date>8/2019</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>DS-PTAM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s10846-018-0913-6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:38</dcterms:dateSubmitted>
        <bib:pages>365-377</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0921-0296,%201573-0409">
        <prism:volume>95</prism:volume>
        <dc:title>Journal of Intelligent &amp; Robotic Systems</dc:title>
        <dc:identifier>DOI 10.1007/s10846-018-0913-6</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>J Intell Robot Syst</dcterms:alternative>
        <dc:identifier>ISSN 0921-0296, 1573-0409</dc:identifier>
    </bib:Journal>
    <bib:BookSection rdf:about="urn:isbn:978-3-030-23806-3%20978-3-030-23807-0">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>11649</prism:volume>
                <dc:identifier>ISBN 978-3-030-23806-3 978-3-030-23807-0</dc:identifier>
                <dc:title>Towards Autonomous Robotic Systems</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Althoefer</foaf:surname>
                        <foaf:givenName>Kaspar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Konstantinova</foaf:surname>
                        <foaf:givenName>Jelizaveta</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Ketao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Webb</foaf:surname>
                        <foaf:givenName>Andrew M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brown</foaf:surname>
                        <foaf:givenName>Gavin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luján</foaf:surname>
                        <foaf:givenName>Mikel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>ORB-SLAM-CNN: Lessons in Adding Semantic Map Construction to Feature-Based SLAM</dc:title>
        <dc:date>2019</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>ORB-SLAM-CNN</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-030-23807-0_19</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:45</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-030-23807-0_19</dc:description>
        <bib:pages>221-235</bib:pages>
    </bib:BookSection>
    <bib:Article rdf:about="https://iopscience.iop.org/article/10.1088/1742-6596/1693/1/012068">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1742-6588,%201742-6596"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Qiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kang</foaf:surname>
                        <foaf:givenName>Jia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yangxi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Xiaofang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>An improved feature matching ORB-SLAM algorithm</dc:title>
        <dcterms:abstract>Abstract
            To solve the problems of low accuracy and poor real-time performance in traditional mobile robot vision simultaneous positioning and map construction (SLAM), the original algorithm was improved. First, the ORB features of adjacent images are extracted, and the PROSAC algorithm is used to achieve feature point matching. At the same time, the PROSAC algorithm is improved and optimized, and the execution time of the optimized PROSAC algorithm is significantly reduced; finally, based on the graph optimization model, a global Bundle Adjustment algorithm based on the largest common-view weight frame is proposed to achieve dense and sparse map creation. The algorithm is verified by Tum data set, and the experiment shows that the root mean square error has dropped significantly. The results on the data set effectively prove the effectiveness of the algorithm in this paper.</dcterms:abstract>
        <dc:date>2020-12-01</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://iopscience.iop.org/article/10.1088/1742-6596/1693/1/012068</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:17:52</dcterms:dateSubmitted>
        <bib:pages>012068</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1742-6588,%201742-6596">
        <prism:volume>1693</prism:volume>
        <dc:title>Journal of Physics: Conference Series</dc:title>
        <dc:identifier>DOI 10.1088/1742-6596/1693/1/012068</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>J. Phys.: Conf. Ser.</dcterms:alternative>
        <dc:identifier>ISSN 1742-6588, 1742-6596</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="http://journals.sagepub.com/doi/10.1177/0278364910385483">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0278-3649,%201741-3176"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cummins</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Newman</foaf:surname>
                        <foaf:givenName>Paul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Appearance-only SLAM at large scale with FAB-MAP 2.0</dc:title>
        <dcterms:abstract>We describe a new formulation of appearance-only SLAM suitable for very large scale place recognition. The system navigates in the space of appearance, assigning each new observation to either a new or a previously visited location, without reference to metric position. The system is demonstrated performing reliable online appearance mapping and loop-closure detection over a 1000 km trajectory, with mean filter update times of 14 ms. The scalability of the system is achieved by defining a sparse approximation to the FAB-MAP model suitable for implementation using an inverted index. Our formulation of the problem is fully probabilistic and naturally incorporates robustness against perceptual aliasing. We also demonstrate that the approach substantially outperforms the standard term-frequency inverse-document-frequency (tf-idf) ranking measure. The 1000 km data set comprising almost a terabyte of omni-directional and stereo imagery is available for use, and we hope that it will serve as a benchmark for future systems.</dcterms:abstract>
        <dc:date>08/2011</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://journals.sagepub.com/doi/10.1177/0278364910385483</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:19:19</dcterms:dateSubmitted>
        <bib:pages>1100-1123</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0278-3649,%201741-3176">
        <prism:volume>30</prism:volume>
        <dc:title>The International Journal of Robotics Research</dc:title>
        <dc:identifier>DOI 10.1177/0278364910385483</dc:identifier>
        <prism:number>9</prism:number>
        <dcterms:alternative>The International Journal of Robotics Research</dcterms:alternative>
        <dc:identifier>ISSN 0278-3649, 1741-3176</dc:identifier>
    </bib:Journal>
    <rdf:Description rdf:about="urn:isbn:978-0-7695-2597-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2</prism:volume>
                <dc:identifier>ISBN 978-0-7695-2597-6</dc:identifier>
                <dc:title>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2006.264</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nister</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stewenius</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Scalable Recognition with a Vocabulary Tree</dc:title>
        <dc:date>2006</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/1641018/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:19:26</dcterms:dateSubmitted>
        <bib:pages>2161-2168</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:BookSection rdf:about="urn:isbn:978-3-642-15560-4%20978-3-642-15561-1">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>6314</prism:volume>
                <dc:identifier>ISBN 978-3-642-15560-4 978-3-642-15561-1</dc:identifier>
                <dc:title>Computer Vision – ECCV 2010</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berlin, Heidelberg</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Berlin Heidelberg</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hutchison</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kanade</foaf:surname>
                        <foaf:givenName>Takeo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kittler</foaf:surname>
                        <foaf:givenName>Josef</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kleinberg</foaf:surname>
                        <foaf:givenName>Jon M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mattern</foaf:surname>
                        <foaf:givenName>Friedemann</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mitchell</foaf:surname>
                        <foaf:givenName>John C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naor</foaf:surname>
                        <foaf:givenName>Moni</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nierstrasz</foaf:surname>
                        <foaf:givenName>Oscar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pandu Rangan</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Steffen</foaf:surname>
                        <foaf:givenName>Bernhard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sudan</foaf:surname>
                        <foaf:givenName>Madhu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Terzopoulos</foaf:surname>
                        <foaf:givenName>Demetri</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tygar</foaf:surname>
                        <foaf:givenName>Doug</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vardi</foaf:surname>
                        <foaf:givenName>Moshe Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weikum</foaf:surname>
                        <foaf:givenName>Gerhard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Calonder</foaf:surname>
                        <foaf:givenName>Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lepetit</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Strecha</foaf:surname>
                        <foaf:givenName>Christoph</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fua</foaf:surname>
                        <foaf:givenName>Pascal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Daniilidis</foaf:surname>
                        <foaf:givenName>Kostas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maragos</foaf:surname>
                        <foaf:givenName>Petros</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paragios</foaf:surname>
                        <foaf:givenName>Nikos</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <link:link rdf:resource="#item_73"/>
        <dc:title>BRIEF: Binary Robust Independent Elementary Features</dc:title>
        <dc:date>2010</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>BRIEF</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-642-15561-1_56</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:19:53</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-642-15561-1_56</dc:description>
        <bib:pages>778-792</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_73">
        <z:itemType>attachment</z:itemType>
        <dc:title>Полный текст</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2F978-3-642-15561-1_56.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:19:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="urn:isbn:978-3-540-33832-1%20978-3-540-33833-8">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>3951</prism:volume>
                <dc:identifier>ISBN 978-3-540-33832-1 978-3-540-33833-8</dc:identifier>
                <dc:title>Computer Vision – ECCV 2006</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berlin, Heidelberg</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Berlin Heidelberg</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leonardis</foaf:surname>
                        <foaf:givenName>Aleš</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bischof</foaf:surname>
                        <foaf:givenName>Horst</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pinz</foaf:surname>
                        <foaf:givenName>Axel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rosten</foaf:surname>
                        <foaf:givenName>Edward</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Drummond</foaf:surname>
                        <foaf:givenName>Tom</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Machine Learning for High-Speed Corner Detection</dc:title>
        <dc:date>2006</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://link.springer.com/10.1007/11744023_34</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:20:20</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/11744023_34</dc:description>
        <bib:pages>430-443</bib:pages>
    </bib:BookSection>
    <bib:BookSection rdf:about="http://link.springer.com/10.1007/11744023_32">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>3951</prism:volume>
                <dc:identifier>ISBN 978-3-540-33832-1 978-3-540-33833-8</dc:identifier>
                <dc:title>Computer Vision – ECCV 2006</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berlin, Heidelberg</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Berlin Heidelberg</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leonardis</foaf:surname>
                        <foaf:givenName>Aleš</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bischof</foaf:surname>
                        <foaf:givenName>Horst</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pinz</foaf:surname>
                        <foaf:givenName>Axel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bay</foaf:surname>
                        <foaf:givenName>Herbert</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tuytelaars</foaf:surname>
                        <foaf:givenName>Tinne</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Gool</foaf:surname>
                        <foaf:givenName>Luc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_77"/>
        <dc:title>SURF: Speeded Up Robust Features</dc:title>
        <dc:date>2006</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SURF</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://link.springer.com/10.1007/11744023_32</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:20:50</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/11744023_32</dc:description>
        <bib:pages>404-417</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_77">
        <z:itemType>attachment</z:itemType>
        <dc:title>Отправленная версия</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://lear.inrialpes.fr/%7Edouze/enseignement/2014-2015/presentation_papers/SURF.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:21:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1023/B:VISI.0000029664.99615.94">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0920-5691"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lowe</foaf:surname>
                        <foaf:givenName>David G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Distinctive Image Features from Scale-Invariant Keypoints</dc:title>
        <dc:date>11/2004</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1023/B:VISI.0000029664.99615.94</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:21:24</dcterms:dateSubmitted>
        <bib:pages>91-110</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0920-5691">
        <prism:volume>60</prism:volume>
        <dc:title>International Journal of Computer Vision</dc:title>
        <dc:identifier>DOI 10.1023/B:VISI.0000029664.99615.94</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>International Journal of Computer Vision</dcterms:alternative>
        <dc:identifier>ISSN 0920-5691</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="http://ieeexplore.ieee.org/document/4160954/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0162-8828,%202160-9292"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>Andrew J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reid</foaf:surname>
                        <foaf:givenName>Ian D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Molton</foaf:surname>
                        <foaf:givenName>Nicholas D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stasse</foaf:surname>
                        <foaf:givenName>Olivier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>MonoSLAM: Real-Time Single Camera SLAM</dc:title>
        <dc:date>6/2007</dc:date>
        <z:shortTitle>MonoSLAM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/4160954/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:21:55</dcterms:dateSubmitted>
        <bib:pages>1052-1067</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0162-8828,%202160-9292">
        <prism:volume>29</prism:volume>
        <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
        <dc:identifier>DOI 10.1109/TPAMI.2007.1049</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>IEEE Trans. Pattern Anal. Mach. Intell.</dcterms:alternative>
        <dc:identifier>ISSN 0162-8828, 2160-9292</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="#item_79">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>24</prism:volume>
                <dc:title>IEEE Transactions on Robotics</dc:title>
                <dc:identifier>DOI 10.1109/TRO.2008.2003276</dc:identifier>
                <prism:number>5</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Civera</foaf:surname>
                        <foaf:givenName>Javier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>Andrew J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel</foaf:surname>
                        <foaf:givenName>J. M. MartÍnez</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Inverse Depth Parametrization for Monocular SLAM</dc:title>
        <dc:date>2008</dc:date>
        <bib:pages>932-945</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_80">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2014.6906584</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forster</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pizzoli</foaf:surname>
                        <foaf:givenName>Matia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>SVO: Fast semi-direct monocular visual odometry</dc:title>
        <dc:date>2014</dc:date>
        <bib:pages>15-22</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_81">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</dc:title>
                <dc:identifier>DOI 10.1109/ISMAR.2013.6671781</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Haomin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Zilong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Guofeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bao</foaf:surname>
                        <foaf:givenName>Hujun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Robust monocular SLAM in dynamic environments</dc:title>
        <dc:date>2013</dc:date>
        <bib:pages>209-218</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_82">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2014.6907055</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lim</foaf:surname>
                        <foaf:givenName>Hyon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lim</foaf:surname>
                        <foaf:givenName>Jongwoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>H. Jin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Real-time 6-DOF monocular visual SLAM in a large-scale environment</dc:title>
        <dc:date>2014</dc:date>
        <bib:pages>1532-1539</bib:pages>
    </rdf:Description>
    <bib:Article rdf:about="#item_83">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>26</prism:volume>
                <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
                <dc:identifier>DOI 10.1109/TPAMI.2004.17</dc:identifier>
                <prism:number>6</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nister</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>An efficient solution to the five-point relative pose problem</dc:title>
        <dc:date>2004</dc:date>
        <bib:pages>756-770</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="https://royalsocietypublishing.org/doi/10.1098/rspb.1986.0030">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0080-4649,%202053-9193"/>
        <dc:title>The reconstruction of a plane surface from two perspective projections</dc:title>
        <dcterms:abstract>This paper addresses the mathematical problem of reconstructing a visually textured plane surface from a pair of photographs taken from finitely separated camera positions of unknown relative orientation, lying on the same side or on opposite sides of the visible plane. If the surface lies at infinity, or perpendicularly bisects the line joining the centres of projection O and O', the reconstruction fails; otherwise the two images permit either one or two three-dimensional interpretations, obtainable by diagonalizing a 3 x 3 matrix. If all the visible texture elements lie nearer to one viewpoint than to the other, then there are two interpretations, which coincide if the line OO' is perpendicular to the visible plane. Otherwise, only the veridical interpretation survives. The relevance of these results to human and computer vision is briefly discussed.</dcterms:abstract>
        <dc:date>1986-05-22</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://royalsocietypublishing.org/doi/10.1098/rspb.1986.0030</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2021-12-26 20:27:19</dcterms:dateSubmitted>
        <bib:pages>399-410</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0080-4649,%202053-9193">
        <prism:volume>227</prism:volume>
        <dc:title>Proceedings of the Royal Society of London. Series B. Biological Sciences</dc:title>
        <dc:identifier>DOI 10.1098/rspb.1986.0030</dc:identifier>
        <prism:number>1249</prism:number>
        <dcterms:alternative>Proc. R. Soc. Lond. B.</dcterms:alternative>
        <dc:identifier>ISSN 0080-4649, 2053-9193</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="https://doi.org/10.1023/A:1008140928553">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1573-1405"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torr</foaf:surname>
                        <foaf:givenName>Philip H.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fitzgibbon</foaf:surname>
                        <foaf:givenName>Andrew W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>The Problem of Degeneracy in Structure and Motion Recovery from Uncalibrated Image Sequences</dc:title>
        <dcterms:abstract>The aim of this work is the recovery of 3D structure and camera projection matrices for each frame of an uncalibrated image sequence. In order to achieve this, correspondences are required throughout the sequence. A significant and successful mechanism for automatically establishing these correspondences is by the use of geometric constraints arising from scene rigidity. However, problems arise with such geometry guided matching if general viewpoint and general structure are assumed whilst frames in the sequence and/or scene structure do not conform to these assumptions. Such cases are termed degenerate.</dcterms:abstract>
        <dc:date>Август 1, 1999</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1023/A:1008140928553</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>27-44</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1573-1405">
        <prism:volume>32</prism:volume>
        <dc:title>International Journal of Computer Vision</dc:title>
        <dc:identifier>DOI 10.1023/A:1008140928553</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>International Journal of Computer Vision</dcterms:alternative>
        <dc:identifier>ISSN 1573-1405</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="#item_86">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>24</prism:volume>
                <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
                <dc:identifier>DOI 10.1109/34.993559</dc:identifier>
                <prism:number>4</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chiuso</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Favaro</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>Hailin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Soatto</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Structure from motion causally integrated over time</dc:title>
        <dc:date>2002</dc:date>
        <bib:pages>523-535</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_87">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:title>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2006.263</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Eade</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Drummond</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Scalable Monocular SLAM</dc:title>
        <dc:date>2006</dc:date>
        <bib:pages>469-476</bib:pages>
    </rdf:Description>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0262885612000248">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0262-8856"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Strasdat</foaf:surname>
                        <foaf:givenName>Hauke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel</foaf:surname>
                        <foaf:givenName>J. M. M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>Andrew J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>SLAM</dc:subject>
        <dc:subject>Bundle adjustment</dc:subject>
        <dc:subject>EKF</dc:subject>
        <dc:subject>Information filter</dc:subject>
        <dc:subject>Monocular vision</dc:subject>
        <dc:subject>Stereo vision</dc:subject>
        <dc:subject>Structure from motion</dc:subject>
        <dc:title>Visual SLAM: Why filter?</dc:title>
        <dcterms:abstract>While the most accurate solution to off-line structure from motion (SFM) problems is undoubtedly to extract as much correspondence information as possible and perform batch optimisation, sequential methods suitable for live video streams must approximate this to fit within fixed computational bounds. Two quite different approaches to real-time SFM – also called visual SLAM (simultaneous localisation and mapping) – have proven successful, but they sparsify the problem in different ways. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods retain the optimisation approach of global bundle adjustment, but computationally must select only a small number of past frames to process. In this paper we perform a rigorous analysis of the relative advantages of filtering and sparse bundle adjustment for sequential visual SLAM. In a series of Monte Carlo experiments we investigate the accuracy and cost of visual SLAM. We measure accuracy in terms of entropy reduction as well as root mean square error (RMSE), and analyse the efficiency of bundle adjustment versus filtering using combined cost/accuracy measures. In our analysis, we consider both SLAM using a stereo rig and monocular SLAM as well as various different scenes and motion patterns. For all these scenarios, we conclude that keyframe bundle adjustment outperforms filtering, since it gives the most accuracy per unit of computing time.</dcterms:abstract>
        <dc:date>2012</dc:date>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0262885612000248</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>65-77</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0262-8856">
        <prism:volume>30</prism:volume>
        <dc:title>Image and Vision Computing</dc:title>
        <dc:identifier>DOI https://doi.org/10.1016/j.imavis.2012.02.009</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 0262-8856</dc:identifier>
    </bib:Journal>
    <rdf:Description rdf:about="urn:isbn:978-3-540-88688-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-3-540-88688-4</dc:identifier>
                <dc:title>Computer Vision – ECCV 2008</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berlin, Heidelberg</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Berlin Heidelberg</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Klein</foaf:surname>
                        <foaf:givenName>Georg</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murray</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forsyth</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torr</foaf:surname>
                        <foaf:givenName>Philip</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dc:title>Improving the Agility of Keyframe-Based SLAM</dc:title>
        <dcterms:abstract>The ability to localise a camera moving in a previously unknown environment is desirable for a wide range of applications. In computer vision this problem is studied as monocular SLAM. Recent years have seen improvements to the usability and scalability of monocular SLAM systems to the point that they may soon find uses outside of laboratory conditions. However, the robustness of these systems to rapid camera motions (we refer to this quality as agility) still lags behind that of tracking systems which use known object models. In this paper we attempt to remedy this. We present two approaches to improving the agility of a keyframe-based SLAM system: Firstly, we add edge features to the map and exploit their resilience to motion blur to improve tracking under fast motion. Secondly, we implement a very simple inter-frame rotation estimator to aid tracking when the camera is rapidly panning – and demonstrate that this method also enables a trivially simple yet effective relocalisation method. Results show that a SLAM system combining points, edge features and motion initialisation allows highly agile tracking at a moderate increase in processing time.</dcterms:abstract>
        <dc:date>2008</dc:date>
        <bib:pages>802–815</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_90">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2011.6094588</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pirker</foaf:surname>
                        <foaf:givenName>Katrin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rüther</foaf:surname>
                        <foaf:givenName>Matthias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bischof</foaf:surname>
                        <foaf:givenName>Horst</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>CD SLAM - continuous localization and mapping in a dynamic world</dc:title>
        <dc:date>2011</dc:date>
        <bib:pages>3990-3997</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_91">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2013 IEEE International Conference on Robotics and Automation</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2013.6631246</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Shiyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chandraker</foaf:surname>
                        <foaf:givenName>Manmohan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guest</foaf:surname>
                        <foaf:givenName>Clark C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Parallel, real-time monocular visual odometry</dc:title>
        <dc:date>2013</dc:date>
        <bib:pages>4698-4705</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_92">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</dc:title>
                <dc:identifier>DOI 10.1109/ISMAR.2012.6402537</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Kwang-Ting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>LDB: An ultra-fast feature for scalable Augmented Reality on mobile devices</dc:title>
        <dc:date>2012</dc:date>
        <bib:pages>49-57</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_93">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2011 IEEE International Conference on Robotics and Automation</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2011.5979949</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kümmerle</foaf:surname>
                        <foaf:givenName>Rainer</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grisetti</foaf:surname>
                        <foaf:givenName>Giorgio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Strasdat</foaf:surname>
                        <foaf:givenName>Hauke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Konolige</foaf:surname>
                        <foaf:givenName>Kurt</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Burgard</foaf:surname>
                        <foaf:givenName>Wolfram</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>G&lt;sup&gt;2&lt;/sup&gt;o: A general framework for graph optimization</dc:title>
        <dc:date>2011</dc:date>
        <bib:pages>3607-3613</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_94">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2012.6385773</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sturm</foaf:surname>
                        <foaf:givenName>Jürgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Engelhard</foaf:surname>
                        <foaf:givenName>Nikolas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Endres</foaf:surname>
                        <foaf:givenName>Felix</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Burgard</foaf:surname>
                        <foaf:givenName>Wolfram</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cremers</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>A benchmark for the evaluation of RGB-D SLAM systems</dc:title>
        <dc:date>2012</dc:date>
        <bib:pages>573-580</bib:pages>
    </rdf:Description>
    <bib:Article rdf:about="https://doi.org/10.1177/0278364909103911">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>28</prism:volume>
                <dc:title>The International Journal of Robotics Research</dc:title>
                <dc:identifier>DOI 10.1177/0278364909103911</dc:identifier>
                <prism:number>5</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smith</foaf:surname>
                        <foaf:givenName>Mike</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baldwin</foaf:surname>
                        <foaf:givenName>Ian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Churchill</foaf:surname>
                        <foaf:givenName>Winston</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paul</foaf:surname>
                        <foaf:givenName>Rohan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Newman</foaf:surname>
                        <foaf:givenName>Paul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>The New College Vision and Laser Data Set</dc:title>
        <dcterms:abstract>In this paper we present a large dataset intended for use in mobile robotics research. Gathered from a robot driving several kilometers through a park and campus, it contains a five-degree-of-freedom dead-reckoned trajectory, laser range/reflectance data and 20 Hz stereoscopic and omnidirectional imagery. All data is carefully timestamped and all data logs are in human readable form with the images in standard formats. We provide a set of tools to access the data and detailed tagging and segmentations to facilitate its use.</dcterms:abstract>
        <dc:date>2009</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1177/0278364909103911</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>_eprint: https://doi.org/10.1177/0278364909103911</dc:description>
        <bib:pages>595-599</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="https://doi.org/10.1177/0278364913491297">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>32</prism:volume>
                <dc:title>The International Journal of Robotics Research</dc:title>
                <dc:identifier>DOI 10.1177/0278364913491297</dc:identifier>
                <prism:number>11</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Geiger</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lenz</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stiller</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Urtasun</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Vision meets robotics: The KITTI dataset</dc:title>
        <dcterms:abstract>We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10–100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.</dcterms:abstract>
        <dc:date>2013</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1177/0278364913491297</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>_eprint: https://doi.org/10.1177/0278364913491297</dc:description>
        <bib:pages>1231-1237</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="https://doi.org/10.1007/s11263-008-0152-6">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>81</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-008-0152-6</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>International Journal of Computer Vision</dcterms:alternative>
                <dc:identifier>ISSN 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lepetit</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moreno-Noguer</foaf:surname>
                        <foaf:givenName>Francesc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fua</foaf:surname>
                        <foaf:givenName>Pascal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>EPnP: An Accurate O(n) Solution to the PnP Problem</dc:title>
        <dcterms:abstract>We propose a non-iterative solution to the PnP problem—the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences—whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n5) or even O(n8), without being more accurate. Our method is applicable for all n≥4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12×12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed-form solution can be used to initialize a Gauss-Newton scheme, which improves accuracy with negligible amount of additional time. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.</dcterms:abstract>
        <dc:date>Июль 19, 2008</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s11263-008-0152-6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>155</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="http://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-4-4-629">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>4</prism:volume>
                <dc:title>J. Opt. Soc. Am. A</dc:title>
                <dc:identifier>DOI 10.1364/JOSAA.4.000629</dc:identifier>
                <prism:number>4</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Horn</foaf:surname>
                        <foaf:givenName>Berthold K. P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>Composite materials</dc:subject>
        <dc:subject>Photogrammetry</dc:subject>
        <dc:subject>Reflection</dc:subject>
        <dc:title>Closed-form solution of absolute orientation using unit quaternions</dc:title>
        <dcterms:abstract>Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 × 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points.</dcterms:abstract>
        <dc:date>1987</dc:date>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-4-4-629</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: OSA</dc:description>
        <bib:pages>629–642</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="#item_99">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>30</prism:volume>
                <dc:title>IEEE Transactions on Robotics</dc:title>
                <dc:identifier>DOI 10.1109/TRO.2013.2279412</dc:identifier>
                <prism:number>1</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Endres</foaf:surname>
                        <foaf:givenName>Felix</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hess</foaf:surname>
                        <foaf:givenName>Jürgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sturm</foaf:surname>
                        <foaf:givenName>Jürgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cremers</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Burgard</foaf:surname>
                        <foaf:givenName>Wolfram</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>3-D Mapping With an RGB-D Camera</dc:title>
        <dc:date>2014</dc:date>
        <bib:pages>177-187</bib:pages>
    </bib:Article>
    <rdf:Description rdf:about="#item_100">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2011 International Conference on Computer Vision</dc:title>
                <dc:identifier>DOI 10.1109/ICCV.2011.6126513</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Newcombe</foaf:surname>
                        <foaf:givenName>Richard A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lovegrove</foaf:surname>
                        <foaf:givenName>Steven J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>Andrew J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>DTAM: Dense tracking and mapping in real-time</dc:title>
        <dc:date>2011</dc:date>
        <bib:pages>2320-2327</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="#item_101">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2011 IEEE Intelligent Vehicles Symposium (IV)</dc:title>
                <dc:identifier>DOI 10.1109/IVS.2011.5940546</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lovegrove</foaf:surname>
                        <foaf:givenName>Steven</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>Andrew J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ibañez-Guzmán</foaf:surname>
                        <foaf:givenName>Javier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Accurate visual odometry from a rear parking camera</dc:title>
        <dc:date>2011</dc:date>
        <bib:pages>788-793</bib:pages>
    </rdf:Description>
    <rdf:Description rdf:about="urn:isbn:978-3-540-44480-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-3-540-44480-0</dc:identifier>
                <dc:title>Vision Algorithms: Theory and Practice</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berlin, Heidelberg</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Berlin Heidelberg</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torr</foaf:surname>
                        <foaf:givenName>P. H. S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Triggs</foaf:surname>
                        <foaf:givenName>Bill</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szeliski</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dc:title>Feature Based Methods for Structure and Motion Estimation</dc:title>
        <dcterms:abstract>This report is a brief overview of the use of “feature based” methods in structure and motion computation. A companion paper by Irani and Anandan [16] reviews “direct” methods.</dcterms:abstract>
        <dc:date>2000</dc:date>
        <bib:pages>278–294</bib:pages>
    </rdf:Description>
    <z:Collection rdf:about="#collection_1">
        <dc:title>lib</dc:title>
        <dcterms:hasPart rdf:resource="#item_1"/>
        <dcterms:hasPart rdf:resource="#item_2"/>
        <dcterms:hasPart rdf:resource="#item_3"/>
        <dcterms:hasPart rdf:resource="#item_4"/>
        <dcterms:hasPart rdf:resource="#item_5"/>
        <dcterms:hasPart rdf:resource="#item_6"/>
        <dcterms:hasPart rdf:resource="#item_11"/>
        <dcterms:hasPart rdf:resource="#item_12"/>
        <dcterms:hasPart rdf:resource="#item_13"/>
        <dcterms:hasPart rdf:resource="#item_14"/>
        <dcterms:hasPart rdf:resource="#item_15"/>
        <dcterms:hasPart rdf:resource="#item_16"/>
        <dcterms:hasPart rdf:resource="#item_17"/>
        <dcterms:hasPart rdf:resource="#item_18"/>
        <dcterms:hasPart rdf:resource="#item_19"/>
        <dcterms:hasPart rdf:resource="#item_20"/>
        <dcterms:hasPart rdf:resource="#item_21"/>
        <dcterms:hasPart rdf:resource="#item_22"/>
        <dcterms:hasPart rdf:resource="#item_23"/>
        <dcterms:hasPart rdf:resource="#item_24"/>
        <dcterms:hasPart rdf:resource="#item_25"/>
        <dcterms:hasPart rdf:resource="#item_26"/>
        <dcterms:hasPart rdf:resource="#item_27"/>
        <dcterms:hasPart rdf:resource="#item_28"/>
        <dcterms:hasPart rdf:resource="#item_29"/>
        <dcterms:hasPart rdf:resource="#item_30"/>
        <dcterms:hasPart rdf:resource="#item_31"/>
        <dcterms:hasPart rdf:resource="#item_32"/>
        <dcterms:hasPart rdf:resource="#item_33"/>
        <dcterms:hasPart rdf:resource="#item_34"/>
        <dcterms:hasPart rdf:resource="#item_35"/>
        <dcterms:hasPart rdf:resource="#item_36"/>
        <dcterms:hasPart rdf:resource="#item_37"/>
        <dcterms:hasPart rdf:resource="#item_38"/>
        <dcterms:hasPart rdf:resource="#item_39"/>
        <dcterms:hasPart rdf:resource="#item_40"/>
        <dcterms:hasPart rdf:resource="#item_41"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-540-67973-8%20978-3-540-44480-0"/>
        <dcterms:hasPart rdf:resource="https://www.emerald.com/insight/content/doi/10.1108/k.2001.30.9_10.1333.2/full/html"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/1640781/"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4244-1749-0"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/6202705/"/>
        <dcterms:hasPart rdf:resource="http://photonics.pl/PLP/index.php/letters/article/view/13-9"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-0-262-51681-5"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-319-10604-5%20978-3-319-10605-2"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4244-6674-0"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4577-1102-2%20978-1-4577-1101-5%20978-1-4577-1100-8"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4799-3685-4"/>
        <dcterms:hasPart rdf:resource="#item_60"/>
        <dcterms:hasPart rdf:resource="https://linkinghub.elsevier.com/retrieve/pii/S0921889009000876"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72819-077-8"/>
        <dcterms:hasPart rdf:resource="https://www.frontiersin.org/articles/10.3389/fenrg.2021.803631/full"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72815-859-4"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s10846-018-0913-6"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-030-23806-3%20978-3-030-23807-0"/>
        <dcterms:hasPart rdf:resource="https://iopscience.iop.org/article/10.1088/1742-6596/1693/1/012068"/>
        <dcterms:hasPart rdf:resource="http://journals.sagepub.com/doi/10.1177/0278364910385483"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-0-7695-2597-6"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-642-15560-4%20978-3-642-15561-1"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-540-33832-1%20978-3-540-33833-8"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/11744023_32"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1023/B:VISI.0000029664.99615.94"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/4160954/"/>
        <dcterms:hasPart rdf:resource="#item_79"/>
        <dcterms:hasPart rdf:resource="#item_80"/>
        <dcterms:hasPart rdf:resource="#item_81"/>
        <dcterms:hasPart rdf:resource="#item_82"/>
        <dcterms:hasPart rdf:resource="#item_83"/>
        <dcterms:hasPart rdf:resource="https://royalsocietypublishing.org/doi/10.1098/rspb.1986.0030"/>
        <dcterms:hasPart rdf:resource="https://doi.org/10.1023/A:1008140928553"/>
        <dcterms:hasPart rdf:resource="#item_86"/>
        <dcterms:hasPart rdf:resource="#item_87"/>
        <dcterms:hasPart rdf:resource="https://www.sciencedirect.com/science/article/pii/S0262885612000248"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-540-88688-4"/>
        <dcterms:hasPart rdf:resource="#item_90"/>
        <dcterms:hasPart rdf:resource="#item_91"/>
        <dcterms:hasPart rdf:resource="#item_92"/>
        <dcterms:hasPart rdf:resource="#item_93"/>
        <dcterms:hasPart rdf:resource="#item_94"/>
        <dcterms:hasPart rdf:resource="https://doi.org/10.1177/0278364909103911"/>
        <dcterms:hasPart rdf:resource="https://doi.org/10.1177/0278364913491297"/>
        <dcterms:hasPart rdf:resource="https://doi.org/10.1007/s11263-008-0152-6"/>
        <dcterms:hasPart rdf:resource="http://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-4-4-629"/>
        <dcterms:hasPart rdf:resource="#item_99"/>
        <dcterms:hasPart rdf:resource="#item_100"/>
        <dcterms:hasPart rdf:resource="#item_101"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-540-44480-0"/>
    </z:Collection>
</rdf:RDF>
