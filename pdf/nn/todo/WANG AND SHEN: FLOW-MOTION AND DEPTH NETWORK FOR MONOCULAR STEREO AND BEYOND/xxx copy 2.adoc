= Поток-движение и сеть глубины для монокулярного стерео и не только
// \ название {\ БОЛЬШОЙ \ bf
// Поток-движение и сеть глубины для монокулярного стерео и не только}
 
// \ author {Kaixuan Wang и Shaojie Shen% <-this% останавливает пробел
// \ thanks {Все авторы из ECE, HKUST, Гонконг, Китай. {\ tt \ small \ {kwangap, eeshaojie \} @ ust.hk}.
//}}
 
//% \ thanks {* Эта работа не поддерживалась ни одной организацией}% <-this% останавливает пробел
//% \ thanks {$ ^ {1} $ Альберт Автор с факультета электротехники, математики и компьютерных наук, Университет Твенте, 7500 AE Enschede, Нидерланды {\ tt \ small albert.author@papercept.net}} %
//% \ thanks {$ ^ {2} $ Бернард Д. Исследователи с факультета электротехники Государственного университета Райта, Дейтон, Огайо, 45435, США {\ tt \ small b.d.researcher@ieee.org}}}%
 
//% \ resetconfstandarts
//% ----------------------------------------------- ---------------------------
 
// \ begin {document}
 
 
// \ maketitle
//% \ thispagestyle {scrheadings}% заставляет первую страницу также иметь фут и строку заголовка
// \ thispagestyle {empty}% заставляет первую страницу не иметь заголовка
// \ стиль страницы {пусто}
 
//% \ ieeefootline {Workshop on Latex Style Files \\ International Conference on Latex 2014, Las Vegas, NV, USA}% создает футлайн
 
//% \ ieeeheadline {Workshop on Latex Style Files \\ International Conference on Latex 2014, Las Vegas, NV, USA}% создает заголовок
 
 
// %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
// \ begin {abstract}

== аннотация

Мы предлагаем основанный на обучении метод \ footnote {https://github.com/HKUST-Aerial-Robotics/Flow-Motion-Depth}, который решает проблему монокулярного стерео и может быть расширен для объединения информации о глубине из нескольких целевых кадров. Учитывая два неограниченных изображения с монокулярной камеры с известной внутренней калибровкой, наша сеть оценивает относительные позы камеры и карту глубины исходного изображения. Основной вклад предлагаемого метода состоит из трех частей. Во-первых, сеть адаптирована для статических сцен, которые совместно оценивают оптический поток и движение камеры. Посредством совместной оценки пространство поиска оптического потока постепенно уменьшается, что приводит к эффективной и точной оценке потока. Во-вторых, предлагается новый слой триангуляции для кодирования предполагаемого оптического потока и движения камеры, избегая при этом общих числовых проблем, вызванных эпиполярностью. В-третьих, помимо оценки глубины с двумя ракурсами, мы дополнительно расширяем вышеупомянутые сети, чтобы объединить информацию о глубине из нескольких целевых изображений и оценить карту глубины исходного изображения. Чтобы еще больше принести пользу исследовательскому сообществу, мы представляем инструменты для создания фотореалистичных наборов данных на основе движения, чтобы глубокие сети можно было хорошо обучить и оценить. Предлагаемый метод сравнивается с предыдущими методами и позволяет получить самые современные результаты за меньшее время. Изображения из реальных приложений и Google Планета Земля используются для демонстрации обобщающей способности метода.
\ end {abstract}

//% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
== {ВВЕДЕНИЕ}

Из-за большого количества информации в изображениях, структура из движения (SfM) имеет жизненно важное значение для компьютерного зрения и робототехники. Учитывая набор неограниченных изображений, SfM стремится оценить карты глубины и относительные позы камеры. Традиционные системы, например, COLMAP ~ \ cite {sfm_revist, schonberger_2016}, сначала оценивают относительные положения камер, находя соответствия разреженных характерных точек, а затем используют оценочную позу камеры для расчета плотных карт глубины. Извлеченные разреженные элементы игнорируют другую информацию в изображениях, такую ​​как линии, и не влияют на следующую оценку плотной глубины. Приоритеты сцены, такие как структуры и формы объектов, также трудно интегрировать в конвейер традиционных методов.
 
Чтобы лучше использовать информацию об изображении и использовать априорные значения контекста, было предложено множество методов ~ \ cite {demon, ls_net, BA-Net} для решения задач монокулярного стерео (двухпозиционная SfM) с использованием сверточных нейронных сетей (CNN). DeMoN ~ \ cite {demon} - новаторская работа, которая сначала оценивает оптический поток, а затем разлагает его на карту глубины и позу камеры. Затем оптический поток, карты глубины и позы камеры итеративно уточняются цепочкой сетей кодер-декодер для обработки больших углов обзора. LS-Net ~ \ cite {ls_net} использует прогнозируемую карту глубины и позу камеры в качестве инициализации, чтобы итеративно минимизировать фотометрическую ошибку перепроецирования с помощью решающей программы, основанной на обучении. В отличие от LS-Net, где шаги обновления вычисляются сетью, BA-Net ~ \ cite {BA-Net} предлагает слой настройки пакета для прогнозирования коэффициента демпфирования алгоритма Левенберга-Марквардта ~ \ cite {numeric_optimization} и вычисляет обновление. Чтобы еще больше сократить пространство для оптимизации, BA-Net также параметризует карту глубины как линейную комбинацию одноэкранных прогнозируемых базовых карт за $ 128. Используя информацию обо всем изображении, вышеуказанные методы создают надежные позы камеры и плавные карты глубины. Хотя эти методы достигают впечатляющих результатов по сравнению с традиционными методами, для сходимости им требуется несколько итераций (например, 15 итераций в LS-Net и BA-Net), и большинство методов (например, LS-Net и DeMoN) оценивают карту глубины, используя только одну цель. Рамка.
 
// \ begin {figure} [t]
   
// \ begin {center}
       
// \ includegraphics [width = 1.0 \ linewidth] {figs / multiview.pdf}
   
// \ конец {центр}
   
// \ vspace {-0,4 см}
   
\ caption {Иллюстрация предлагаемого метода. Учитывая несколько изображений с движущейся монокулярной камеры, сеть потока-движения (зеленая на рисунке) сначала оценивает оптический поток и положение камеры между исходным изображением и каждым целевым изображением. Предполагаемый поток и движение дополнительно объединяются сетью глубины (серой на рисунке) для вычисления карты глубины исходного изображения.}
   
// \ vspace {-0,6 см}
   
// \ label {fig: multiview_example}
// \ end {рисунок}
 
В этом письме мы улучшаем как эффективность, так и точность современного состояния за счет включения знаний в предметной области и дальнейшего расширения метода для объединения нескольких глубинных данных. Первый вклад нашей работы - это совместная оценка оптического потока и поз камеры. Мы наблюдаем, что в задачах монокулярного стерео оптический поток между многовидовыми изображениями вызван эго-движением движущейся камеры в статических сценах, так что оптический поток ограничен вдоль эпиполярных линий. Совместно рассматривая оптический поток и позы камеры, можно постепенно сокращать пространство поиска пикселей, повышая как эффективность, так и точность. Предлагается новый слой триангуляции для кодирования предполагаемого оптического потока и движения камеры без численных проблем, вызванных неограниченными движениями камеры. Закодированная информация из триангуляционного слоя используется для оценки карты глубины исходного изображения. Во многих приложениях исходное изображение просматривается несколькими целевыми изображениями. Помимо проблемы двух представлений, мы дополнительно расширяем сети, чтобы объединить информацию о глубине из нескольких целевых кадров. Информация о глубине из разных пар изображений объединяется средними слоями и затем используется для прогнозирования карты глубины. На рисунке ~ \ ref {fig: multiview_example} показан рабочий процесс нашего метода оценки оптического потока, положения камеры и карты глубины для нескольких изображений. Используя многовидовые наблюдения, можно создавать надежные и точные карты глубины.
 
Обучение и оценка основанных на обучении методов SfM требует большого количества изображений с позициями наземной камеры и картами глубины. Существующие наборы данных, например, SUN3D ~ \ cite {sun3d} и Scenes11 ~ \ cite {demon}, содержат либо изображения низкого качества с камер RGB-D, либо нефотореалистичные синтетические изображения. Чтобы обучить и оценить предлагаемые нами сети, мы разрабатываем инструменты, которые могут создавать неограниченное количество высококачественных фотореалистичных изображений с наземными картами глубины и положениями камеры из игры Grand Theft Auto V (GTA5). В интересах сообщества компьютерного зрения мы выпускаем инструменты и сгенерированные наборы данных в виде открытого исходного кода.
 
Подводя итог, можно сказать, что письмо состоит из следующих элементов:
\ begin {itemize}
   
\ item Сеть, которая совместно оценивает оптический поток и позы камеры для двух изображений. При предполагаемых позах камеры оптический поток ограничивается эпиполярными линиями, так что поток может быть регуляризован, а пространство поиска сокращается.
   
\ item Новый слой триангуляции, который кодирует предполагаемый оптический поток и положение камеры, так что сеть глубины может триангулировать глубину каждого пикселя без числовых проблем.
   
\ item Сеть глубины дополнительно расширена, чтобы объединить информацию о глубине (например, поток и движение) из нескольких пар изображений. Путем объединения нескольких наблюдений можно более точно и надежно оценить глубину исходного изображения.
   
\ item Инструменты с открытым исходным кодом для настройки неограниченного количества фотореалистичных синтетических изображений с различными дневными, внутренними параметрами и т. д. Извлеченные изображения служат в качестве дополнительного набора данных для обучения и оценки основанных на обучении методов SfM.
\ end {itemize}


== {Сопутствующие работы}
 
// \ begin {рисунок *} [h]
// \ vspace {0,3 см}
// \ begin {center}
// \ includegraphics [width = 0.9 \ linewidth] {figs / architecture.pdf}
// \ конец {центр}
// \ vspace {-0,3 см}
\ caption {Архитектура предлагаемой сети движения потока и глубинной сети. Здесь для простоты показана только архитектура с двумя представлениями. Расширение для объединения нескольких данных о глубине показано на рисунке ~ \ ref {fig: multiview_support} и обсуждается в разделе ~ \ ref {multiframe_sfm}. Сеть потока-движения совместно оценивает оптический поток и позы камеры, а сеть глубины триангулирует глубину каждого пикселя в исходном изображении. Хотя обе сети основаны на простых архитектурах кодер-декодер, предлагаемая совместная оценка (Раздел ~ \ ref {flow_motion_net}) и уровень триангуляции (Раздел ~ \ ref {depth_net}) обеспечивают высококачественную и эффективную оценку.}
   
// \ label {fig: architecture}
   
// \ vspace {-0,5 см}
// \ end {рисунок *}
 
В этом разделе мы описываем связанную работу с использованием нейронных сетей для оценки положений камеры и карт глубины для двух или более изображений.
 
DeMoN ~ \ cite {demon} - это новаторская работа, которая совместно оценивает карты глубины и позы камеры для изображений с двумя ракурсами. Чтобы эффективно использовать наблюдения с двумя ракурсами, DeMoN адаптирует FlowNetS ~ \ cite {flownet} для первой оценки оптического потока между двумя изображениями, а затем разлагает поток на позы камеры и карты глубины. Чтобы еще больше улучшить качество, DeMoN итеративно уточняет оптический поток, положение камеры и карту глубины, используя две сети кодер-декодер, и, наконец, повышает дискретизацию карты глубины до более высокого разрешения.
 
CodeSLAM ~ \ cite {code_slam} и BA-Net ~ \ cite {BA-Net} параметризуют карты глубины как компактные представления, так что и движение камеры, и карта глубины могут быть решены явно с помощью классических методов оптимизации. CodeSLAM использует автокодировщик и декодер для представления карты глубины как функции соответствующего изображения и неизвестного кода. Неизвестный код может быть решен совместно с позой камеры за счет минимизации фотометрической ошибки и геометрической ошибки. Используя гибкость классической оптимизации, CodeSLAM может одновременно оценивать несколько карт глубины и положений камеры. Чтобы сделать представление глубины подходящим для задач SfM, BA-Net встраивает настройку пакета в качестве дифференцируемого слоя в сеть, и весь процесс можно обучать от начала до конца. В отличие от CodeSLAM и BA-Net, LS-Net ~ \ cite {ls_net} обучает CNN в качестве решателя наименьших квадратов для обновления поз камеры и значений глубины. Начиная с инициализированных карт глубины и поз камеры, эти методы требуют нескольких итераций для схождения.
 
% MRFlow ~ \ cite {mr_flow} явно разделяет сцену на жесткие и нежесткие части. Глубина и эго-движение для жестких частей могут быть решены с помощью метода плоскость + параллакс ~ \ cite {p + p}. Оптический поток жестких частей уточняется с использованием предполагаемой глубины и эго-движения.
 
Было предложено множество подходов для решения многовидового стерео или слежения за камерой с использованием нейронных сетей. Учитывая несколько изображений с известными позами камеры и внутренней калибровкой, DeepMVS ~ \ cite {DeepMVS} генерирует объемы затрат с использованием изученных карт характеристик, а затем оценивает карту несоответствия путем объединения нескольких объемов затрат. MVDepthNet ~ \ cite {mvdepthnet}, DPSNet ~ \ cite {DPSNet} и MVSNet ~ \ cite {MVSNet, RMVSNet} решают ту же задачу реконструкции, но отличаются расчетом объемов затрат и структурой сетей. С другой стороны, учитывая ключевой кадр RGB-D, DeepTAM ~ \ cite {deeptam} постепенно отслеживает позу камеры с использованием синтетических точек обзора и может дополнительно оценить карту глубины отслеживаемого кадра.
 
Здесь мы предлагаем метод, который отличается от всех упомянутых выше методов монокулярного стерео. Основное отличие состоит в том, что наш метод не итеративно уточняет оценку, а скорее генерирует результаты, используя только один прямой проход в сети потока-движения и сети глубины. Ключом к повышению эффективности и качества является совместная оценка оптического потока и движения камеры. Высококачественный оптический поток напрямую устанавливает точные соответствия плотных пикселей между изображениями, обеспечивая точную триангуляцию по глубине. Кроме того, предлагаемый метод может быть расширен для оценки карты глубины исходного изображения путем объединения информации из нескольких целевых изображений.
 
== {Сетевая архитектура}
 
Как показано на рисунке ~ \ ref {fig: architecture}, предлагаемый метод состоит из двух сетей: одной сети потока-движения и одной глубинной сети. Учитывая исходное изображение $ I_ {s} $ и целевое изображение $ I_ {t} $ статической сцены, сеть потока-движения оценивает оптический поток между двумя изображениями и относительную позу камеры от грубого к точному. . При оценке поз камеры пространство поиска оптического потока может постепенно сокращаться вдоль эпиполярной линии. Более того, проблема апертуры в оптическом потоке может быть уменьшена за счет ограничения эпиполярной линии. При оценке оптического потока и движения камеры значение глубины каждого пикселя может быть напрямую триангулировано. Однако шаг триангуляции не является численно стабильным вокруг эпиполярной ~ \ cite {multiview_geometry}. Вместо этого мы предлагаем слой триангуляции для кодирования информации о предполагаемом оптическом потоке и положениях камеры. Слой обрабатывается сетью глубины для оценки карты глубины $ I_s $. Сеть глубины также может быть расширена для объединения информации из нескольких целевых изображений. Когда исходное изображение наблюдается несколькими целевыми изображениями, карта глубины исходного изображения может быть решена путем объединения информации из пар исходный-целевой объект \ textit {all}.
 
В следующих разделах мы сначала объясняем конструкцию сети потока-движения, глубинной сети, которая обрабатывает двухкадровые задачи SfM. В разделе ~ \ ref {multiframe_sfm} сеть глубины дополнительно расширена для объединения нескольких данных глубины и оценки карты глубины исходного изображения.
 
=== {Flow-Motion Network} \ label {flow_motion_net}
 
Ряд работ ~ \ cite {flownet, flownet2, spy_net, pwc_net} продемонстрировали успех использования CNN для оценки плотного оптического потока между двумя изображениями. Предлагаемая сеть «поток-движение» имеет структуру, аналогичную современной PWC-Net ~ \ cite {pwc_net}, но адаптирована для статических сцен и совместно оценивает позы камеры.
 
Чтобы обеспечить устойчивость к изменениям освещения и угла обзора, входные изображения преобразуются в пирамиды функций L-уровня с помощью простой CNN. Карта объектов на $ i $ -м уровне, $ \ mathbf {f} ^ i $, обрабатывается тремя простыми сверточными слоями для создания карты объектов следующего уровня $ \ mathbf {f} ^ {i + 1} $ с размер уменьшен на 2 доллара. В этой работе используются $ L = 6 $ пирамидальных уровней, причем $ \ mathbf {f} ^ 0 $ является исходным 3-канальным изображением. $ \ mathbf {f} _s ^ {i} $ и $ \ mathbf {f} _t ^ {i} $ используются для обозначения карт функций $ i $ -го уровня $ I_s $ и $ I_t $ соответственно.
 
Оптический поток $ \ mathbf {w} $ оценивается от грубого до точного, чтобы справиться с большим смещением пикселей. На $ i $ -м уровне оптический поток $ \ mathbf {w} ^ {i + 1} $ с \ textit {i + 1} -го уровня сначала подвергается билинейному преобразованию в $ \ mathbf {w} ^ { i + 1} _ {up} $ как инициализация $ \ mathbf {w} ^ i $. Объем затрат $ \ mathbf {c} ^ {i} $ создается с использованием $ \ mathbf {f} _s ^ {i} $ и $ \ mathbf {f} _t ^ {i} $. Каждый элемент в объеме затрат представляет собой сходство функций между пикселем $ \ mathbf {x} _s $ в $ \ mathbf {f} _s ^ {i} $ и пикселем $ \ mathbf {x} _t $ в $ \ mathbf { f} _t ^ {i} $,
// \ begin {уравнение}
// \ mathbf {c} ^ {i} (\ mathbf {x} _s, \ mathbf {x} _t) = \ frac {1} {N_i} {(\ mathbf {f} _s ^ {i} (\ mathbf {x} _s))} ^ {T} \ mathbf {f} _t ^ {i} (\ mathbf {x} _t),
// \ end {уравнение}
а $ N_i $ - размерность функции $ \ mathbf {f} _s ^ {i} $. Из-за грубого к точному способу для расчета объема затрат требуется только подмножество пикселей в $ \ mathbf {f} _t ^ {i} $. Объем затрат $ \ mathbf {c} ^ {i} $, оптический поток с повышенной дискретизацией $ \ mathbf {w} ^ {i + 1} _ {up} $ и $ \ mathbf {f} _s ^ {i} $ равны используется для прогнозирования оптического потока $ \ mathbf {w} ^ {i} $ с использованием структуры DenseNet ~ \ cite {density_net}.

// \ begin {figure} [t]
// \ begin {center}
// \ includegraphics [width = 0.9 \ linewidth] {figs / cost_volume.pdf}
// \ конец {центр}
// \ vspace {-0,5 см}
// \ caption {Различия между вычислением объема затрат в PWC-Net (слева) и предлагаемой сетью потока-движения (справа). Для каждого пикселя $ \ mathbf {x} _s $ в $ \ mathbf {f} _s ^ i $ PWC-Net сопоставляет фиксированный набор пикселей (окрашенных в оранжевый цвет) вокруг $ \ mathbf {x} _s + \ mathbf {w } _ {up} ^ {i + 1} (\ mathbf {x} _s) $, чтобы сгенерировать объем затрат. С другой стороны, предлагаемая сеть потока-движения сначала упорядочивает исходный поток $ \ mathbf {w} _ {up} ^ {i + 1} (\ mathbf {x} _s) $ в $ \ mathbf {w} _ { вверх, r} ^ {i + 1} (\ mathbf {x} _s) $ и соответствует пикселям вокруг эпиполярной линии.}
// \ label {fig: cost_volume}
// \ vspace {-0,5 см}
// \ end {рисунок}
 
Вышеупомянутое построение стоимостного объема и оценка оптического потока повторяются от грубого к точному до тех пор, пока не будет оценен оптический поток с желаемым разрешением. В этой работе мы адаптируем и улучшаем описанные выше процессы, включая предварительную статическую сцену и совместно оценивая позу камеры.
 
На разных уровнях пирамиды несколько сверточных слоев и линейных слоев используются для прогнозирования положения исходного кадра по отношению к целевому кадру. Поза состоит из матрицы вращения $ R $ и вектора сдвига $ \ mathbf {t} $. С помощью оцененного движения камеры и откалиброванного внутреннего значения $ K $ вектор потока каждого пикселя может быть регуляризован вдоль соответствующей эпиполярной линии, а пространство поиска пикселей в объеме затрат может быть сужено.
 
В статических средах пиксель $ \ mathbf {x} $ в исходном изображении и его вектор оптического потока $ \ mathbf {w} (\ mathbf {x}) $ к целевому изображению имеют следующие отношения:
// \ begin {уравнение}
// \ begin {bmatrix} \ mathbf {x} + \ mathbf {w} (\ mathbf {x}) \\ 1 \ end {bmatrix} ^ TF \ begin {bmatrix} \ mathbf {x} \\ 1 \ end {bmatrix} = 0,
// \ end {уравнение}
// где $ F = K ^ {- T} \ mathbf {t} _ \ times R K ^ {- 1} $ - фундаментальная матрица. С помощью предполагаемой позы камеры оптический поток с повышенной дискретизацией каждого пикселя $ \ mathbf {w} ^ {i + 1} _ {up} (\ mathbf {x}) $ можно регуляризовать, проецируя соответствующую точку на эпиполярную линию,
// \ begin {уравнение}
// \ mathbf {w} ^ {i + 1} _ {up, r} (\ mathbf {x}) = \ frac {1} {e_x ^ 2 + e_y ^ 2}
// \ begin {bmatrix}
// \ scalebox {0.9} {$ x 'e_y ^ 2 - y' e_x e_y - e_x e_z $} \\ [0,15 см]
// \ scalebox {0.9} {$ y 'e_x ^ 2 - x' e_x e_y - e_y e_z $}
// \ end {bmatrix} - \ mathbf {x},
// \ end {уравнение}
// где \ scalebox {0.9} {$ \ small [e_x, e_y, e_z] ^ T = F [\ mathbf {x}, 1] ^ T $} и \ scalebox {0.9} {$ [x ', y' ] ^ T = \ mathbf {x} + \ mathbf {w} ^ {i + 1} _ {up} (\ mathbf {x}) $}.
 
Поскольку соответствующие пиксели ограничены эпиполярными линиями, нет необходимости сопоставлять пиксели, расположенные далеко от линий. Кроме того, проблема апертуры, когда соответствия пикселей не могут быть определены из-за неоднозначного сопоставления, может быть уменьшена путем включения ограничения эпиполярной линии. Однако эпиполярные линии, которые определяются по оценкам позы камеры, могут быть недостаточно точными, чтобы исключить все пиксели за пределами линий. Здесь мы постепенно уменьшаем пространство поиска от грубых пирамидальных уровней до тонких. На $ i $ -м уровне совпадающие пиксели пикселя $ \ mathbf {x} _s $ параметризуются как
// \ begin {уравнение}
// \ scalebox {0.9} {$
// \ начало {выровнено}
// \ mathbf {x} _t \ in \ {& \ mathbf {x} _s + \ mathbf {w} ^ {i + 1} _ {up, r} (\ mathbf {x} _s) + \ frac {h (e_y, -e_x) ^ T + v (e_x, e_y) ^ T} {e_x ^ 2 + e_y ^ 2} \ mid \\
// & h \ in [-h_ {max} ^ i, h_ {max} ^ i], v \ in [-v_ {max} ^ i, v_ {max} ^ i] \},
// \ конец {выровнено} $}
// \ end {уравнение}
// где $ h_ {max} ^ i $ обозначает диапазон поиска по эпиполярным линиям, а $ v_ {max} ^ i $ - диапазон поиска по вертикали к линиям. Всего $ (2h_ {max} ^ i + 1) (2v_ {max} ^ i + 1) $ пикселей сопоставляются для каждого пикселя на $ i $ -м уровне.
 
Рисунок ~ \ ref {fig: cost_volume} иллюстрирует разницу между вычислением объема затрат в PWC-Net и предлагаемой сетью потока-движения. При предварительной статической сцене и предполагаемом движении расчетный оптический поток может быть регуляризован, а размер объема затрат уменьшен, что приводит к эффективной оценке.
 
// \ begin {figure} [t]
// \ begin {center}
// \ includegraphics [width = 0.9 \ linewidth] {figs / degenerate_example.pdf}
// \ конец {центр}
// \ vspace {-0,5 см}
\ caption {Пример, демонстрирующий числовую устойчивость в триангуляции. $ O_s $ и $ O_t $ - оптические центры $ I_s $ и $ I_t $ соответственно. $ d_ {max} $ и $ d_ {min} $ - максимальная и минимальная глубина сцены. $ \ mathbf {e} _i $ - соответствующая эпиполярная линия пикселя $ \ mathbf {x} _i $. (a) В стерео конфигурациях глубину можно надежно вычислить, найдя соответствующую точку на $ \ mathbf {e} _0 $. (b) В задачах монокулярного стерео без ограничений эпиполярная линия $ \ mathbf {e} _1 $ точки $ \ mathbf {x} _1 $ (\ textit {эпиполярная точка}) вырождается в точку, поэтому глубина не наблюдается. Для пикселей около эпиполярной точки, таких как $ \ mathbf {x} _2 $, эпиполярная линия $ \ mathbf {e} _2 $ очень короткая, и результат подвержен шуму.}
\ label {fig: stereo_vs_mono}
\ vspace {-0,5 см}
\ end {рисунок}
 
=== {Глубина сети} \ label {depth_net}
 
Учитывая предполагаемый оптический поток $ \ mathbf {x} $ и положение камеры $ R $, $ \ mathbf {t} $, глубину пикселя $ d $ можно легко триангулировать, решив,
\ begin {Equation} \ label {triangulate}
\ mathbf {w} (\ mathbf {x}) + \ mathbf {x} = \ lambda (KRK ^ {- 1} [\ mathbf {x}, 1] ^ T \ cdot d + K \ mathbf {t}) ,
\ end {уравнение}
где $ \ lambda ([x, y, z] ^ T) = [x / z, y / z] ^ T $ - функция дегомогенизации. Однако на этом этапе триангуляции есть два недостатка. Во-первых, глубина решается независимо для каждого пикселя, таким образом игнорируются общая гладкость и априорные значения сцены. Во-вторых, пиксели вокруг эпиполярной области (проекция оптического центра целевого кадра на исходное изображение) не могут быть надежно триангулированы. На рисунке ~ \ ref {fig: stereo_vs_mono} показаны возможные численные проблемы при различных движениях камеры.
 
Для решения вышеуказанных проблем DeMoN использует сети для уточнения триангулированных карт глубины (с заданной глубиной, установленной на 0). Здесь вместо уточнения триангулированных карт глубины мы предлагаем восьмиканальный слой, который кодирует всю информацию для триангуляции. Слой называется триангуляционным слоем $ \ mathbf {tri} $, и для каждого пикселя $ \ mathbf {x} $,
\ begin {уравнение}
\ mathbf {tri} (\ mathbf {x}) = [\ mathbf {w} (\ mathbf {x}) + \ mathbf {x}, KRK ^ {- 1} [\ mathbf {x}, 1] ^ T , К \ mathbf {t}] ^ T.
\ end {уравнение}
 
Глубинная сеть - это сеть кодировщика-декодера, которая принимает в качестве входных данных слой триангуляции $ \ mathbf {tri} $, исходное изображение $ I_s $, расчетный оптический поток $ \ mathbf {w} $ и последний уровень сети потока-движения. для оценки карты глубины исходного изображения.
 
=== {Multiview Depth Fusion} \ label {multiframe_sfm}
 
// \ begin {figure} [t]
// \ begin {center}
// \ includegraphics [width = 0.9 \ linewidth] {figs / multiview_support.pdf}
// \ конец {центр}
// \ vspace {-0,3 см}
// \ caption {Расширение сети глубины для объединения нескольких данных о глубине. (а) Сеть оценки глубины с двумя ракурсами. (b) Удлинитель с несколькими глубинами слияния. \ Textit {сеть кодировщика с двумя представлениями} кодирует информацию о глубине каждой пары изображений в коды глубины $ \ mathbf {dc} _i $. Несколько кодов объединяются в $ \ mathbf {dc} '$, а \ textit {многовидовая сеть слияния} использует $ \ mathbf {dc}' $ для оценки карты глубины.}
// \ label {fig: multiview_support}
// \ vspace {-0,5 см}
// \ end {рисунок}
 
В реальных приложениях (например, в навигации роботов) глубина исходного изображения может быть решена с помощью нескольких целевых изображений. Здесь мы расширяем предлагаемые монокулярные стереосети с двумя ракурсами для объединения многовидовой информации. По сравнению с парами изображений с двумя ракурсами, изображения с несколькими ракурсами несут больше информации о структуре окружающей среды, поэтому объединенные карты глубины могут быть более надежными и точными. Однако объединение информации о глубине из многовидовых изображений нетривиально из-за произвольного количества пар изображений и различных масштабов глубины. В отличие от CodeSLAM, который объединяет информацию с помощью методов оптимизации, мы предлагаем объединить многоракурсную информацию с помощью обученной сети.
 
На рисунке ~ \ ref {fig: multiview_support} показано, как расширяется сеть глубины с двумя представлениями. Сеть глубины с двумя ракурсами, представленная в разделе ~ \ ref {depth_net}, разделена на две части: кодировщик с двумя ракурсами и слияние с несколькими ракурсами. Первая часть независимо кодирует слой триангуляции $ \ mathbf {tri} $ каждой пары изображений в коды глубины с разным разрешением $ \ mathbf {dc} $. Коды глубины из нескольких пар изображений, \ {$ \ mathbf {dc} _0 $, ..., $ \ mathbf {dc} _ {N-1} $ \}, объединяются посредством среднего объединения слоев. Объединенный код каждого пикселя $ \ mathbf {dc} '(\ mathbf {x}) $ рассчитывается как,
// \ begin {уравнение}
// \ mathbf {dc} '(\ mathbf {x}) = \ frac {1} {N} \ sum ^ {N-1} _ {i = 0} \ mathbf {dc} _i (\ mathbf {x} ).
// \ end {уравнение}

Использование уровней объединения для объединения информации использовалось во многих мультиэкранных стерео работах (например, DeepMVS ~ \ cite {DeepMVS}). В отличие от этих работ, мы используем несколько слоев объединения для объединения кодов глубины с разными разрешениями, чтобы сохранить как глобальную информацию, так и мелкие детали. Объединенная сеть использует объединенный код глубины $ \ mathbf {dc} '$ и исходное изображение $ I_s $ для оценки соответствующей карты глубины.
 
== {Сведения о сети}
 
=== {Оптический поток и движение камеры}
 
Пространство поиска при расчете объема затрат постепенно сокращается от грубого до мелкого. Сеть потока-движения оценивает оптический поток от уровня $ 5 $ до уровня $ 1 $. От уровня $ 5 $ до уровня $ 1 $ шаги поиска $ h_ {max} $ и $ v_ {max} $ устанавливаются равными $ \ {4,4,4,4,3 \} $ и $ \ {4,4,4,2,1 \} $ соответственно. На уровне $ 1 $ сопоставляется только $ 21 $ пикселя (в PWC-Net используется 81 $ пиксель). Потери оптического потока определяются как
// \ begin {уравнение}
// L_ {поток} = \ sum_ {l = 1} ^ {5} \ sum _ {\ mathbf {x}} {\ lVert \ mathbf {w} ^ l (\ mathbf {x}) - \ hat {\ mathbf {w}} ^ l (\ mathbf {x}) \ rVert} _2,
// \ end {уравнение}
// где $ \ hat {\ mathbf {w}} ^ l $ - соответствующий наземный оптический поток на $ l $ -м уровне.
 
// Вращение камеры $ \ mathbf {r} $ параметризовано как трехмерный вектор вращения: $ \ mathbf {r} = \ theta \ mathbf {v} $, где $ \ theta $ - угол поворота, а $ \ mathbf {v} $ - ось вращения. Подобно DeMoN, перемещение камеры $ \ mathbf {t} $ нормализуется как единичный вектор из-за ненаблюдаемого масштаба. Поскольку оптический поток на грубых разрешениях не может обеспечить точных соответствий пикселей, движение камеры оценивается от уровня 3 $ до уровня 1 $. При движении камеры наземного контроля $ \ hat {\ mathbf {r}} $ и $ \ hat {\ mathbf {t}} $ потеря движения составляет,
// \ begin {уравнение}
// L_ {движение} = \ sum_ {l = 1} ^ {3} {\ lVert \ mathbf {r} ^ l - \ hat {\ mathbf {r}} \ rVert} _2 + \ sum_ {l = 1} ^ {3} {\ lVert \ mathbf {t} ^ l - \ hat {\ mathbf {t}} \ rVert} _2.
// \ end {уравнение}
 
// === {Оценка глубины}
 
// Множественные карты глубины оцениваются сетью глубин с разными разрешениями (от уровня $ 3 $ до уровня $ 1 $). Мы принимаем параметризацию глубины из Eigen et al. ~ \ Cite {mono_depth_2014}, согласно которой выходом сети является глубина журнала: $ \ text {log} (d) \ in R $. Из-за неоднозначности масштаба в задачах SfM, масштабно-инвариантная ошибка глубины для каждого пикселя $ \ mathbf {x} $ рассчитывается как,
// \ begin {уравнение}
// d_ {e} ^ l (\ mathbf {x}) = \ text {log} (d ^ l) (\ mathbf {x}) + \ alpha ^ l - \ text {log} (\ hat {d ^ l}) (\ mathbf {x})
// \ end {уравнение}
// где $ \ hat {d} $ - это наземная карта истинной глубины, а \ scalebox {0.9} {$ \ alpha ^ l = \ frac {1} {N} \ sum _ {\ mathbf {x}} \ text { log} (\ hat {d ^ l}) (\ mathbf {x}) - \ text {log} (d ^ l) (\ mathbf {x}) $} масштабирует расчетные карты глубины. И ошибка глубины $ L_ {d} $, и ошибка градиента $ L_ {g} $ вычисляются для обучения сети триангуляции,
// \ begin {уравнение}
// \ scalebox {0.9} {$
// L_ {d} = \ sum_ {l = 1} ^ {3} \ sum _ {\ mathbf {x}} {\ lVert d_ {e} ^ l (\ mathbf {x}) \ rVert} _ {berHu} ,
// $}
// \ end {уравнение}
// \ begin {уравнение}
// \ scalebox {0.9} {$
// L_ {g} = \ sum_ {l = 1} ^ {3} \ sum _ {\ mathbf {x}} \ left | \ nabla_x d_ {e} ^ l (\ mathbf {x}) \ right | + \ влево | \ nabla_y d_ {e} ^ l (\ mathbf {x}) \ right |,
// $}
// \ end {уравнение}
// где $ {\ lVert \ cdot \ rVert} _ {berHu} $ - обратный Huber ~ \ cite {fcrn_mono_depth, berhu_loss}:
// \ begin {уравнение}
// \ scalebox {0.9} {$
// {\ lVert x \ rVert} _ {berHu} =
// \ begin {case}
// \ влево | х \ право | & \ text {if} \ left | х \ право | \ leq 1 \\
// x ^ 2 & \ text {if} \ left | х \ право | > 1
// \ end {case} $}.
// \ end {уравнение}
// Используя норму berHu, большие ошибки глубины наказываются нормой L2, а небольшие ошибки глубины также могут быть эффективно оптимизированы нормой L1.
 
 
== {Наборы данных}
=== {Набор данных DeMoN}
 
DeMoN предлагает набор наборов данных для обучения и оценки глубоких сетей. Набор данных содержит изображения из нескольких источников, таких как камеры RGB-D ~ \ cite {sun3d, sturm12iros}, многоракурсные результаты SfM ~ \ cite {sfm_revist, schonberger_2016, mve_2014, ummenhofer_2015} и синтетические изображения ~ \ cite {demon}. В общей сложности набор данных DeMoN содержит 57 $ k пар изображений для обучения и 354 $ пар для тестирования.
 
Хотя набор данных DeMoN широко использовался в предыдущих работах ~ \ cite {demon, ls_net, BA-Net}, он содержит несколько ограничений. Во-первых, карты глубины с камер RGB-D не синхронизируются с цветными изображениями и обеспечивают только измерения глубины менее 10 долларов США. Во-вторых, большинство положений камеры для реальных изображений рассчитывается с помощью методов, основанных на оптимизации, на которые могут влиять шумы изображения или резко выделяющиеся характеристики. Наконец, обработанные синтетические изображения в наборе данных не фотореалистичны. Все эти аспекты ограничивают производительность обученных сетей.
 
=== {Набор данных GTA-SfM}
 
// \ begin {figure} [t]
// \ begin {center}
// \ vspace {0,3 см}
// \ includegraphics [width = 0.85 \ linewidth] {figs / extract.pdf}
// \ конец {центр}
// \ vspace {-0,3 см}
\ caption {Примеры из набора данных GTA-SfM, включая различную погоду, время и сцены. Гибкость, позволяющая изменять среду и настройки камеры, повышает удобство использования набора данных в исследованиях глубокого обучения.}
// \ label {fig: извлечено}
// \ vspace {-0,5 см}
// \ end {рисунок}
 
Чтобы преодолеть ограничения в наборе данных DeMoN, мы предлагаем набор данных GTA-SfM в качестве дополнения. Набор данных визуализирован из GTA-V, игры с открытым миром с крупномасштабными моделями городов. Благодаря активному сообществу мы разрабатываем инструменты для извлечения неограниченного количества фотореалистичных изображений с картами глубины и положениями камеры. Извлеченные карты глубины обеспечивают измерения глубины для всех объектов на изображениях, включая мелкие структуры или отражающие поверхности. Мы извлекли пар изображений на сумму 71 тыс. Долларов для обучения и пары на сумму 5 тыс. Долларов для тестирования. Наборы данных для обучения и тестирования не имеют общих сцен. В отличие от набора данных DeMoN, одно исходное изображение может иметь несколько целевых изображений, поэтому можно протестировать многоракурсное слияние по глубине.
 
Аналогичный набор данных MVS-SYNTH выпущен DeepMVS ~ \ cite {DeepMVS} с использованием инструментов отладки графики. По сравнению с MVS-SYNTH, инструменты GTA-SfM могут свободно устанавливать угол обзора камеры, погоду и дневное время, так что разнообразие наборов данных и удобство использования улучшаются. Кроме того, траектория камеры вручную аннотируется, что камеры перемещаются с большим перемещением и вращением. На рисунке ~ \ ref {fig: extract} показаны образцы из предложенного набора данных.
 
== {эксперименты}
 
// \ begin {рисунок *} [t]
// \ begin {center}
// \ includegraphics [width = 1.0 \ linewidth] {figs / quality_compare.pdf}
// \ конец {центр}
// \ vspace {-0,4 см}
\ caption {Качественные результаты по базе данных DeMoN. Слева направо: исходное изображение, расчетный оптический поток, расчетная карта глубины, карта ошибок L1-rel и ошибка L1-rel карты глубины DeMoN. Карта ошибок имеет цветовую кодировку JET. Как показано, наш метод генерирует высококачественные карты оптического потока и глубины. С предлагаемым слоем триангуляции карты глубины имеют меньше ошибок L1-rel.}
// \ label {fig: quality_compare}
// \ vspace {-0,4 см}
// \ end {рисунок *}
 
В этом разделе мы подробно оцениваем производительность предложенной сети потока-движения и глубинной сети. Сначала мы сравним предложенную сеть с предыдущими работами ~ \ cite {demon, ls_net, BA-Net} на парах изображений с двумя представлениями с использованием набора данных DeMoN. Затем с помощью предложенного набора данных GTA-SfM оценивается эффективность глубинного совмещения. Эффективность предложенной совместной оценки потока и движения и триангуляционного слоя $ \ mathbf {tri} $ также продемонстрирована в исследовании абляции. Мы также демонстрируем обобщающую способность метода с использованием реальных изображений и изображений Google Earth.
 
=== {Оценочные показатели}
Для оценки расчетных карт движения камеры и глубины используются различные метрики. Мы следуем методу оценки, используемому в DeMoN. Ошибка вращения определяется относительным углом между предполагаемым вращением камеры и точным вращением земли. Из-за неоднозначности масштаба в задачах SfM ошибка трансляции определяется углом между нормализованными векторами трансляции. Для оценки глубины сначала оптимально масштабируется оценочная глубина $ d $ ~ \ cite {demon}, затем вычисляются три метрики глубины:
// \ begin {уравнение}
// \ scalebox {0.9} {$ \ text {L1-inv} (d, \ hat {d}) = \ frac {1} {N} \ sum_ \ mathbf {x} \ left | 1 / d (\ mathbf {x}) - 1 / \ hat {d} (\ mathbf {x}) \ right | $},
// \ end {уравнение}
// \ begin {уравнение}
// \ scalebox {0.9} {$ \ text {sc-inv} (d, \ hat {d}) = \ sqrt {\ frac {1} {N} \ sum_ \ mathbf {x} z (\ mathbf {x }) ^ 2 - \ frac {1} {N ^ 2} (\ sum_ \ mathbf {x} z (\ mathbf {x})) ^ 2} $},
// \ end {уравнение}
// \ begin {уравнение}
// \ scalebox {0.9} {$ \ text {L1-rel} (d, \ hat {d}) = \ frac {1} {N} \ sum_ \ mathbf {x} \ left | d (\ mathbf {x}) - \ hat {d} (\ mathbf {x}) \ right | / \ hat {d} (\ mathbf {x}) $},
// \ end {уравнение}
// где \ scalebox {0.8} {$ z (\ mathbf {x}) = \ text {log} (d (\ mathbf {x})) - \ text {log} (\ hat {d} (\ mathbf { x})) $}, а $ N $ - номер пикселя.

=== {Оценка с двух сторон}
 
// \ begin {table} [t]
// \ центрирование
// \ vspace {0,3 см}
// \ caption {Сравнение двух представлений задач}
// \ label {two_view_compare}
// \ includegraphics [width = 0.9 \ linewidth] {figs / two_view_compare.pdf}
// \ vspace {-0,7 см}
// \ конец {таблица}
 
Мы обучаем сеть потока-движения и сеть глубины, используя \ textit {only} набор данных DeMoN для честного сравнения. Обратите внимание, что DeMoN обучается с большим набором данных, включая другие синтетические изображения. В экспериментах размер изображений уменьшен до 320 $ \ times256 $. Сеть потока-движения была обучена для $ 750 000 шагов с помощью оптимизатора Adam ~ \ cite {adam}. С обученной сетью «поток-движение» глубинная сеть обучается за 260 тыс. $ Шагов. В соответствии с размером модели размер мини-партии устанавливается равным 16 долларам США для сети потока-движения и 24 долларам США для сети триангуляции. В эксперименте сравниваются как методы, основанные на обучении (DeMoN, LS-Net и BA-Net), так и классический метод. В DeMoN предлагается и оценивается классический метод, который решает позы камеры с помощью нормализованного 8-балльного алгоритма ~ \ cite {8_point} (с использованием функций SIFT) с последующей минимизацией ошибки перепроецирования. Карты глубины оцениваются с использованием стерео развертки плоскости и полуглобального сопоставления ~ \ cite {hirschmuller2008stereo}.
 
Таблица ~ \ ref {two_view_compare} и рисунок ~ \ ref {fig: quality_compare} показывают результаты сравнения глубины и движения. Благодаря совместной оценке движения потока и движения предложенный метод в большинстве случаев обеспечивает наилучшую оценку движения камеры. Предлагаемая глубинная сеть также обеспечивает стабильно лучшую производительность по сравнению с DeMoN. По сравнению с BA-Net, который итеративно уточняет результаты (всего 95 миллионов долларов), наш метод генерирует неизменно лучшие позы камеры и конкурентные карты глубины без каких-либо итераций (всего 42 миллиона долларов). Как показано на рисунке ~ \ ref {fig: quality_compare}, благодаря слою триангуляции, который кодирует геометрическую информацию, как близкие, так и удаленные объекты восстанавливаются точно.

=== {Оценка глубины слияния}
 
Поскольку набор данных DeMoN предоставляет только пары изображений с двумя ракурсами, мы используем предложенный набор данных GTA-SfM для обучения и оценки производительности многоэкранного слияния по глубине. Сначала мы обучаем сеть «поток-движение», используя пары изображений с двумя ракурсами, за 210 тысяч шагов, а затем обучаем расширенную многовидовую сеть слияния за 130 тысяч шагов.
 
Сначала мы оцениваем качество расчетных карт глубины, используя различное количество целевых изображений. Мы также сравниваем сетку глубины с DeepMVS ~ \ cite {DeepMVS}, который также обучается с использованием изображений из GTA5. DeepMVS принимает позы наземной камеры в качестве входных данных. Для каждого количества целевых изображений мы \ textit {случайным образом} выбираем пары по $ 300 и вычисляем среднюю ошибку глубины. Таблица ~ \ ref {gta_sfm_depth_map} показывает качество глубины при разном количестве целевых изображений. Очевидно, что качество глубины улучшается при увеличении количества изображений, что показывает эффективность многооконного слияния и соответствует опыту классических методов SfM. Мы также визуализируем расчетные карты глубины для качественного сравнения на рисунке ~ \ ref {fig: deepmvs}. Наш метод оценивает гладкие и подробные карты глубины, а DeepMVS оценивает дискретные карты глубины с выбросами.
 
// \ begin {table} [h]
// \ центрирование
// \ caption {Ошибка карты глубины в наборе данных GTA-SfM.}
// \ label {gta_sfm_depth_map}
// \ vspace {-0,2 см}
// \ scalebox {0.8} {
// \ begin {tabular} {| c | cc | cc | cc |}
// \ hline
// & \ multicolumn {6} {c |} {Ошибка глубины}
// \\ \ hline
// \ multirow {2} {*} {Просмотреть число} & \ multicolumn {2} {c |} {L1-inv (1e-3)} & \ multicolumn {2} {c |} {sc-inv} & \ multicolumn {2} {c |} {L1-rel} \\ \ cline {2-7}
// & Наши & DeepMVS & Наши & DeepMVS & Наши & DeepMVS \\ \ hline
       
// 2 и 6,19 и 16,6 и 0,213 и 0,526 и 0,145 и 0,766 \\
       
// 3 и 6,07 и 15,6 и 0,207 и 0,496 и 0,137 и 0,753 \\
// 4 и 5,36 и 15,1 и 0,192 и 0,475 и 0,124 и 0,735 \\
       
// 5 и 5,68 и 14,8 и 0,192 и 0,465 и 0,123 и 0,723 \\
       
// 6 & 4.86 & 14.8 & 0.181 & 0.464 & 0.114 & 0.729 \\ \ hline
       
// \ end {tabular}}
// \ vspace {-0,5 см}
// \ конец {таблица}
 
// \ begin {figure} [h]
// \ begin {center}
// \ includegraphics [width = 0.90 \ linewidth] {figs / deepmvs.pdf}
// \ конец {центр}
// \ vspace {-0,2 см}
\ caption {Качественное сравнение созданных карт глубины предложенным методом и DeepMVS. За каждым исходным изображением наблюдают 6 целевых изображений, а DeepMVS предоставляет позы наземной камеры.}
// \ vspace {-0,2 см}
// \ label {fig: deepmvs}
// \ end {рисунок}
 
=== {Исследование абляции}
 
Здесь мы изучаем эффективность вкладов: совместной оценки потока и движения и триангуляционного слоя.
 
\ textbf {Совместная оценка} Чтобы оценить важность ограничения эпиполярной линии и уменьшения пространства поиска, мы удаляем оценку положения камеры на средних уровнях, и движение камеры оценивается с использованием окончательной оценки потока. Без ограничения эпиполярной линии поиск в размере 81 $ пикселя (как и в PWC-Net) ищется на каждом уровне. Как показано в Таблице ~ \ ref {Joint_study}, совместная оценка улучшает \ textit {как} оптический поток, так и оценку положения камеры.
 
// \ begin {table} [h]
// \ центрирование
// \ caption {Эффективность совместной оценки движения потока}
// \ label {Joint_study}
// \ scalebox {1.0} {
// \ begin {tabular} {c | ccc}
// & Ошибка вращения и Ошибка перевода & Ошибка потока \\ \ hline
// исходный & 1.879 & 10.307 & 3.472 \\ \ hline
// без соединения & 2.043 & 11.703 & 3.567 \\ \ hline
// \ end {tabular}}
// \ vspace {-0,4 см}
// \ конец {таблица}
 
\ textbf {Слой триангуляции} Слой триангуляции предлагается для кодирования предполагаемого оптического потока и движения камеры без какой-либо числовой нестабильности. Чтобы продемонстрировать эффективность, мы заменяем слой триангуляции непосредственно триангулированной картой глубины. Подобно DeMoN ~ \ cite {demon}, значения NaN установлены в 0. Обе сети обучаются с той же сетью потока-движения, что и интерфейс, для эпох по 50 $. Сравнение показано в таблице ~ \ ref {no_tri}. С предлагаемым \ textbf {tri} глубинная сеть может лучше использовать предполагаемый оптический поток и позы камеры.
 
// \ begin {table} [h]
// \ центрирование
// \ caption {Эффективность слоя триангуляции.}
// \ label {no_tri}
// \ scalebox {1.0} {
// \ begin {tabular} {c | ccc}
// & L1-inv & sc-inv & L1-rel \\ \ hline
// исходный & 0,015 & 0,195 & 0,134 \\ \ hline
// без \ textbf {tri} & 0.017 & 0.200
// & 0,140 \\ \ hline
// \ end {tabular}}
// \ vspace {-0,4 см}
// \ конец {таблица}
 
=== {Возможность обобщения}
 
Для проверки обобщающей способности предложенного метода в дальнейшем мы используем метод для оценки карт глубины изображений из разных источников. На рисунке ~ \ ref {fig: real_world_dji} показаны приблизительные карты глубины изображений, снятых с помощью DJI Phantom 4 (на открытом воздухе) или портативной камеры (внутри помещения). На рисунке ~ \ ref {fig: real_world_google} показаны приблизительные карты глубины изображений из Google Earth. Карта глубины каждого исходного изображения объединяется из 5 или 6 целевых изображений. Поскольку предлагаемый метод сначала строит высококачественные соответствия пикселей, а затем триангулирует глубину каждого пикселя, он может эффективно использовать многовидовые наблюдения и хорошо обобщается на другие изображения. Подробнее в дополнительном материале.
 
// \ begin {figure} [t]
// \ begin {center}
// \ vspace {0,3 см}
// \ includegraphics [width = 0.95 \ linewidth] {figs / main_paper_dji.pdf}
// \ конец {центр}
// \ vspace {-0,5 см}
// \ caption {Сгенерировать предложенный метод для реальных изображений.}
// \ label {fig: real_world_dji}
// \ vspace {-0,3 см}
// \ end {рисунок}

// \ begin {figure} [t]
// \ begin {center}
// \ includegraphics [width = 0.95 \ linewidth] {figs / main_paper_google.pdf}
// \ конец {центр}
// \ vspace {-0,4 см}
// \ caption {Сгенерировать предложенный метод для изображений Google Планета Земля.}
// \ label {fig: real_world_google}
// \ vspace {-0,5 см}
// \ end {рисунок}

== {Заключение и дальнейшая работа}
В этом письме мы предлагаем сеть потока-движения и сеть глубины, которая может оценивать движение камеры и карту глубины с учетом нескольких движущихся стереоизображений. Обе сети тщательно спроектированы с учетом геометрических ограничений нескольких ракурсов между оптическим потоком, движением камеры и картами глубины. Мы дополнительно расширяем сеть глубины, чтобы объединить несколько данных о глубине в карту глубины. Чтобы расширить доступные наборы данных, предлагается инструмент с открытым исходным кодом для извлечения неограниченного количества фотореалистичных изображений с помощью поз камеры и карт глубины. В будущем мы планируем дальнейшее развитие метода за счет включения сетей графов, чтобы он мог одновременно оценивать все позы камеры и карты глубины с учетом набора изображений.
 
//% ----------------------------------------------- --------------------------
// {
// % \маленький
//% \ bibliographystyle {ieee}
// \ bibliographystyle {unsrt}
// \ библиография {egbib}
//}
 
// \ конец {документ}
