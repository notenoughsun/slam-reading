:imagesdir: images
:toc: preamble

:author: timur chikichev
:email: t.chikichev@navigine.ru

:pygments-style: Coderay

:toc: macro


DeepSLAM: надежная монокулярная SLAM-система с неконтролируемым глубоким обучением
// Жуйхао Ли, Сен Ван, член IEEE, и Дунбин Гу, старший член IEEE


Аннотация. В этой статье мы предлагаем DeepSLAM, новую систему неконтролируемого глубокого обучения, основанную на визуальной одновременной локализации и отображении (SLAM). Обучение DeepSLAM полностью бесконтрольно, так как оно требует только стереоизображения, а не аннотирования истинных поз. При его тестировании в качестве входных данных используется последовательность монокулярных изображений. Таким образом, это монокулярная парадигма SLAM. DeepSLAM состоит из нескольких основных компонентов, включая Mapping-Net, Tracking-Net, Loop-Net и модуль оптимизации графа. В частности, Mapping-Net - это архитектура кодировщика и декодера для описания трехмерной структуры окружающей среды, тогда как Tracking-Net - это архитектура рекуррентной сверточной нейронной сети для захвата движения камеры. Loop-Net - это предварительно обученный двоичный классификатор для обнаружения замыканий цикла. DeepSLAM может одновременно генерировать оценку позы, карту глубины и маску отклонения выбросов. В этой статье мы оцениваем его производительность на различных наборах данных и обнаруживаем, что DeepSLAM обеспечивает хорошую производительность с точки зрения точности оценки позы и надежен в некоторых сложных сценах.
Ключевые слова: оценка глубины, машинное обучение, рекуррентная сверточная нейронная сеть (RCNN), одновременная локализация и отображение (SLAM), неконтролируемое глубокое обучение (DL).

== ВВЕДЕНИЕ
ISUAL с одновременной локализацией и картированием (SLAM) имеет важное значение для роботов, работающих автономно с камерами,
а также является ядром огромных приложений, основанных на видении, например, виртуальной и дополненной реальности. Огромные усилия были приложены к визуальному SLAM в робототехнике и компьютерном зрении.



сообщества. В частности, за последнее десятилетие было разработано несколько современных систем визуального SLAM на основе разреженных характерных точек [1] ​​- [5] и фотометрической согласованности плотных пикселей [6] - [8].
Однако, поскольку большинство этих методов основаны на геометрической модели, они не могут автоматически обучаться на необработанных изображениях или извлекать выгоду из постоянно увеличивающихся наборов данных. Некоторые из них также становятся хрупкими в сложных сценах. Все чаще возникает вопрос, особенно при столкновении с крупномасштабными наборами данных, о том, можно ли понять и решить проблему визуального SLAM с точки зрения управления данными и являются ли подходы, основанные на данных, полезными.
Недавно методы, основанные на глубоком обучении (DL), продемонстрировали многообещающую эффективность при оценке позы и глубины [9], [10]. Большинство из них учатся на необработанных изображениях с ограниченным учетом геометрических моделей, которые были хорошо поняты за эти годы и признаны основами визуальных SLAM-систем. Однако было продемонстрировано, что обучающее представление для оценки глубины более эффективно, если соблюдаются геометрические ограничения [11]. Поэтому интересно посмотреть, как обучающее представление может быть эффективно использовано для визуального SLAM, беспрепятственно объединяя знания, накопленные за десятилетия в геометрических моделях. Как совместить геометрические модели и ограничения с сетевой архитектурой и функцией потерь, все еще остается сложной и открытой проблемой. Геометрические ограничения связаны с движением эго и структурой окружающей среды, что означает, что они могут быть использованы при проектировании пространственно-временных фотометрических потерь и геометрических потерь для методов, основанных на DL. Неправильное использование геометрических ограничений в функции потерь может привести к плохой производительности оценки или, что еще хуже, несовпадению процесса обучения. Напротив, надлежащее использование может привести к повышению точности оценки в среде обучения без учителя.
Большинство методов на основе DL основаны на схемах обучения с учителем, для которых требуются наборы данных с аннотированной достоверной информацией. Однако пометить большие объемы данных сложно и дорого, что ограничивает возможные сценарии применения методов на основе DL. Это особенно верно в контексте визуального SLAM, потому что роботы обычно работают в совершенно неизвестной среде. Более того, для визуальной SLAM-системы очень интересно обучаться по неконтролируемой схеме, так что производительность может постоянно улучшаться за счет увеличения размера наборов данных без аннотированной наземной истины.
В этой статье мы предлагаем DeepSLAM, неконтролируемую монокулярную SLAM-систему на основе DL. Принимает монокулярный цвет





Рис. 1. (a) Фреймворк для тестирования предлагаемой DeepSLAM. Он принимает монокулярные цветные изображения в качестве входных данных и создает карты глубины, позы и облака точек в качестве выходных данных с помощью Mapping-Net, Tracking-Net и Loop-Net. Mapping-Net - это архитектура автоэнкодера для оценки глубины. Tracking-Net - это архитектура на основе RCNN для оценки позы. Loop-Net - это архитектура CNN для обнаружения замыкания петель. Построение и оптимизация графа поз реализованы в качестве back-end.
    (b) DeepSLAM демонстрирует высокую производительность в некоторых сложных сценах.

изображения в качестве входных и выходных данных одновременно представляют траекторию, карту глубины и трехмерное облако точек (см. рис. 1). Наши основные вклады резюмируются следующим образом.
        1) Предлагается новая визуальная структура SLAM, основанная на неконтролируемом DL. Он использует комбинацию DL и геометрических ограничений.
        2) Глубокая рекуррентная сверточная нейронная сеть (RCNN) предназначена для моделирования движения эго путем использования как пространственных, так и временных свойств последовательности стереоизображений во время обучения.
        3) DeepSLAM объединяет результат отслеживания на основе DL и обнаружение замыкания цикла с механизмом оптимизации на основе графиков, формируя полную визуальную систему SLAM.
        4) Отклонение выбросов обрабатывается неопределенностью, полученной из карт ошибок геометрической и фотометрической согласованности. Это повышает надежность DeepSLAM в сложных условиях.
Обратите внимание, что хотя DeepSLAM использует стереосистему для обучения, для тестирования требуется только монокулярное зрение. Таким образом, DeepSLAM представляет собой монокулярную визуальную SLAM-систему.
Остальная часть этой статьи организована следующим образом. В разделе II рассматривается соответствующая работа. Системная архитектура предлагаемого DeepSLAM представлена ​​в Разделе III, за которым следует представление потерь при обучении и отклонения выбросов в Разделе IV. Раздел V знакомит с построением и оптимизацией графа позы в DeepSLAM. В разделе VI представлены наши экспериментальные результаты. Наконец, Раздел VII завершает эту статью.

== II. СВЯЗАННЫХ С РАБОТОЙ
    A. Контролируемый DL для оценки позы
Хорошо известно, что сверточные нейронные сети (CNN) успешны в классификации объектов. Новая идея - использовать
CNN для регрессии позы. Kendall et al. [9] предложил PoseNet решить проблему регрессии позы с помощью CNN. Он был обучен позам правды и мог использоваться для сценариев перемещения. Ли и др. [12] затем расширили PoseNet, включив в нее глубинную сеть, чтобы повысить производительность перенаправления в сложных условиях. Walch et al. [13] включили в PoseNet модуль пространственной долгосрочной краткосрочной памяти (LSTM) для повышения производительности. Чтобы оценить неопределенность оценки позы, Кендалл и Чиполла [14] предложили байесовскую сеть PoseNet, рассматривая отсев в сети как средство выборки. Кларк и др. [15] предложили использовать RCNN для реализации регрессии позы с видеоклипами, принимая во внимание временную информацию.
Помимо регрессии позы, движение эго между двумя кадрами изображения можно было оценить с помощью DL, вдохновленного стереогеометрическими моделями. Costante et al. [16] разработали CNN для оценки движения эго с контролируемым обучением. Mohanty et al. [17] использовали CNN для оценки преобразования между двумя последовательными кадрами. Wang et al. [18], [19] обучили RCNN оценивать движение камеры. Мелехов и др. [20] представили систему относительной оценки положения камеры с помощью CNN. Oliveira et al. [21] построили метрическую сеть для оценки движения эго и топологическую сеть для оценки топологического местоположения. Затем прогнозы этих двух сетей были объединены путем последовательной оптимизации. Ummenhofer et al. [22] предложили «DeMoN» для одновременной оценки движения эго, глубины изображения, нормали к поверхности и оптического потока, но требовались достоверные данные. DeTone et al. [23] разработали одну сеть для предсказания местоположения характерных точек и другую сеть для вычисления гомографии с данными, синтезированными вручную. Вместо описанной выше сквозной оценки позы с помощью CNN, Tateno et al. [24] использовали CNN для оценки карты глубины, а затем использовали систему SLAM на основе геометрической модели для выполнения оценки позы. Ли и др. [25] объединили метод прогнозирования глубины на основе CNN с ORB-SLAM [5], чтобы преодолеть проблему масштабирования для монокулярных систем SLAM. Ji et al. [26] объединили разреженные точки карты из системы SLAM и плотные предсказанные карты из CNN, чтобы иметь дело с границами глубины в трехмерной реконструкции. Laidlow et al. [27] предложили систему трехмерной реконструкции под названием Deepfusion. Они представили карты предсказанной плотной глубины в ORB-SLAM [5] и приняли оцененные градиенты глубины ключевых кадров в качестве ограничения для обеспечения глобальной согласованности трехмерной реконструкции.

    Б. Неконтролируемый DL для оценки глубины и эго-движения
Основная проблема в системах оценки позы с учителем - это требование большого количества данных с аннотированной наземной достоверностью для обучения сетей. В настоящее время размер наборов данных с аннотированной достоверной информацией ограничен, и их сбор дорого обходится. Это препятствует дальнейшему совершенствованию систем обучения с учителем. Недавно неконтролируемые методы DL были успешно применены для оценки глубины, вдохновленные техникой наложения изображений «пространственный преобразователь» [28]. Гарг и др. [11] предложили метод неконтролируемой оценки глубины, используя лево-правое фотометрическое ограничение в стерео.




Рис. 2. Схема обучения предлагаемой системы DeepSLAM. Мы используем стереоизображения для обучения системы, чтобы восстановить масштабную информацию окружающей среды, аналогичную той, которая используется при обучении. Потери пространственного изображения между парой стереоизображений и временные потери изображения между парой изображений последовательности формулируются для обучения сетей. Карта ошибок, созданная системой, используется в качестве масок потерь для отклонения выбросов. Неопределенность, создаваемая системой, также используется для обучения сетей.


пары изображений. Обучение сети проходило полностью без учителя в сквозной манере, и оно даже превзошло некоторые контролируемые методы с точки зрения точности оценки глубины. Xie et al. [29] предложил метод преобразования видео из 2D в 3D с неконтролируемым обучением. Godard et al. улучшил метод Garg et al. [30] путем наложения левого и правого изображений друг на друга. Чжоу и др. [31] предложил SfMLearner, который использовал последовательность монокулярных изображений для совмещения изображений, чтобы оценить глубину и движение эго одновременно с обучением без учителя. Однако оценочная карта глубины и движение эго не содержали информации о масштабе. Виджаянарасимхан и др. [32] предложили SfM-Net, которая добавляла маски движения к фотометрическим потерям. Он мог одновременно оценивать оптический поток, карту глубины и движение эго. Ли и др. [33] предложили метод визуальной одометрии, основанный на обучении без учителя. Он использовал стереоизображения для обучения и монокулярные изображения для тестирования, которые могли восстановить масштаб предполагаемых траекторий.
Отказ от выбросов, обнаружение замыкания цикла [34], [35] и оптимизация графа позы [36] являются очень важными компонентами для визуальных систем SLAM для уменьшения кумулятивных ошибок. Однако лишь несколько работ представили их в системах визуальной одометрии на основе DL. В последнее время методы на основе DL также достигли большого успеха в распознавании места и обнаружении замыкания петель [37], [38]. Важно сочетать методы обнаружения петель на основе DL с системами визуальной одометрии на основе DL для повышения точности.
Таким образом, неконтролируемые методы DL обещают новую исследовательскую тенденцию в области исследования визуального SLAM, потенциально создавая новую парадигму визуальных систем SLAM и способствуя дальнейшему повышению их производительности.

== III. ОБЗОР СИСТЕМЫ DEEPSLAM
Согласно схеме тестирования DeepSLAM на рис. 1, обученные Tracking-Net, Mapping-Net и Loop-Net можно рассматривать как интерфейс, дающий граф поз из последовательности монокулярных изображений. В частности, Tracking-Net - это архитектура RCNN, построенная из CNN-части VGGNet [39] и рекуррентной нейронной сети (RNN) для оценки поз и неопределенностей, Mapping-Net - это архитектура кодировщика-декодера для создания плотных карт глубины, и Loop-Net создает разреженные векторы признаков для обнаружения замыкания цикла. Между тем, оптимизация графа поз используется для уточнения поз в качестве задней части.
Схема обучения DeepSLAM показана на рис. 2. Tracking-Net и Mapping-Net обучаются без учителя с использованием пар стереоизображений и геометрических потерь в Разделе IV. Целью использования пар стереоизображений вместо монокулярных для обучения является восстановление масштабной информации окружающей среды. В ходе наших экспериментов мы обнаружили, что информацию о масштабе можно восстановить, если условия обучения и тестирования схожи. Loop-Net - это предварительно обученная CNN для определения замыканий петель.
Как показано на рис. 2, мы используем как пространственную, так и временную геометрическую согласованность последовательностей стереоизображений, чтобы сформулировать функцию потерь. Пространственная геометрическая согласованность представляет собой геометрическую проективную связь между соответствующими точками в парах левого и правого изображений, тогда как временная геометрическая согласованность представляет собой геометрическую проективную связь между соответствующими точками в двух последовательных монокулярных изображениях. Используя эти ограничения для построения функций потерь и минимизируя их все вместе, сети учатся

        1) Потеря согласованности диспаратности: карта диспаратности определяется
Q = H × w (4)
где w - ширина изображения. Следовательно, оцененные карты левого и правого диспаратности также могут быть ограничены посредством H. Обозначим Q1 и Qr как левые и правые карты диспаратности соответственно. Подобно потере фотометрической согласованности, мы можем использовать H для синтеза
Q × l и Qr × из Qr и Ql соответственно. Используя эти карты диспаратности, потери согласованности диспаратности могут быть построены как

// д л, г
// = Σ ǁQl - Q × lǁ1
// (5)
// Ld = Σ ǁQr - Q × ǁ. (6)
// и временные потери.


оценивать масштабированные позы с 6 степенями свободы и карты глубины сквозным неконтролируемым образом. Теперь мы можем обсудить детали различных функций потерь, используемых для обучения.


        2) Потеря согласованности позы: если последовательности левого и правого изображений используются отдельно для оценки преобразований с 6 степенями свободы движения камеры через сеть слежения, в идеале эти относительные перемещения должны быть приблизительными, а повороты должны быть точно такими же. . Следовательно, различия между этими двумя группами оценок позы могут быть представлены как потеря согласованности позы слева и справа как
    IV. БЕСПРОВОДНАЯ ПОДГОТОВКА НА ОСНОВЕ ПРОСТРАНСТВЕННОЙ И
ВРЕМЕННЫЕ ГЕОМЕТРИЧЕСКИЕ СОГЛАСОВАНИЯ
В этом разделе описаны потери, рассчитанные на обучение
L = λp ǁxl - xrǁ1 + λr ǁϕl - ϕrǁ1
^ л ^ л ^ г ^ г
(7)
Mapping-Net и Tracking-Net. В общем, существует два вида потерь, которые необходимо минимизировать при обучении: потеря пространственного изображения и потеря временного изображения. Связь между этими двумя потерями и последовательностью стереоизображений показана на рисунке 3.

    A. Потеря пространственного изображения пары стереоизображений
Потеря пространственного изображения использует геометрические ограничения [показанные в (1)] между стереоизображениями, чтобы позволить Mapping-Net создавать значимые карты глубины, которые содержат информацию о масштабе. Для пары стереоизображений каждый перекрывающийся пиксель i в одном изображении может найти свое соответствие в другом изображении с горизонтальным расстоянием Hi [11]. Учитывая его значение глубины Di, расстояние Hi можно рассчитать по формуле
Hi = Bf / Di (1)
где B - базовая линия стереокамеры, а f - фокусное расстояние. Следовательно, используя предсказанную карту глубины Di из Mapping-Net, карта расстояний H может быть сгенерирована для всего изображения. На основе H мы можем синтезировать новое изображение, искажая изображение из другого с помощью пространственного преобразователя [28]. В виде-
суммы Il × и Ir × - это синтезированные левое и правое изображения из исходного правого изображения Ir и левого изображения Il, соответственно.
Потери фотометрической согласованности слева и справа можно построить как
Lp = Σ λsfs (Il, I ×) + (1 - λs) ǁIl - I × ǁ (2)

правые последовательности изображений от Tracking-Net соответственно. λp и λr - веса положения и поворота, а λp намного меньше λr. Обратите внимание, что длина последовательности изображений может быть переменной благодаря повторяющейся сети в Tracking-Net.

    Б. Временная потеря изображения последовательности монокулярных изображений
Временная потеря изображения использует геометрические ограничения (а именно движения эго) между несколькими видами последовательности монокулярных изображений, чтобы позволить Mapping-Net создавать значимые карты глубины, а Tracking-Net - оценивать движение камеры.
Как показано на рис. 3, архитектура RCNN обеспечивает корреляцию между двумя последовательными монокулярными изображениями. Он включает потерю фотометрической согласованности и потерю трехмерной геометрической регистрации.
        1) Потеря фотометрической согласованности: В отличие от предыдущей потери фотометрической согласованности пары стереоизображений, фотометрические потери здесь сосредоточены на временной информации в последовательности монокулярных изображений. Для каждой пары изображений Ik, Ik + 1 с некоторыми перекрытиями сцен мы можем получить их синтезированные
изображения Ik × и Ik × +1 с помощью пространственной трансформаторной сети [28].
// В частности, для перекрывающегося пикселя pk в k-м кадре мы можем получить соответствующий ему пиксель pk × +1 в (k + 1) -м кадре через
// p × k + 1 = KT ^ k, k + 1D ^ kK − 1pk (8)
// л, р л
// Σ


// где K - внутренняя матрица камеры, D ^ k - пиксельная

// Lp = λsfs (Ir, I ×) + (1 - λs) ǁIr - I × ǁ (3)
// глубина оценивается из Mapping-Net, T ^ k, k + 1 - камера
// где 1 - норма L1, fs () = (1 SSIM ()) / 2 и
// SSIM () - это показатель структурного сходства (SSIM) для оценки качества синтезированного изображения [40], [41] с весом λs.
// (k + 1) -й кадр, предсказанный Tracking-Net. Основываясь на этом,
// Ik × и Ik × +1 могут быть построены из Ik + 1 и Ik соответственно. Определите карту временной фотометрической ошибки между изображениями

// Ik и его синтезированное изображение I × как Ek = Ik I ×. Тогда карты фотометрических ошибок для согласований k-to- (k + 1) и (k + 1) -to-k имеют вид
сеть как модель смеси [15] для получения неопределенности оценки позы. В отличие от этих контролируемых методов, DeepSLAM может создавать карты прогнозируемых фотометрических ошибок Ek, Ek + 1 и карты прогнозируемых геометрических ошибок Ek, Ek + 1 для
Ek = I
- I ×, Ek + 1 = I
- Я ×
. (9) п п г г


// Тогда фотометрические потери пары изображений Ik, Ik + 1 из μk + 1 будут средним значением Ek, Ek + 1, Ek и Ek + 1 соответственно.
// г п п г г
// последовательность монокулярных изображений
// Затем неопределенность оценки позы и оценки глубины
// с k-м кадром и (k + 1) -м кадром можно представить как
// Lp = Σ Mk λsfs (Ik, I ×) + (1 - λs) ¨Ek¨ (10)

// Lp = Σ Mk + 1 λsfs (Ik + 1, I ×)
// где S (·) - сигмоидальная функция, а λe - нормирующая
// к + 1, к п к + 1

// + (1 - λs) ¨Ek + 1¨ (11)


коэффициент между геометрической и фотометрической ошибками. Так как
μk, μk + 1, μk и μk + 1 являются здесь положительными сигмоидальными функциями.

карта тометрических ошибок. Мы обсудим маску в Разделе IV-C.
Обратите внимание, что кадры k и k +1 не обязательно идут подряд.
оценка неопределенности σk, k + 1 Tracking-Net

В архитектуре RNN фотометрические потери определяются несколькими парами изображений в последовательности изображений, что позволяет
где σk, k + 1
оценивается Tracking-Net и представляет
построение локального графа. См. Более подробную информацию о локальном графике
в разделе V-A.
а ^ к, к + 1
мала, когда предполагаемые позы и карты глубины
        2) Потеря трехмерной геометрической регистрации: геометрические потери используются для ограничения и оценки преобразований путем рассмотрения трехмерных облаков точек. Это похоже на итерационный метод определения ближайшей точки, хорошо известный метод выравнивания облаков точек. В DeepSLAM мы также используем эту потерю для оценки позы.
Предположим, что Pk и Pk + 1 - это трехмерные облака точек в k-й и (k + 1) -й координатах камеры, а Pk × и Pk × +1 - преобразованные облака точек в этих двух системах координат.
Затем построим геометрические потери в последовательности монокулярных изображений как
Σ ¨ ¨

мала, когда предполагаемые позы и карты глубины

достаточно точный, чтобы уменьшить фотометрические и геометрические ошибки. В реальных условиях фотометрические и геометрические потери могут быть искажены динамическими объектами. Поэтому мы вводим маски для карт ошибок в предыдущих временных потерях. Мы предлагаем новый метод построения побитовых масок, чтобы отбросить выброс во время обучения. В соответствии со значениями ошибок на картах ошибок маски строятся с процентилем qth пикселей как 1 и процентилем (1 qth) пикселей как 0. В частности, на основе неопределенности σk, k + 1, процентиля qth от пиксели
определяется
// qth = q0 + (1 - q0) (1 - σk, k + 1) (16)

// Lg =
// Mk ¨Ek¨


// (12)
// где
// - базовый постоянный процентиль. Тогда мы
// Lg = Σ Mk + 1 ¨Ek + 1¨
// (13)
// можно построить поразрядные маски Mp и Mg, чтобы отфильтровать
// где Ek = Pk - P × и Ek + 1 = Pk + 1 - P ×
// темп-
// карты. Сгенерированные маски не только автоматически адаптируются к
// г к г
// к + 1
различный процент выбросов, но также может использоваться для вывода
карты геометрической ошибки, а Mg - маска соответствующей карты геометрической ошибки. Для Pk, Pk ×, Pk + 1 и Pk × +1 все они являются тензорами с размером hw 3, где h - высота изображений, w - ширина изображений, а 3 обозначает (x, y, z) в
согласование камеры. Ek и Ek + 1 получаются поэлементным вычитанием облака точек.
Что касается существующих работ, Garg et al. [11] и Godard et al. [30] использовали лево-правые фотометрические потери для оценки карты глубины, Godard et al. [30] использовали потерю согласованности несоответствия слева и справа, а Zhou et al. [31] использовали фотометрическую потерю последовательности изображений, чтобы восстановить эго-движение и глубину. Тем не менее, небольшое количество работ исследовало комбинацию всех этих потерь для оценки как масштабированной позы камеры, так и карты глубины.

=== C. Оценка неопределенности и отклонение выбросов
Оценка неопределенности и отклонение выбросов очень важны в системах SLAM. Для оценки неопределенности методов регрессии позы с учителем с DL они либо применяют метод выборки с использованием Dropout [14], либо добавляют коэффициент баланса к
динамические объекты в сцене. Это будет подробно рассмотрено в Разделе VI-D.

== ПОСТРОЕНИЕ И ОПТИМИЗАЦИЯ ПОЗИЦИОННЫХ ГРАФИКОВ
Оптимизация графа поз играет важную роль в системах SLAM из-за своей способности уменьшать совокупное смещение позы. В нашей системе мы также выполняем оптимизацию графа позы с локальными и глобальными связями позы. Граф локальных поз строится на короткой последовательности последовательных изображений как прямой результат рекуррентной модели Tracking-Net с учетом последовательности изображений, то есть граф локальных поз строится из последовательных кадров изображений. Замыкания глобальных циклов обнаруживаются Loop-Net по историческим изображениям, которые обычно не являются последовательными.

=== A. Локальный поз-граф на основе RCNN
Архитектура RCNN Tracking-Net может изучать взаимосвязь между функциями CNN с течением времени по мере движения камеры,




Рис. 4. Граф поз с локальными и глобальными связями. Пунктирные линии представляют глобальные петли, обнаруженные с помощью Loop-Net, тогда как сплошные линии представляют локальные петли, созданные с помощью Tracking-Net. Здесь в качестве примера показан локальный граф с длиной последовательности изображений 5.



=== Б. Обнаружение глобальной петли на основе CNN
Для глобального графа позы мы используем Loop-Net для распознавания места и обнаружения петель между несоседними кадрами. Loop-Net в нашей системе DeepSLAM - это модель CNN, предварительно обученная на наборе данных ImageNet для распознавания объектов, поскольку она показала хорошую производительность при обучении представлений. Обратите внимание, что для Loop-Net обучение не требуется. Здесь принята архитектура Inception ResNet V2 [43]. Loop-Net сопоставляет изображения с векторами признаков для обнаружения замыкания петель. Затем мы можем вычислить косинусное расстояние двух векторов признаков из пары изображений, чтобы обнаружить замыкания цикла.
dcos = cos (v1, v2) (17)
разрешение, чтобы вычислить потери и настроить сети в конце. Для тестирования использовался ноутбук, оснащенный графическим процессором NVIDIA GeForce GTX 980 M и процессором Intel Core i7-6820HK 2,7 ГГц. Память графического процессора, необходимая для Tracking-Net, составляла менее 400 МБ с производительностью в реальном времени 40 Гц. Для Mapping-Net и Loop-Net мы выполняли прогнозирование плотной глубины и обнаружение замыкания цикла каждые пять кадров. Mapping-Net потребовалось около 48 мс для предсказания плотной карты глубины для каждого кадра и около 120 мс для Loop-Net, чтобы закодировать изображение в соответствующий вектор признаков. Tracking-Net, Mapping-Net, Loop-Net и модуль оптимизации графа поз выполнялись в отдельных потоках. Для всей системы он может работать с частотой около 20 Гц. Для повышения эффективности обучения были предприняты некоторые меры по увеличению данных, такие как увеличение левого и правого изображения, увеличение данных вращения и цвет изображения.
увеличение.

=== A. Выступление на точность позы на KITTI
Сначала мы оценили точность нашей системы Deep-SLAM на наборе данных одометрии KITTI [44]. Полная система DeepSLAM включает Tracking-Net с обнаружением замыкания цикла и оптимизацией графиков. Подробные количественные результаты приведены в Таблице I. Мы использовали стандартный метод оценки, предоставленный вместе с набором данных KITTI: средний дрейф (%) трансляционной среднеквадратичной ошибки (RMSE) и средний
вращательный дрейф СКО (◦ / 100 м) на длине 100–800 м. Мы также добавили два метода обучения на основе данных (ESP-VO и
SfMLearner) и трех модельных методов (монокуляр ORB-SLAM, монокуляр VISO2-M и стерео VISO2-S) в таблицу для сравнения. VISO2-M и монокуляр ORB-SLAM не работали с разрешением 416 × 128, и мы использовали входные изображения.

размером 1241 × 376. Для стерео методов также использовался VISO2-S.
входные изображения размером 1241 × 376. Все методы, основанные на обучении


== VI. ЭКСПЕРИМЕНТАЛЬНАЯ ОЦЕНКА
В этом разделе мы демонстрируем производительность отслеживания и отображения предлагаемой системы DeepSLAM. Мы провели оценку точности позы и глубины отдельно, чтобы увидеть, как работает каждая сеть.
Предлагаемый DeepSLAM был разработан с использованием структуры DL TensorFlow и обучен на NVIDIA DGX-1 с Tesla P100. Оптимизатор Adam использовался для обучения сети до 20–30 эпох. Начальная скорость обучения составляла 0,001 и снижалась наполовину на каждую пятую от общего числа итераций. Параметр β1 равен 0,9, а β2 равен 0,99. Длина последовательности
изображений, подаваемых в Tracking-Net, было 5. Размер изображения был 416 × 128. Мы также изменили размер выходных изображений на более высокий


(DeepSLAM, ESP-VO и SfMlearner) использовали последовательности KITTI 00–02, 08, 09 для обучения сети и последовательности KITTI 03–07, 10 в качестве наборов данных для тестирования. Наш DeepSLAM - это метод обучения без учителя, и для его обучения не требуется достоверная информация. Чтобы продемонстрировать преимущества обучения без учителя и полностью раскрыть потенциал DeepSLAM, мы также использовали последовательности KITTI 00–02, 08, 09, 11–21 для обучения сети. Жирным шрифтом выделены лучшие результаты трекинга среди методов обучения.
Как показано в таблице, наш DeepSLAM превосходит ESP-VO и SfMLearner с точки зрения точности отслеживания. По сравнению с ESP-VO, мы использовали больше наборов данных (последовательности KITTI 11–21) для обучения сети, поскольку нашему DeepSLAM не нужны наборы данных с аннотированной наземной достоверностью. ESP-VO - это контролируемый метод обучения, который не может использовать последовательности KITTI 11–21 для обучения. Результат показывает, что методы обучения без учителя могут использовать больше наборов данных для обучения и получить от этого преимущество в производительности. По сравнению с SfMLearner, система DeepSLAM использует более тщательно разработанные функции пространственных и временных потерь и использует RCNN как архитектуру Tracking-Net. DeepSLAM также превосходит монокуляр VISO2-M, но его производительность не так хороша, как ORB-SLAM и стерео VISO2-S, поскольку DeepSLAM не может поддерживать локальную карту и глобальную карту, как ORB-SLAM. Расчетные траектории на последовательностях

ТАБЛИЦА I
РЕЗУЛЬТАТЫ ОТСЛЕЖИВАНИЯ НА KITTI DATASET С НАШЕЙ ПРЕДЛАГАЕМОЙ СИСТЕМОЙ DEEPSLAM





trel: Средний поступательный снос СКО (%) на длине 100–800 м.
rrel: Средний вращательный дрейф RMSE (◦ / 100 м) на длине 100–800 м. A: Последовательности KITTI 00–02, 08, 09 в качестве обучающих данных.
B: последовательности KITTI 11–21 в качестве обучающих данных.



Рис. 5. Траектории на последовательностях KITTI 03–07, 10 с использованием нашей системы Deep-SLAM (лучше всего видно в цвете). Траектории с ESP-VO [19], SfMLearner [31], ORB-SLAM [5], VISO2-M [42] и VISO2-S [42] являются
также нанесен на график для сравнения. (a) 03. (b) 04. (c) 05. (d) 06. (e) 07. (f) 10.




// 03–07, 10 with above methods are plotted in Fig. 5. As shown in the figure, the trajectories from the DeepSLAM achieve good performance in terms of pose estimation.
// In order to perform more experiments and comparisons, we also used KITTI sequences 00–08 for network training and the rest sequences for testing. There are no ground truths provided for KITTI sequences 11–21; thus, we plotted trajectories with stereo ORB-SLAM (ORB-SLAM-S) for reference. The trajec- tories of sequences 13 and 15–19 are plotted in the figure. As shown in Fig. 6, the trajectories produced by our DeepSLAM are similar with the ones produced by ORB-SLAM-S. In order to highlight the role of the loop closure in localization, we have added the results of our proposed system without Loop-Net in Fig. 6. As shown in the figure, for sequences 13, 15, 16, 18, and 19 that have loops, the system without Loop-Net cannot achieve the same performance as with loop closure due to the fact that the accumulated errors cannot be reduced.

// === B. Robustness Performance on Challenging Scenes
// Robustness is a significant factor for wider applications of visual SLAM. We used the public RobotCar [45] dataset to test the robustness of the proposed DeepSLAM system. The RobotCar dataset was collected in different environments over
// Fig. 6. Trajectories on KITTI dataset using our DeepSLAM system. The trajectories using our DeepSLAM system without Loop-Net are also given. There is no ground truth for these trajectories. We plotted trajectories with ORB-SLAM-S for reference. KITTI sequences 00–08 are used for network training. (a) 13. (b) 15. (c) 16. (d) 17. (e) 18. (f) 19.



// one year. We chose some datasets collected in challenging environments to test our trained models. Dataset (b) in Fig. 7 was used for training the Tracking-Net and Mapping-Net.
// As shown in Fig. 7, the challenging scenes include image dis- tortion, excessive exposure, night-time, bad white balance, and raining. These scenes are difficult for visual SLAM systems to perform accurately and robustly. Model-based SLAM methods (such as LSD-SLAM and ORB-SLAM) are sensitive to camera parameters, and not robust when performing feature extraction or transformation estimation under the above situations. So these methods are fragile when faced with challenging scenes. Fig. 7(a) shows the situation with image distortion. Fig. 7(b) shows the situation that there is excessive exposure in parts of the trajectory. Fig. 7(c) shows the situation that the data are collected at night while raining. Fig. 7(d) shows the situation that the images are collected while raining. No ground truth is provided. We compared our results with the trajectories collected by GPS/inertial navigation system (INS). For Fig. 7(c), the GPS signal was poor due to the rain. For Fig. 7(d), the GPS/INS almost did not work due to the terrible weather, and we plotted the trajectory with our DeepSLAM in Google Map for reference. As shown in Fig. 7, the DeepSLAM demonstrated a resilient behavior when encountering these challenging scenes.




// Fig. 7. Robustness performance of our DeepSLAM on some challenging environments in RobotCar dataset. The left part of each subfigure shows the trajectory produced from our DeepSLAM, and the right part is the testing images taken in different locations. (a) Images with distortion.
// (b) Images with excessive exposure. (c) Images collected at night while raining. (d) Images collected while raining. Note that the GPS and INS data are very poor in RoboCar dataset 2015-05-29-09-36-29 (seq. 06) due to raining.




// Fig. 8. Estimated depth maps with our Mapping-Net. The left two columns are the images from the KITTI dataset. The right two columns are the images from challenging scenes in the RobotCar dataset.



// Further, the right columns of the images in Fig. 8 provide the estimated depth images for the RoboCar dataset. It can be seen that the Mapping-Net demonstrates a robust performance on depth estimation (i.e., reconstruction/mapping of the environ- ments) even facing challenging scenes, such as night-time and raining with underexposure and overexposure. For example, the depths of the cars on the right parts of both the sixth and the seventh images in Fig. 8 can be accurately estimated. Therefore, the Mapping-Net provides reliable scene mapping, which is used by the Tracking-Net to enhance the robustness for pose estimates during training. Meanwhile, as shown in Fig. 7, the Tracking-Net can keep estimating the 6-DoF poses when facing challenging environments in the RoboCar dataset, leveraging the accurate 3-D reconstruction from the Mapping-Net. In summary, it is believed that the Tracking-Net and Mapping-Net both play an important role in improving the robustness and work in tandem when facing challenging scenes.
// Table II lists the results of DeepSLAM, LSD-SLAM, and ORB-SLAM-S on challenging scenes of RobotCar dataset. When encountering the challenging scenes such as raining, night-time, and bad white balanc


03–07, 10 с описанными выше методами показаны на рис. 5. Как показано на рисунке, траектории DeepSLAM достигают хороших показателей с точки зрения оценки позы.
Чтобы провести больше экспериментов и сравнений, мы также использовали последовательности KITTI 00–08 для обучения сети, а остальные последовательности - для тестирования. Для последовательностей KITTI 11–21 нет основополагающих истин; Таким образом, мы построили траектории с помощью стерео ORB-SLAM (ORB-SLAM-S) для справки. Траектории последовательностей 13 и 15–19 нанесены на рисунок. Как показано на рис. 6, траектории, создаваемые нашим DeepSLAM, аналогичны траекториям, создаваемым ORB-SLAM-S. Чтобы подчеркнуть роль замыкания цикла в локализации, мы добавили результаты нашей предлагаемой системы без Loop-Net на рис. 6. Как показано на рисунке, для последовательностей 13, 15, 16, 18 и 19, которые есть петли, система без Loop-Net не может достичь той же производительности, что и с замыканием петли, из-за того, что накопленные ошибки не могут быть уменьшены.

=== Б. Устойчивость к работе в сложных условиях
Устойчивость - важный фактор для более широкого применения визуального SLAM. Мы использовали общедоступный набор данных RobotCar [45], чтобы проверить надежность предлагаемой системы DeepSLAM. Набор данных RobotCar был собран в разных средах в течение
Рис. 6. Траектории на датасете KITTI с использованием нашей системы DeepSLAM. Также приведены траектории с использованием нашей системы DeepSLAM без Loop-Net. Эти траектории не являются достоверными. Для справки мы построили траектории с помощью ORB-SLAM-S. Последовательности KITTI 00–08 используются для обучения сети. (a) 13. (b) 15. (c) 16. (d) 17. (e) 18. (f) 19.



один год. Мы выбрали несколько наборов данных, собранных в сложных условиях, для тестирования наших обученных моделей. Набор данных (b) на рис. 7 использовался для обучения Tracking-Net и Mapping-Net.
Как показано на рис. 7, сложные сцены включают искажение изображения, чрезмерную экспозицию, ночное время, плохой баланс белого и дождь. Визуальным SLAM-системам сложно выполнять эти сцены точно и надежно. Методы SLAM на основе моделей (такие как LSD-SLAM и ORB-SLAM) чувствительны к параметрам камеры и не являются надежными при выполнении выделения признаков или оценки преобразования в вышеупомянутых ситуациях. Таким образом, эти методы ненадежны при столкновении со сложными сценами. Рис. 7 (а) показывает ситуацию с искажением изображения. На рис. 7 (b) показана ситуация, когда на некоторых участках траектории имеется чрезмерная экспозиция. На рис. 7 (c) показана ситуация, когда данные собираются ночью во время дождя. Рис. 7 (d) показывает ситуацию, когда изображения собираются во время дождя. Никакой достоверной информации не предоставляется. Мы сравнили наши результаты с траекториями, полученными с помощью GPS / инерциальной навигационной системы (INS). На рис. 7 (c) сигнал GPS был плохим из-за дождя. Для рис. 7 (d) GPS / INS почти не работал из-за ужасной погоды, и для справки мы построили траекторию с помощью нашего DeepSLAM в Google Map. Как показано на рис. 7, DeepSLAM продемонстрировал гибкое поведение при столкновении с этими сложными ситуациями.




Рис. 7. Показатели устойчивости нашего DeepSLAM в некоторых сложных условиях в наборе данных RobotCar. Левая часть каждой подфигуры показывает траекторию, созданную нашим DeepSLAM, а правая часть - это тестовые изображения, сделанные в разных местах. (а) Изображения с искажением.
(б) Изображения с чрезмерной выдержкой. (c) Изображения, сделанные ночью во время дождя. (d) Изображения, полученные во время дождя. Обратите внимание, что данные GPS и INS очень плохи в наборе данных RoboCar 2015-05-29-09-36-29 (seq.06) из-за дождя.




Рис. 8. Расчетные карты глубины с помощью нашей Mapping-Net. Два левых столбца - это изображения из набора данных KITTI. Два правых столбца - это изображения сложных сцен в наборе данных RobotCar.



Кроме того, в правых столбцах изображений на рис. 8 представлены изображения с оценкой глубины для набора данных RoboCar. Можно видеть, что Mapping-Net демонстрирует надежную производительность при оценке глубины (т.е. реконструкция / картографирование окружающей среды) даже при столкновении со сложными сценами, такими как ночное время и дождь с недодержкой и передержкой. Например, можно точно оценить глубины автомобилей в правой части шестого и седьмого изображений на рис. 8. Таким образом, Mapping-Net обеспечивает надежное отображение сцены, которое используется Tracking-Net для повышения надежности оценок позы во время обучения. Между тем, как показано на рис. 7, Tracking-Net может продолжать оценивать позы с 6 степенями свободы при столкновении со сложными условиями в наборе данных RoboCar, используя точную трехмерную реконструкцию из Mapping-Net. Таким образом, считается, что Tracking-Net и Mapping-Net играют важную роль в повышении надежности и работают в тандеме при столкновении с трудными ситуациями.
В таблице II перечислены результаты DeepSLAM, LSD-SLAM и ORB-SLAM-S в сложных сценах набора данных RobotCar. При столкновении с трудными сценами, такими как дождь, ночь и плохой баланс белого






// e, LSD-SLAM and ORB- SLAM can hardly work, but our DeepSLAM works well by exploiting the prior knowledge learned through training.

// TABLE II
// ROBUSTNESS PERFORMANCE ON CHALLENGING SCENES OF ROBOTCAR DATASET




// √ represents working well and × represents losing tracking.

// Fig. 9. Testing on our self-collected dataset using a low-cost ZED camera.


// === C. Testing With a Low-Cost Camera
// We also used a low-cost stereo camera ZED to collect some data ourselves and tested our system. We used a laptop, a cheap GPS and a ZED camera to collect outdoor data. No other types of equipment were used, and we did not have the ground truth. We used the GPS data to provide the reference. The trajectories from our DeepSLAM and GPS are plotted in Fig. 9. The weather was cloudy when we collected the data, so the images are quite dim. As shown in Fig. 9, the DeepSLAM works well with this low-cost camera.

// TABLE III
// DEPTH ESTIMATION RESULTS ON KITTI USING THE SPLIT OF EIGEN ET AL. [10]



// Fig. 10. Geometric mask and photometric mask for outlier rejection. The red boxes represent moving objects and the green boxes represent depth values with high uncertainty.

// === D. Outlier Rejection
// As introduced above, We used the photometric error maps and geometric error maps (3-D point cloud registration error maps) from monocular image sequences to generate the loss mask and uncertainty. The loss mask can reject the outlier and refine the estimated poses. This is due to the fact that the 3-D registration error map includes depth and pose estimation information, and the photometric error map includes photometric information, depth and pose estimation information. The uncertainty is re- lated to the mean of the error map, which is used to automatically select the size of the mask.
// Fig. 10 shows the 3-D registration error mask and the photo- metric error mask. The red boxes in the figure are moving objects in the scenes, such as pedestrians and vehicles. The green boxes in the figure are depth values with high uncertainty. These values tend to be sky, extreme dark places, edges of objects or nonover- lap areas of left–right images. It is very hard for the network to estimate the real depth value of these places, and therefore the estimated depth value has a high uncertainty. In our system, the photometric mask is directly related to moving objects, and the geometric mask is related to estimated depth values with high uncertainty. We also plot the estimated uncertainty against the corresponding translational and rotational errors in Fig. 11. As shown in the figure, the estimated uncertainty values from our network are strongly correlated with both of them.

// === E. Depth Estimation and 3-D Reconstruction
// The Mapping-Net of the DeepSLAM can also produce the scaled dense depth map. Fig. 8 shows some raw RGB images


// Fig. 11. Estimated uncertainty against the corresponding translational and rotational errors. It shows that the estimated uncertainty values are strongly correlated with both of them. (a) Seq. 03. (b) Seq. 05.

// and the dense depth maps generated by the DeepSLAM. The left two columns are the estimated depth maps selected from KITTI dataset, and the right two columns are the estimated depth maps of the challenging scenes selected from RobotCar dataset. As shown in the figure, the depths of cars, trees, and trunks are clearly visible.
// The detailed quantitative depth estimation results are listed in Table III. RobotCar dataset does not provide the ground truth for depth maps. We used KITTI dataset to evaluate the performance of the Mapping-Net quantitatively. As shown in the table, the DeepSLAM outperforms the supervised one [10] and the unsu- pervised one without scale [31], but performs not as good as [30]. This could be caused by a few reasons. First, we only used parts of KITTI dataset (KITTI odometry dataset) for training, whereas all other methods use full KITTI dataset to train their networks. Second, Godard et al. [30] used higher resolution (512 256) input and a more sophisticated deep neural network (ResNet- based architecture). Third, the temporal image loss we used could introduce some noise (such as moving objects) for depth estimation.
// Exploiting the powerful ability of pose estimation and depth estimation with the DeepSLAM system, we can also reconstruct the dense 3-D point cloud of the scenes with a monocular camera. Fig. 12 shows some 3-D dense maps generated by the DeepSLAM system.




// Fig. 12. Reconstructed 3-D map with our DeepSLAM system.


// == VII. CONCLUSION
// Most state-of-the-art SLAM algorithms rely on geometric models and optimization engines to estimate the structure of environment and the motion of camera. This article approached to the problem from a data-driven perspective, i.e., training deep neural networks with existing datasets. The system architecture of DeepSLAM mimics that of model-based SLAMs. The impor- tant geometric models and constraints were respected and em- bedded into the network architecture and 







е, LSD-SLAM и ORB-SLAM вряд ли могут работать, но наш DeepSLAM работает хорошо, используя предварительные знания, полученные в процессе обучения.

ТАБЛИЦА II.
ПРОИЗВОДИТЕЛЬНОСТЬ НАДЕЖНОСТИ НА СЛОЖНЫХ СЦЕНАХ НАБОРА ДАННЫХ ROBOTCAR




√ означает хорошую работу, а × означает потерю отслеживания.

Рис. 9. Тестирование на нашем собственном наборе данных с помощью недорогой камеры ZED.


=== C. Тестирование с помощью недорогой камеры
Мы также использовали недорогую стереокамеру ZED, чтобы сами собрать некоторые данные и протестировать нашу систему. Мы использовали ноутбук, дешевый GPS-навигатор и камеру ZED для сбора данных на открытом воздухе. Никакое другое оборудование не использовалось, и достоверной информации у нас не было. Мы использовали данные GPS для справки. Траектории от наших DeepSLAM и GPS показаны на рис. 9. Когда мы собирали данные, погода была пасмурной, поэтому изображения получаются довольно тусклыми. Как показано на рис. 9, DeepSLAM хорошо работает с этой недорогой камерой.

ТАБЛИЦА III
РЕЗУЛЬТАТЫ ОЦЕНКИ ГЛУБИНЫ KITTI С ИСПОЛЬЗОВАНИЕМ SPLIT OF EIGEN ET AL. [10]



Рис. 10. Геометрическая маска и фотометрическая маска для отбраковки выбросов. Красные прямоугольники представляют движущиеся объекты, а зеленые прямоугольники представляют значения глубины с высокой погрешностью.

=== D. Отклонение выбросов
Как было сказано выше, мы использовали карты фотометрических ошибок и карты геометрических ошибок (карты ошибок регистрации трехмерных облаков точек) из последовательностей монокулярных изображений для создания маски потерь и неопределенности. Маска потерь может отклонить выброс и уточнить предполагаемые позы. Это связано с тем, что трехмерная карта ошибок регистрации включает информацию об оценке глубины и позы, а карта фотометрических ошибок включает фотометрическую информацию, информацию об оценке глубины и позы. Неопределенность связана со средним значением карты ошибок, которое используется для автоматического выбора размера маски.
На рис. 10 показаны трехмерная маска ошибки регистрации и маска фотометрической ошибки. Красные прямоугольники на рисунке - это движущиеся объекты в сценах, такие как пешеходы и автомобили. Зеленые прямоугольники на рисунке - это значения глубины с высокой погрешностью. Эти значения обычно представляют собой небо, очень темные места, края объектов или неперекрывающиеся области левого и правого изображений. Сети очень сложно оценить реальное значение глубины этих мест, и поэтому расчетное значение глубины имеет высокую неопределенность. В нашей системе фотометрическая маска напрямую связана с движущимися объектами, а геометрическая маска связана с расчетными значениями глубины с высокой неопределенностью. Мы также сопоставляем расчетную неопределенность с соответствующими ошибками поступательного и вращательного движения на рис. 11. Как показано на рисунке, расчетные значения неопределенности из нашей сети сильно коррелируют с ними обоими.

=== E. Оценка глубины и трехмерная реконструкция
Mapping-Net DeepSLAM также может создавать масштабированную карту плотной глубины. На рис.8 показаны некоторые необработанные изображения RGB.


Рис. 11. Расчетная погрешность относительно соответствующих поступательных и вращательных ошибок. Это показывает, что оцененные значения неопределенности сильно коррелируют с ними обоими. (а) Посл. 03. (b) След. 05.

и плотные карты глубины, созданные DeepSLAM. Два левых столбца - это расчетные карты глубины, выбранные из набора данных KITTI, а два правых столбца - это расчетные карты глубины сложных сцен, выбранных из набора данных RobotCar. Как показано на рисунке, хорошо видны глубины машин, деревьев и стволов.
Подробные результаты количественной оценки глубины перечислены в Таблице III. Набор данных RobotCar не дает достоверных сведений о картах глубины. Мы использовали набор данных KITTI для количественной оценки производительности Mapping-Net. Как показано в таблице, DeepSLAM превосходит контролируемый [10] и неконтролируемый без масштабирования [31], но работает не так хорошо, как [30]. Это могло быть вызвано несколькими причинами. Во-первых, мы использовали только части набора данных KITTI (набор данных одометрии KITTI) для обучения, тогда как все другие методы используют полный набор данных KITTI для обучения своих сетей. Во-вторых, Godard et al. [30] использовали вход с более высоким разрешением (512 256) и более сложную глубокую нейронную сеть (архитектура на основе ResNet). В-третьих, временная потеря изображения, которую мы использовали, может привести к некоторому шуму (например, движущимся объектам) для оценки глубины.
Используя мощные возможности оценки позы и глубины с помощью системы DeepSLAM, мы также можем реконструировать плотное трехмерное облако точек сцен с помощью монокулярной камеры. На рис. 12 показаны некоторые трехмерные плотные карты, созданные системой DeepSLAM.




Рис. 12. Реконструированная трехмерная карта с помощью нашей системы DeepSLAM.


== VII. ВЫВОД
Большинство современных алгоритмов SLAM полагаются на геометрические модели и механизмы оптимизации для оценки структуры окружающей среды и движения камеры. Эта статья подошла к проблеме с точки зрения управления данными, то есть обучения глубоких нейронных сетей с существующими наборами данных. Системная архитектура DeepSLAM имитирует архитектуру SLAM на основе моделей. Важные геометрические модели и ограничения были учтены и внедрены в архитектуру сети и

// the loss function. This provided a guarantee for estimation accuracy. Our evaluation re- sults showed that the data-driven approach DeepSLAM achieves good performance in terms of accuracy and robustness. Deep- SLAM falls within an unsupervised learning framework as no manual annotation was required for training. This distinguishes itself from supervised deep leaning approaches to SLAM. In the future, more unlabeled datasets will be made available as they are easy to collect. It is expected the DeepSLAM will have the opportunity to further improve the performance. Following this direction, our future work is planned to train DeepSLAM with more data.



функция потерь. Это давало гарантию точности оценки. Результаты нашей оценки показали, что подход, основанный на данных, DeepSLAM обеспечивает хорошую производительность с точки зрения точности и надежности. Deep-SLAM входит в рамки обучения без учителя, поскольку для обучения не требуется ручных аннотаций. Это отличает себя от контролируемых подходов к SLAM. В будущем будет доступно больше немаркированных наборов данных, поскольку их легко собирать. Ожидается, что DeepSLAM получит возможность дальнейшего повышения производительности. Следуя этому направлению, в нашей будущей работе планируется обучить DeepSLAM большему количеству данных.


== REFERENCES
[1] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “MonoSLAM: Real-time single camera SLAM,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 6, pp. 1052–1067, Jun. 2007.
[2] G. Klein and D. Murray, “Parallel tracking and mapping for small AR workspaces,” in Proc. IEEE ACM Int. Symp. Mixed Augmented Reality, 2007, pp. 225–234.
[3] M. Lin, C. Yang, and D. Li, “An improved transformed unscented Fast- SLAM with adaptive genetic resampling,” IEEE Trans. Ind. Electron., vol. 66, no. 5, pp. 3583–3594, May 2019.
[4] T.-J. Lee, C.-H. Kim, and D.-I. D. Cho, “A monocular vision sensor- based efficient SLAM method for indoor service robots,” IEEE Trans. Ind. Electron., vol. 66, no. 1, pp. 318–328, Jan. 2019.
[5] R. Mur-Artal, J. Montiel, and J. D. Tardos, “ORB-SLAM: A versatile and accurate monocular SLAM system,” IEEE Trans. Robot., vol. 31, no. 5, pp. 1147–1163, Oct. 2015.
[6] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “DTAM: Dense tracking and mapping in real-time,” in Proc. IEEE Int. Conf. Comput. Vision, 2011, pp. 2320–2327.
[7] J. Engel, T. Schöps, and D. Cremers, “LSD-SLAM: Large-scale di- rect monocular SLAM,” in Proc. Eur. Conf. Comput. Vision, 2014, pp. 834–849.
[8] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 611–625, Mar. 2018.
[9] A. Kendall, M. Grimes, and R. Cipolla, “PoseNet: A convolutional network for real-time 6-DoF camera relocalization,” in Proc. IEEE Int. Conf. Comput. Vision, 2015, pp. 2938–2946.
[10] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single image using a multi-scale deep network,” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2366–2374.

[11] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised CNN for single view depth estimation: Geometry to the rescue,” in Proc. Eur. Conf. Comput. Vision, 2016, pp. 740–756.
[12] R. Li, Q. Liu, J. Gui, D. Gu, and H. Hu, “Indoor relocalization in challeng- ing environments with dual-stream convolutional neural networks,” IEEE Trans. Autom. Sci. Eng., vol. 15, no. 2, pp. 651–662, Apr. 2018.
[13] F. Walch, C. Hazirbas, L. Leal-Taixé, T. Sattler, S. Hilsenbeck, and
D. Cremers, “Image-based localization using LSTMs for structured feature correlation,” in Proc. IEEE Int. Conf. Comput. Vision, 2017, pp. 627–637.
[14] A. Kendall and R. Cipolla, “Modelling uncertainty in deep learning for camera relocalization,” in Proc. IEEE Int. Conf. Robot. Autom., 2016, pp. 4762–4769.
[15] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, “VidLoc: 6-DoF video-clip relocalization,” in Proc. Conf. Comput. Vision Pattern Recognit., 2017, pp. 6856–6864.
[16] G. Costante, M. Mancini, P. Valigi, and T. A. Ciarfuglia, “Exploring repre- sentation learning with CNNs for frame-to-frame ego-motion estimation,” IEEE Robot. Autom. Lett., vol. 1, no. 1, pp. 18–25, Jan. 2016.
[17] V. Mohanty, S. Agrawal, S. Datta, A. Ghosh, V. D. Sharma, and D. Chakravarty, “DeepVO: A deep learning approach for monocular visual odometry,” 2016, arXiv:1611.06069.
[18] S. Wang, R. Clark, H. Wen, and N. Trigoni, “DeepVO: Towards end-to-end visual odometry with deep recurrent convolutional neural networks,” in Proc. IEEE Int. Conf. Robot. Autom., 2017, pp. 2043–2050.
[19] S. Wang, R. Clark, H. Wen, and N. Trigoni, “End-to-end, sequence-to- sequence probabilistic visual odometry through deep neural networks,” Int. J. Robot. Res., vol. 37, no. 4/5, pp. 513–542, 2018.
[20] I. Melekhov, J. Kannala, and E. Rahtu, “Relative camera pose estimation using convolutional neural networks,” in Int. Conf. Adv. Concepts Intell. Vision Syst., pp. 675–687, 2017.
[21] G. L. Oliveira, N. Radwan, W. Burgard, and T. Brox, “Topometric localization with deep learning,” in Robotics Res., pp. 505–520 2020, arXiv:1706.08775.
[22] B. Ummenhofer et al., “DeMoN: Depth and motion network for learning monocular stereo,” in Proc. Conf. Comput. Vision Pattern Recognit., 2017, pp. 5038–5047.
[23] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Toward geometric deep SLAM,” 2017, arXiv:1707.07410.
[24] K. Tateno, F. Tombari, I. Laina, and N. Navab, “CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction,” in Proc. Conf. Comput. Vision Pattern Recognit., 2017, pp. 6565–6574.
[25] Y. Li, C. Xie, H. Lu, X. Chen, J. Xiao, and H. Zhang, “Scale-aware monocular SLAM based on convolutional neural network,” in Proc. IEEE Int. Conf. Inf. Autom., 2018, pp. 51–56.
[26] X. Ji, X. Ye, H. Xu, and H. Li, “Dense reconstruction from monocular SLAM with fusion of sparse map-points and CNN-inferred depth,” in Proc. IEEE Int. Conf. Multimedia Expo., 2018, pp. 1–6.
[27] T. Laidlow, J. Czarnowski, and S. Leutenegger, “DeepFusion: Real- time dense 3D reconstruction for monocular SLAM using single-view depth and gradient predictions,” in Proc. Int. Conf. Robot. Autom., 2019, pp. 4068–4074.
[28] M. Jaderberg, K. Simonyan, and A. Zisserman, “Spatial transformer networks,” in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 2017–2025.
[29] J. Xie, R. Girshick, and A. Farhadi, “Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks,” in Proc. Eur. Conf. Comput. Vision, 2016, pp. 842–857.
[30] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth estimation with left-right consistency,” in Proc. Conf. Comput. Vision Pattern Recognit., 2017, pp. 270–279.
[31] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning of depth and ego-motion from video,” in Proc. Conf. Comput. Vision Pattern Recognit., 2017, pp. 1851–1858.
[32] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki, “SfM-Net: Learning of structure and motion from video,” 2017, arXiv:1704.07804.
[33] R. Li, S. Wang, Z. Long, and D. Gu, “UnDeepVO: Monocular visual odometry through unsupervised deep learning,” in Proc. IEEE Int. Conf. Robot. Autom., 2018, pp. 7286–7291.
[34] D. Gálvez-López and J. D. Tardos, “Bags of binary words for fast place recognition in image sequences,” IEEE Trans. Robot., vol. 28, no. 5, pp. 1188–1197, Oct. 2012.
[35] R. Mur-Artal and J. D. Tardós, “Fast relocalisation and loop closing in keyframe-based SLAM,” in Proc. IEEE Int. Conf. Robot. Autom., 2014, pp. 846–853.

[36] R. Kümmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard, “g2o: A general framework for graph optimization,” in Proc. IEEE Int. Conf. Robot. Autom., 2011, pp. 3607–3613.
[37] X. Gao and T. Zhang, “Unsupervised learning to detect loops using deep neural networks for visual SLAM system,” Auton. Robots, vol. 41, no. 1, pp. 1–18, 2017.
[38] J. Li, H. Zhan, B. M. Chen, I. Reid, and G. H. Lee, “Deep learning for 2D scan matching and loop closure,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2017, pp. 763–768.
[39] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in Int. Conf. Учиться. Representations, pp. 1–14, 2015.
[40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.
[41] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for neural networks for image processing” IEEE Trans. Computational Imaging, vol. 3, no. 1, pp. 47–57, Mar. 2017.
[42] A. Geiger, J. Ziegler, and C. Stiller, “StereoScan: Dense 3D reconstruction in real-time,” in Proc. Intell. Veh. Symp., 2011, pp. 963–968.
[43] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, Inception-ResNet and the impact of residual connections on learning,” in Proc. AAAI Conf. Artif. Intell., 2017, pp. 4278–4284.
[44] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? The KITTI vision benchmark suite,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2012, pp. 3354–3361.
[45] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 year, 1000 km: The Oxford RobotCar dataset,” Int. J. Robot. Res., vol. 36, no. 1, pp. 3–15, 2017, doi: 10.1177/0278364916679498.
