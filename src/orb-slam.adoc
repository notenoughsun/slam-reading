= –û–ø–∏—Å–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ORB SLAM, –æ—Å–æ–±–µ–Ω–Ω–æ –º–µ—Ç–æ–¥–æ–≤ –ò–ò, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –Ω–∏—Ö.

* [ ] –Ω–∞–ø–∏—Å–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
* [ ] –µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ orb--slam, –≤–∑—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
* [ ] –µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ orb--slam3, –Ω—É–∂–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å
* [ ] –≤—ã–±—Ä–∞—Ç—å —Ç–µ–∑–∏—Å—ã –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–æ–±—â–µ –Ω–∞–¥–æ —É–∫–∞–∑–∞—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ

====
* "ROS. –°—Ç–µ–∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ / –•–∞–±—Ä" https://habr.com/ru/post/327888/
* "robotics-smoothing-and-mapping/README.md at master ¬∑ ccorcos/robotics-smoothing-and-mapping" https://github.com/ccorcos/robotics-smoothing-and-mapping/blob/master/README.md
* "raulmur/ORB_SLAM2: Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities" https://github.com/raulmur/ORB_SLAM2
* "ORB-SLAM3 paper summary - Programmer Sought" https://programmersought.com/article/85105331899/
* "Computer Vision Group - Visual SLAM - LSD-SLAM: Large-Scale Direct Monocular SLAM" https://vision.in.tum.de/research/vslam/lsdslam?redirect=1
* "üí§ Vision-based SLAM: —Å—Ç–µ—Ä–µ–æ- –∏ depth-SLAM / –•–∞–±—Ä" https://habr.com/ru/company/singularis/blog/279035/
* "Deep learning combined with SLAM research status - Programmer Sought" https://programmersought.com/article/87761070085/
====

== —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞

. SLAM, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ, –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏–Ω—Ç—Ä–æ
. ORB SLAM - –º–æ–Ω–æ —Å–ª–∞–º
* –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
. ORB SLAM 2 - —Å—Ç–µ—Ä–µ–æ, –≥–ª—É–±–∏–Ω–∞, –æ–ø–∏—Å–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
* –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ —Å—Ç–µ—Ä–µ–æ,
* —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∞–Ω–∞–ª–æ–≥–∞–º–∏
* –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
. ORB SLAM 3
* —á—Ç–æ –ø–æ–º–µ–Ω—è–ª–æ—Å—å
* –º–µ—Ç—Ä–∏–∫–∏
. —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤—ã–≤–æ–¥


== ORB SLAM





read from paper, translate, correct 
https://www.arxiv-vanity.com/papers/1502.00956/

include::orb-slam-supporting.adoc[tag=Bundle_Adjustment]


== ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras

—á–æ—Ç –∑–∞ –º–µ—Ç–æ–¥, –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

NTRODUCTION

// Simultaneous Localization and Mapping (SLAM) has been
// a hot research topic in the last two decades in the Computer Vi-
// sion and Robotics communities, and has recently attracted the
// attention of high-technological companies. 
// SLAM techniques
// build a map of an unknown environment and localize the
// sensor in the map with a strong focus on real-time operation.

// Among the different sensor modalities, cameras are cheap
// and provide rich information of the environment that allows
// for robust and accurate place recognition. Therefore Visual
// SLAM solutions, where the main sensor is a camera, are of
// major interest nowadays. 

// Place recognition is a key module
// of a SLAM system to close loops (i.e. detect when the sensor
// returns to a mapped area and correct the accumulated error
// in exploration) and to relocalize the camera after a tracking
// failure, due to occlusion or aggressive motion, or at system
// re-initialization.

// –ø–æ—á–µ–º—É –º–æ–Ω–æ —Å–ª–∞–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ

// Visual SLAM can be performed by using just a monocular
// camera, which is the cheapest and smallest sensor setup.
// However as depth is not observable from just one camera,
// the scale of the map and estimated trajectory is unknown.

// –∫–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–Ω–∞—á–∞–ª–∞ —Å–Ω—è—Ç—å –ª–æ–∫–∞—Ü–∏—é —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫

//–Ω–µ –ø—Ä–æ–±–ª–µ–º–∞ –µ—Å–ª–∏ –º—ã –∑–∞–∏–Ω–µ—Ä–µ—Å–æ–≤–∞–Ω—ã –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –¥–ª—è —Ä–µ–∞–ª—Ç–∞–π–º –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–µ—Ä–µ–æ –∏–ª–∏ –≥–ª—É–±–∏–Ω—É

// In addition the system bootstrapping require multi-view or
// filtering techniques to produce an initial map as it cannot
// be triangulated from the very first frame. Last but not least,

–æ—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞??

// Stereo input: trajectory and sparse reconstruction of an urban environment
// with multiple loop closures.
image::4-12-2021-13-04-00-PM.png[width=320] 


// RGB-D input: keyframes and dense pointcloud of a room scene with one
// loop closure. The pointcloud is rendered by backprojecting the sensor depth
// maps from estimated keyframe poses. No fusion is performed.

// —á—Ç–æ —Ç–∞–∫–æ–µ pointcloud, dense pointcloud, sensor depth maps, keyframe, fusion, –∫–∞–∫ —Ö—Ä–∞–Ω–∏—Ç—Å—è –∫–∞—Ä—Ç–∞, –≤–∏–¥—ã –¥–∞–Ω–Ω—ã—Ö, —Ç–∏–ø—ã –º–µ—Ç–æ–¥–æ–≤...

// ORB-SLAM2 processes stereo and RGB-D inputs to estimate camera
// trajectory and build a map of the environment. The system is able to close
// loops, relocalize, and reuse its map in real-time on standard CPUs with high
// accuracy and robustness.

// bundle adjustment - –†–µ–≥—É–ª–∏—Ä–æ–≤–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –≥—Ä–∞—Ñ–∞

II. RELATED WORK

A. Stereo SLAM

B. RGB-D SLAM

III. ORB-SLAM2

// ORB-SLAM2 for stereo and RGB-D cameras is built on
// our monocular feature-based ORB-SLAM

A general overview of the system is shown in Fig. 2. The system
has three main parallel threads: 1) the tracking to localize
the camera with every frame by finding feature matches to
the local map and minimizing the reprojection error applying
motion-only BA, 2) the local mapping to manage the local
map and optimize it, performing local BA, 3) the loop closing
to detect large loops and correct the accumulated drift by
performing a pose-graph optimization. This thread launches
a fourth thread to perform full BA after the pose-graph
optimization, to compute the optimal structure and motion
solution. 

The system has embedded a Place Recognition module
based on DBoW2 [16] for relocalization, in case of tracking
failure (e.g. an occlusion) or for reinitialization in an already
mapped scene, and for loop detection. The system maintains
a covisibiliy graph [8] that links any two keyframes observing
common points and a minimum spanning tree connecting
all keyframes. These graph structures allow to retrieve local
windows of keyframes, so that tracking and local mapping
operate locally, allowing to work on large environments, and
serve as structure for the pose-graph optimization performed
when closing a loop.
The system uses the same ORB features [17] for tracking,
mapping and place recognition tasks. These features are robust
to rotation and scale and present a good invariance to camera
auto-gain and auto-exposure, and illumination changes. More-
over they are fast to extract and match allowing for real-time
operation and show good precision/recall performance in bag-
of-word place recognition [18].
In the rest of this section we present how stereo/depth
information is exploited and which elements of the system
are affected. 

// For a detailed description of each system block,
// we refer the reader to our monocular publication [1].

A. Monocular, Close Stereo and Far Stereo Keypoints

.ORB-SLAM2 is composed of three main parallel threads: tracking, local mapping and loop closing, which can create a fourth thread to perform full BA after a loop closure. The tracking thread pre-processes the stereo or RGB-D input so that the rest of the system operates independently of the input sensor. Although it is not shown in this figure, ORB-SLAM2 also works with a monocular input as in [1].
image::4-12-2021-13-22-04-PM.png[] 


== ORB SLAM3

TODO:: copy from ORB-SLAM3 paper summary https://programmersought.com/article/85105331899/

image::images/plan-ef87c.png[]

image::images/plan-19a22.png[]


image::images/plan-72eb8.png[]

Tracking and mapping

* Map fusion and closed loop detection
** Visual map fusion

// * –û–ø–∏—Å–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–ê ORB SLAM

from
https://doi.org/10.1002/rob.21831

// == orb slam notation

// ORB-SLAM2 [Mur-Artal and Tard√≥s, 2017] and S-PTAM [Pire et al., 2017] are currently two of
// the best state-of-the-art feature-based visual SLAM approaches that can be used with a stereo
// camera. More recently, ProSLAM [Schlegel et al., 2017] has been released (only benchmark tools
// available at this time) to provide a comprehensive open source package using well know visual
// SLAM techniques. For ORB-SLAM2, it can be also used with a RGB-D camera. They are all
// graph-based SLAM approaches. For ORB-SLAM2 and S-PTAM, when a loop closure is detected
// using DBoW2 [G√°lvez-L√≥pez and Tard√≥s, 2012], the map is optimized using bundle adjustment.
// Graph optimization after loop closure is done in a separate thread to avoid influencing camera
// tracking frame rate performance. For ProSLAM, loop closures are detected by direct comparison
// of the descriptors in the map, instead of using a bag-of-words approach. For all these approaches,
// loop closure detection and graph optimization processing time increases as the map grows, which
// can make loop closure correction happening with a significant delay after being detected. The
// approaches maintain a sparse feature map. Without occupancy grid or dense point cloud outputs
// available out-of-the-box like lidar approaches, they can be then difficult to use on a real robot.

// ORB-SLAM2 [Mur-Artal and Tard√≥s, 2017]
// S-PTAM [Pire et al., 2017]
// ProSLAM [Schlegel et al., 2017]
// DBoW2 [G√°lvez-L√≥pez and Tard√≥s, 2012]
// loop closure detection
// graph optimization processing
// bundle adjustment
// descriptors in the map
// sparse feature map
// occupancy grid or dense point cloud
// graph-based SLAM approaches
// visual
// SLAM





=== references

https://programmersought.com/article/85105331899/
// https://githubmemory.com/repo/Mauhing/ORB_SLAM3
// ORB-SLAM3 Related Publications: https://github.com/UZ-SLAMLab/ORB_SLAM3
//
// [ORB-SLAM3] Carlos Campos, Richard Elvira, Juan J. G√≥mez Rodr√≠guez, Jos√© M. M. Montiel and Juan D. Tard√≥s, ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM, IEEE Transactions on Robotics 37(6):1874-1890, Dec. 2021. PDF.
//
// [IMU-Initialization] Carlos Campos, J. M. M. Montiel and Juan D. Tard√≥s, Inertial-Only Optimization for Visual-Inertial Initialization, ICRA 2020. PDF
//
// [ORBSLAM-Atlas] Richard Elvira, J. M. M. Montiel and Juan D. Tard√≥s, ORBSLAM-Atlas: a robust and accurate multi-map system, IROS 2019. PDF.
//
// [ORBSLAM-VI] Ra√∫l Mur-Artal, and Juan D. Tard√≥s, Visual-inertial monocular SLAM with map reuse, IEEE Robotics and Automation Letters, vol. 2 no. 2, pp. 796-803, 2017. PDF.
//
// [Stereo and RGB-D] Ra√∫l Mur-Artal and Juan D. Tard√≥s. ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras. IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255-1262, 2017. PDF.
//
// [Monocular] Ra√∫l Mur-Artal, Jos√© M. M. Montiel and Juan D. Tard√≥s. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015. (2015 IEEE Transactions on Robotics Best Paper Award). PDF.
//
// [DBoW2 Place Recognition] Dorian G√°lvez-L√≥pez and Juan D. Tard√≥s. Bags of Binary Words for Fast Place Recognition in Image Sequences. IEEE Transactions on Robotics, vol. 28, no. 5, pp. 1188-1197, 2012. PDF
